{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Digital Garden","text":"<p>This is a collection of my personal notes, and things I am learning.</p>"},{"location":"#cookbooks","title":"Cookbooks","text":"<p>I'm excited to introduce my new \"Cookbook\" series! This is where I'll be sharing step-by-step guides and practical examples for various technologies.</p> <p>The first cookbook in the series is the Google Cloud Gemini Cookbook.</p> <p>I hope you find these resources helpful!</p>"},{"location":"Gemini/","title":"Getting Started: Gemini Cookbook Setup","text":"<p>This guide provides step-by-step instructions to set up your local environment and Google Cloud project to follow the recipes in the Gemini Cookbook.</p>"},{"location":"Gemini/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>Python: Version 3.8 or higher.</li> <li>Google Cloud Account: An active GCP account with billing enabled. If   you're new, you can sign up for a   free trial.</li> <li>Basic Knowledge: Familiarity with Python, the command line, and Git.</li> </ul>"},{"location":"Gemini/#1-environment-setup","title":"1. Environment Setup","text":"<p>First, let's set up the project on your local machine.</p>"},{"location":"Gemini/#clone-the-repository","title":"Clone the Repository","text":"<p>Open your terminal and clone the cookbook's GitHub repository:</p> <pre><code>git clone https://github.com/msampathkumar/msampathkumar.github.io.git\ncd msampathkumar.github.io\n</code></pre>"},{"location":"Gemini/#set-up-a-virtual-environment","title":"Set Up a Virtual Environment","text":"<p>It's a best practice to use a virtual environment to manage project dependencies.</p> <pre><code># Create a virtual environment\npython3 -m venv .venv\n\n# Activate the virtual environment\n# On macOS and Linux:\nsource .venv/bin/activate\n# On Windows:\n# .\\.venv\\Scripts\\activate\n</code></pre>"},{"location":"Gemini/#install-dependencies","title":"Install Dependencies","text":"<p>Install the common Python packages needed for the cookbook examples:</p> <pre><code>pip install google-generativeai google-cloud-aiplatform streamlit python-dotenv\n</code></pre>"},{"location":"Gemini/#2-google-cloud-configuration-cli","title":"2. Google Cloud Configuration (CLI)","text":"<p>Next, configure the Google Cloud CLI (<code>gcloud</code>) to interact with your GCP project.</p>"},{"location":"Gemini/#install-and-initialize-google-cloud-sdk","title":"Install and Initialize Google Cloud SDK","text":"<p>If you don't have it, install the Google Cloud SDK. After installation, initialize it:</p> <pre><code>gcloud init\n</code></pre> <p>Follow the on-screen prompts to log in, select your GCP project, and set a default region.</p>"},{"location":"Gemini/#authenticate-for-local-development","title":"Authenticate for Local Development","text":"<p>For local development, authenticating with your user credentials is the easiest way to get started.</p> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"Gemini/#enable-required-apis","title":"Enable Required APIs","text":"<p>You need to enable the Vertex AI API to use Gemini models.</p> <pre><code># Set your project ID (if you didn't during gcloud init)\nexport PROJECT_ID=\"your-gcp-project-id\"\ngcloud config set project $PROJECT_ID\n\n# Enable the Vertex AI API\ngcloud services enable aiplatform.googleapis.com --project $PROJECT_ID\n</code></pre> <p>Note: Replace <code>your-gcp-project-id</code> with your actual Google Cloud Project ID.</p>"},{"location":"Gemini/#next-steps","title":"Next Steps","text":"<p>You are all set! Your environment is configured and you're ready to start building with Gemini.</p> <p>Head over to the Full Lesson Plan to see all the available recipes.</p>"},{"location":"HelloWorld/","title":"Hello World","text":"<p>this is an example!</p>"},{"location":"cookbook/","title":"Google Cloud Gemini Cookbook: A Practical Guide to Learn Fundamentals and Build Applications","text":""},{"location":"cookbook/#vision-executive-summary","title":"Vision &amp; Executive Summary","text":"<p>This project is a cookbook-style series designed to teach developers and AI enthusiasts how to build practical, real-world applications using Google Cloud's Gemini models. Through a series of hands-on blog posts and a central GitHub repository, this guide will provide clear, step-by-step instructions, making generative AI accessible even to those with limited prior experience. The goal is to empower builders, foster a collaborative community, and showcase the power of Gemini.</p>"},{"location":"cookbook/#guiding-principles","title":"Guiding Principles","text":"<ul> <li>Practical First: Focus on hands-on examples and code snippets that solve   real problems.</li> <li>Clarity and Simplicity: Provide clear, step-by-step instructions that are   easy to follow.</li> <li>Gemini Focused: Deep-dive into Google Cloud Gemini, its specific   features, and its ecosystem.</li> <li>Fundamental Concepts: Cover the necessary foundational knowledge to use   Gemini effectively.</li> </ul>"},{"location":"cookbook/#target-audience","title":"Target Audience","text":"<p>This series is for developers, AI enthusiasts, and anyone interested in learning how to build practical AI applications with Gemini.</p>"},{"location":"cookbook/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic Python programming knowledge.</li> <li>A Google Cloud Platform (GCP) account with billing enabled.</li> <li>Familiarity with the command line and GitHub is helpful.</li> </ul>"},{"location":"cookbook/#content-outline-lesson-plan","title":"Content Outline &amp; Lesson Plan","text":"<p>The series will be released as a sequence of lessons, each building upon the last.</p> <ul> <li> <p>Lesson 1: Building a Basic Chatbot with Gemini and Streamlit</p> </li> <li> <p>Objective: Introduce the fundamentals of the Gemini API and build a     simple, interactive chatbot and deploy to Cloud.</p> </li> <li>Core Concepts: API keys, model initialization, generating text,     streaming responses.</li> <li> <p>Tech Stack: Python, <code>google-genai</code> SDK, Streamlit.</p> </li> <li> <p>Lesson 2: Enhancing the Chatbot with Memory and Gemma</p> </li> <li> <p>Objective: Add conversational memory to the chatbot and explore using     open models like Gemma for specific tasks.</p> </li> <li>Core Concepts: Chat history management, context passing, integrating     local/open-source models.</li> <li>Tech Stack: Vertex AI Memory Bank, Gemma, (Optional) Google ADK.</li> </ul>"},{"location":"cookbook/#future-lessons-proposed-agenda","title":"Future Lessons (Proposed Agenda)","text":"<ul> <li> <p>Lesson 3: Unlocking Multimodality with Gemini Pro Vision</p> </li> <li> <p>Objective: Build an application that can understand and analyze     information from both images and text simultaneously.</p> </li> <li>Use Case Example: An app that takes a picture of a whiteboard diagram     and generates code, or identifies products in an image and searches for     them online.</li> <li> <p>Core Concepts: Multimodal prompts, image data handling, combining     visual and text inputs, prompt engineering for vision models.</p> </li> <li> <p>Lesson 4: Building a Knowledge Base Q&amp;A with RAG</p> </li> <li> <p>Objective: Create a Retrieval-Augmented Generation (RAG) system that     answers questions based on a custom document set (e.g., PDFs, text files).</p> </li> <li>Use Case Example: A chatbot that can answer questions about a company\u2019s     internal policy documents.</li> <li> <p>Core Concepts: Vector embeddings, vector databases (e.g., ChromaDB,     Pinecone), document chunking, semantic search.</p> </li> <li> <p>Lesson 5: Advanced RAG with Knowledge Graphs</p> </li> <li> <p>Objective: Go beyond simple vector search by building a RAG system that     understands the relationships between entities in your data, leading to     more accurate and context-aware answers.</p> </li> <li>Use Case Example: A financial analyst bot that can answer complex     queries like \"Which companies in our portfolio have board members who also     sit on the boards of their competitors?\"</li> <li> <p>Core Concepts: Entity and relationship extraction, building a knowledge     graph (e.g., with Neo4j), translating natural language to graph queries     (e.g., Cypher), combining graph retrieval with LLM generation.</p> </li> <li> <p>Lesson 6: Creating Autonomous Agents with Function Calling</p> </li> <li> <p>Objective: Empower Gemini to interact with external tools and APIs to     perform actions in the real world.</p> </li> <li>Use Case Example: A personal assistant that can check the weather, send     an email, or book a meeting by calling external APIs.</li> <li> <p>Core Concepts: Tool definition, function calling, structured data     extraction, handling API errors and responses.</p> </li> <li> <p>Lesson 7: Building Collaborative AI with Multi-Agent Systems</p> </li> <li> <p>Objective: Design a system where multiple specialized AI agents     collaborate to solve a complex problem that a single agent could not handle     alone.</p> </li> <li>Use Case Example: A research team with a \"Web Search\" agent, a \"Data     Analyst\" agent, and a \"Report Writer\" agent that work together to produce a     market analysis.</li> <li> <p>Core Concepts: Agent roles and specialization, inter-agent     communication, task decomposition, state management, and using a     manager/orchestrator agent.</p> </li> <li> <p>Lesson 8: Practical AI Safety and Model Evaluation</p> </li> <li> <p>Objective: Learn to build responsible, reliable AI applications and     objectively measure their performance before they reach production.</p> </li> <li>Use Case Example: Adding a validation step to a customer service bot to     ensure its answers are factually correct and non-toxic before sending them     to a user.</li> <li> <p>Core Concepts: Implementing guardrails, protecting against prompt     injection, detecting and mitigating bias, using evaluation frameworks     (e.g., RAGAs, TruLens) to measure faithfulness and relevance.</p> </li> <li> <p>Lesson 9: Deploying and Scaling on Google Cloud</p> </li> <li> <p>Objective: Take a prototype application and prepare it for production.</p> </li> <li>Core Concepts: Containerizing with Docker, deploying to Cloud Run,     managing API keys securely with Secret Manager, monitoring and logging.</li> </ul>"},{"location":"cookbook/#distribution-community-strategy","title":"Distribution &amp; Community Strategy","text":"<ul> <li>Source of Truth: A public GitHub repository will host all code,   resources, and drafts.</li> <li>Primary Publications: Blog posts will be published on Medium.com and   Dev.to to reach a broad developer audience.</li> <li>Community Engagement: Announcements, key takeaways, and discussions will   be shared on X (formerly Twitter) and LinkedIn to foster community   interaction and feedback.</li> </ul>"},{"location":"cookbook/#potential-impact-success-metrics","title":"Potential Impact &amp; Success Metrics","text":"<ul> <li>Empower Developers: Lower the barrier to entry for building and deploying   AI-powered applications.</li> <li>Foster Community: Create a hub for Gemini users to share knowledge,   collaborate, and get feedback.</li> <li>Showcase Gemini: Highlight the versatility and power of Gemini for   solving real-world problems.</li> <li>Success Metrics: Track GitHub stars/forks, blog post views/claps, social   media engagement, and community contributions.</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/ProjectMgmt/RACI/","title":"RACI Matrix","text":"<p>A RACI chart can be an extremely effective way to define project roles, give direction to each team member and stakeholder, and ensure work gets done efficiently. Having a RACI chart available throughout the duration of your project as a quick visual can be invaluable. In this reading, we will cover the function of a RACI chart and its components and explore how project managers use RACI charts to define and document project roles and responsibilities.</p> <p>Elements of a RACI chart A RACI chart creates clear roles and gives direction to each team member and stakeholder. Over your career, you may hear a RACI chart referred to as a Responsibility Assignment Matrix (RAM), RACI diagram, or RACI matrix. The ultimate goal of this chart is to clarify each person\u2019s role on your project.</p> <p>First, let\u2019s break down each of the roles people can be assigned:</p> <p>R: Responsible: who gets the work done</p> <p>A: Accountable: who makes sure the work is done</p> <p>C: Consulted: who gives input or feedback on work</p> <p>I: Informed: who needs to know the outcome</p> <p>Note that RACI charts can be organized in different ways, depending on personal preference, number of tasks being assigned, and number of people involved. In the previous video, we showed you one RACI chart format. The template below shows another way a typical RACI chart might be organized:</p> <p></p> <p>Responsible Individuals who are assigned the \u201cresponsible\u201d role for a task are the ones who are actually doing the work to complete the task. Every task needs at least one responsible party. It\u2019s a best practice to try to limit the number of team members assigned to a task\u2019s responsible role, but in some cases, you may have more than one.</p> <p>A couple of questions to ask yourself when determining which person or people should be placed in the responsible role for a given task are:</p> <p>What department does the work fall under?</p> <p>Who will perform the work?</p> <p>It is helpful to evaluate the people on your team to determine the role that suits them. Remember that you may need to list roles rather than names, if some people take on more than one role.</p> <p>Let\u2019s dig deeper into our example with Office Green. Our task is to develop price points for the project, and the Financial Analyst will complete the work for this task. Therefore, we will list \u201cFinancial Analyst\u201d in the responsible role for this task in the RACI chart.</p> <p>A section of a RACI chart, where the Financial Analyst is in the \"responsible\" role Accountable The \u201caccountable\u201d person is responsible for making sure the task gets done. It is important to have only one individual accountable for each task. This helps clarify ownership of the task. The accountable person ultimately has the authority to approve the deliverable of the responsible party.</p> <p>In order to determine who should be tagged as the accountable team member, consider:</p> <p>Who will delegate the task to be completed?</p> <p>Who will review the work to determine if the task is complete?</p> <p>You may encounter a situation where the responsible party is also accountable, but where possible, it is helpful to separate these roles. Ensuring that accountability is not shared ensures that there is no confusion on who the ownership belongs to.</p> <p>Continuing with our Office Green example, you have assigned the \u201caccountable\u201d role to the Head of Finance. The Head of Finance has to make sure the project stays in budget and makes a profit, so they have the ultimate authority over the price points for the product. Therefore, they will need to approve the Financial Analyst\u2019s work on the task.</p> <p>A section of a RACI chart, where the Head of Finance is in the \"accountable\" role Consulted Team members or stakeholders who are placed in the \u201cconsulted\u201d role have useful information to help complete the task. There is no maximum or minimum number of people who can be assigned a \u201cconsulted\u201d role, but it\u2019s important that each person has a reason for being there.</p> <p>Here are a few ways you can help identify who is appropriate for the role:</p> <p>Who will the task impact?</p> <p>Who will have input or feedback for the responsible person to help the work be completed?</p> <p>Who are the subject matter experts (SMEs) for the task?</p> <p>The consulted people will be in frequent, two-way communication with the responsible party, so it is key to make sure that the right people are in this role to help accomplish the task efficiently and correctly.</p> <p>Back to the project at Office Green, we\u2019ve got a \u201cresponsible\u201d Financial Analyst and an \u201caccountable\u201d Head of Finance. Who else would need to provide input on the product\u2019s price points? Whose decisions and feedback will directly affect the task? The Director of Product will need to be consulted on the matter, as they oversee all product offerings. This person will have information about potential changes to the product and how these changes might affect price points.</p> <p>A section of a RACI chart, where the Director of Product is in the \"consulted\" role Informed Individuals who are identified as needing to be \u201cinformed\u201d need to know the final decisions that were made and when a task is completed. It is common to have many people assigned to this category and for some team members to be informed on most tasks. Team members or stakeholders here will not be asked for feedback, so it is key to make sure people who are in this group only require status updates and do not need to provide any direct feedback for the completion of the effort.</p> <p>Key questions to ask yourself in order to ensure that you have appropriately captured individuals in the \u201cinformed\u201d role are:</p> <p>Who cares about this task\u2019s completion?</p> <p>Who will be affected by the outcome?</p> <p>Now that you\u2019ve determined who is responsible, accountable, and consulted on the Office Green project task, it is time to determine who needs to be informed about the task. Your Financial Analyst has set the price points with input from the Director of Product, and the Head of Finance has approved. You will now need to inform the Sales Team about the final price points, as they will need this information to sell the product.</p> <p>A section of a RACI chart, where the Sales Team is in the \"informed\" role Pro tip: You could end up with a large number of team members and stakeholders who are placed in the \u201cinformed\u201d role. If so, make sure that you have a plan to keep them informed that is not labor-intensive. Something as easy as view-only access to your project plan or meeting notes could prevent you from having to create separate communications along the way.</p> <p>Key takeaway The RACI chart is a valuable tool. It can help you define and document project roles and responsibilities, give direction to each team member and stakeholder, and ensure work gets done efficiently. A RACI chart can also help you analyze and balance the workload of your team. While it may take many revisions to make sure that your team members and stakeholders are being placed into the right roles in your RACI chart, doing this work up front helps save time and prevent miscommunications later on.</p>"},{"location":"google-cloud-gemini-cookbook/","title":"Google Cloud - Gemini Cookbook","text":""},{"location":"google-cloud-gemini-cookbook/#introduction","title":"Introduction","text":"<p>This cookbook is a collection of recipes for using the Google Cloud platform.</p>"},{"location":"google-cloud-gemini-cookbook/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Google account with access to the Google Cloud Platform.</li> <li>A valid API key or service account credentials.</li> <li>A working Python environment (Python &gt;= 3.8).</li> </ul>"},{"location":"google-cloud-gemini-cookbook/#lessons","title":"Lessons","text":"<ul> <li>Lesson 1: Building a Hello World app with Streamlit &amp; deploy to Google Cloud Run</li> <li>Lesson 2: Building a Chatbot(Web App) with Gemini 2.5</li> <li>Lesson 3: Adding Context Awareness - part 1</li> <li>Lesson 4: Adding Context Awareness - part 2</li> <li>Lesson 5: WIP</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-01/","title":"\ud83d\ude80 Lesson 01: Your First Chatbot - \"Hello, Streamlit &amp; Cloud Run!\" \ud83c\udf10","text":"<p>Welcome to the Google Cloud Gemini Cookbook! In this very first lesson, we're going to embark on an exciting journey: taking your Python code from a simple idea to a live web application in minutes. Forget complex setups; with Streamlit and Google Cloud Run, deploying your first web app is incredibly fast and fun! \u2728</p> <p>GitHub Source: Link</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#what-youll-learn","title":"What You'll Learn \ud83c\udf93","text":"<p>This lesson focuses on the essentials of getting a web application up and running quickly:</p> <ol> <li>Build a \"Hello World\" with Streamlit: Discover how effortlessly you can    create interactive web apps using just Python. Streamlit handles all the    front-end magic for you! \ud83d\udc0d</li> <li>Deploy to Google Cloud Run: Learn to take your Streamlit app and deploy    it as a scalable, serverless container on Google Cloud Run. This means your    app can handle traffic effortlessly, and you only pay for what you use! \u2601\ufe0f\ud83d\udcb8</li> </ol> <p>By the end of this lesson, you'll have a fully functional web application accessible via a URL, demonstrating the incredible speed of modern cloud development. \ud83d\ude80</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#prerequisites","title":"Prerequisites \ud83d\udee0\ufe0f","text":"<p>Before we begin, ensure you have the following:</p> <ul> <li>A Google Cloud Project with billing enabled.</li> <li>The <code>gcloud</code> CLI installed and configured.</li> <li>Python 3.8+ installed on your local machine.</li> <li><code>pip</code> (Python package installer).</li> </ul> <p>For development, we recommend using the Google Cloud Shell, which comes pre-installed with the necessary tools.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#what-is-streamlit","title":"What is Streamlit?","text":"<p>Streamlit is an open-source Python framework that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few lines of code, you can build and deploy powerful data apps.</p> <p>While Streamlit is not as feature-rich as full-fledged web frameworks like Django or Flask, its strength lies in its simplicity and ability to create highly interactive applications quickly. This makes it an excellent choice for building demos and prototypes.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#what-is-google-cloud-run","title":"What is Google Cloud Run?","text":"<p>Cloud Run is a fully managed serverless platform that enables you to run stateless containers that are invocable via web requests or Pub/Sub events. You can deploy your code to Cloud Run, and it will automatically scale up or down based on traffic.</p> <p>Here are some of the benefits of using Cloud Run:</p> <ul> <li>Easy to use: Deploy your application with a single command.</li> <li>Serverless: No infrastructure to manage.</li> <li>Scalable: Automatically scales to meet demand.</li> <li>Cost-effective: Pay only for the resources you use.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#local-development-your-streamlit-hello-world","title":"\ud83d\udcbb Local Development: Your Streamlit \"Hello World\"","text":"<p>Let's start by creating a simple Streamlit application locally. Here's how you can do it:</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#1-set-up-your-environment","title":"1. Set up your environment","text":"<p>Create a virtual environment and install the required dependencies:</p> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#2-create-a-simple-streamlit-app","title":"2. Create a simple Streamlit app","text":"<p>Create a file named <code>streamlit_app.py</code> with the following content:</p> <pre><code>import streamlit as st\n\nst.title(\"Sample AI App\")\n\nst.text(\"This is a sample app.\")\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#3-run-the-app-locally","title":"3. Run the app locally","text":"<p>To run the app locally, use the following command:</p> <pre><code>streamlit run streamlit_app.py --server.port 8080\n</code></pre> <p>You should see your Streamlit app open in your browser and navigating to <code>http://localhost:8080</code>. Interact with it! This is your app running locally. \ud83d\ude80</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#4-deploy-to-cloud-run","title":"4. Deploy to Cloud Run","text":"<p>To deploy your app to Cloud Run, you'll need to create a <code>Procfile</code> and a <code>deploy.sh</code> script.</p> <p>Procfile</p> <p>Create a file named <code>Procfile</code> with the following content:</p> <pre><code>web: streamlit run streamlit_app.py --server.port=8080 --server.address=0.0.0.0 --server.enableCORS=false --browser.gatherUsageStats=false\n</code></pre> <p>This file tells Cloud Run how to start your application.</p> <p>deploy.sh</p> <p>Create a file named <code>deploy.sh</code> with the following content:</p> <pre><code>#!/bin/bash\n# Purpose: To deploy the App to Cloud Run.\n\n# Google Cloud Project ID\nPROJECT=\"your-gcp-project-id\"\n\n# Google Cloud Region\nLOCATION=\"us-central1\"\n\n# Deploy app from source code\ngcloud run deploy simple-app --source . --region=$LOCATION --project=$PROJECT --allow-unauthenticated\n</code></pre> <p>Important: Replace <code>\"your-gcp-project-id\"</code> with your actual Google Cloud Project ID.</p> <p>Now, run the deployment script:</p> <pre><code>bash deploy.sh\n</code></pre> <p>This command will build a container image from your source code, push it to the container registry, and deploy it to Cloud Run. Once the deployment is complete, you'll see a service URL in the output.</p> <p>Congratulations \ud83c\udf89! You have successfully deployed your Streamlit app to Cloud Run.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#cleanup","title":"Cleanup","text":"<p>To avoid incurring future charges, delete the resources you created:</p> <ul> <li>Go to the Cloud Run console and   delete your application.</li> <li>Go to the Container Registry and   delete the container image.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#learn-more","title":"Learn More","text":"<ul> <li>Cloud Run Documentation</li> <li>Streamlit Documentation</li> <li>Authenticating to Cloud Run</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-02/","title":"Lesson 2: \u2601\ufe0f Deploy Your AI Chatbot to Google Cloud Run: Go Live!","text":"<p>Welcome to the second lesson in our Gemini Cookbook series! This time, we're diving into the exciting world of conversational AI. You'll learn to build a smart, interactive chatbot using the power of Streamlit and Google's Gemini 2.5 Flash model. We'll be using the official Google Cloud Vertex AI SDK, which has powerful features like chat sessions that give your bot a memory.</p> <p>GitHub Source: Link</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#what-youll-create","title":"What You'll Create","text":"<p>Get ready to build a sleek, web-based chatbot. With Streamlit as our frontend, your chatbot will connect to the mighty Gemini 1.5 Flash model, enabling you to have dynamic and stateful conversations. It's like having your own personal AI assistant!</p> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#what-youll-need","title":"What You'll Need","text":"<p>To get started, make sure you have the following essentials:</p> <ul> <li>A Google Cloud project with the Vertex AI API ready to go.</li> <li>Python 3.8 or a newer version.</li> <li>The <code>pip</code> package manager for installing our dependencies.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#mandatory-steps","title":"Mandatory steps","text":"<p>This is a mandatory steps to access Gemini Models from your Google Cloud Project.</p> <p>I have installed the Gcloud tool and used Application Default Credentials to get the credentials. If you want to run the code in Google Cloud project, then you need to update respective service-account with the required permissions. For details, check out this user-managed service account article.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#lets-get-building","title":"Let's Get Building!","text":"<ol> <li>Get the Code: First, clone the repository and hop into the right    directory:</li> </ol> <p><code>bash    git clone https://github.com/msampathkumar/msampathkumar.github.io.git    cd msampathkumar.github.io/docs/google-cloud-gemini-cookbook/lesson-02</code></p> <ol> <li>Set Up Your Workspace: Create a virtual environment to keep things tidy:</li> </ol> <p><code>bash    python3 -m venv .venv    source .venv/bin/activate</code></p> <ol> <li>Install the Magic: Time to install the necessary Python packages:</li> </ol> <p><code>bash    pip install -r requirements.txt</code></p> <ol> <li>Connect to Google Cloud: Authenticate your local environment to use    Google Cloud services:</li> </ol> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#a-look-under-the-hood","title":"A Look Under the Hood","text":"<p>Let's take a peek at the code that makes our chatbot tick.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#the-chatbot-ui-streamlit_apppy","title":"The Chatbot UI: <code>streamlit_app.py</code>","text":"<p>This file is the heart of our Streamlit app. It's responsible for:</p> <ul> <li>Providing a chat interface for user input.</li> <li>Displaying the response from the model.</li> <li>Maintaining the conversation history.</li> </ul> <p>While you would typically use Streamlit's <code>session_state</code> to store the conversation history manually, the Vertex AI SDK simplifies this. We'll use a <code>ChatSession</code> object from the SDK, which automatically handles the history for us. We just need to store this one object in <code>st.session_state</code> to make our chat stateful.</p> <p>You can see the core logic below:</p> <pre><code>import streamlit as st\nimport llm\n\n# Initialize chat session in Streamlit's session state.\n# This will be run only once, on the first run of the session.\nif \"chat_session\" not in st.session_state:\n    st.session_state.chat_session = llm.start_chat()\n\n# Display chat history from the session state\nfor message in st.session_state.chat_session.history:\n    with st.chat_message(\"assistant\" if message.role == \"model\" else \"user\"):\n        st.markdown(message.parts[0].text)\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#the-brains-of-the-operation-llmpy","title":"The Brains of the Operation: <code>llm.py</code>","text":"<p>This file handles all the communication with the Gemini 2.5 Flash model. As we are using GenAI SDK, we can use environment variables to set up the required details for authentication. Also, GenAI SDK provides us with <code>Client</code> class which we can use to create a chat session and send messages to the Gemini Model and receive.</p> <pre><code>from google import genai\n\n# Using environment variables to pass essential parameters to the client.\nclient = genai.Client()\n\n# Create chat session\nchat_session = client.chats.create(\"gemini-2.0-flash-lite-001\")\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#to-run-chatbot-in-cli","title":"To run chatbot in CLI","text":"<pre><code>python llm.py\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#to-run-streamlit-chatbot","title":"To run streamlit chatbot","text":"<pre><code>streamlit run streamlit_app.py\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#deploy-the-application","title":"Deploy the application","text":"<p>You can deploy your chatbot to Google Cloud Run and share it with the world.</p> <p>Use the <code>deploy.sh</code> script to package your app into a Docker image and send it to the Google Artifact Registry.</p> <pre><code>./deploy.sh\n</code></pre> <p>The script will then deploy your app to Cloud Run, making it live on the web.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#you-did-it","title":"You Did It!","text":"<p>High five! You've built and deployed a fully functional chatbot with Streamlit and Gemini Pro. You've seen how to use the new Generative AI SDK and its chat features to create a more natural and engaging conversational experience. Now, go ahead and have a chat with your new AI friend!</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/","title":"Lesson-03","text":""},{"location":"google-cloud-gemini-cookbook/lesson-03/#blog-post-lesson-03","title":"Blog Post: Lesson 03","text":""},{"location":"google-cloud-gemini-cookbook/lesson-03/#build-your-first-context-aware-gemini-chatbot-in-minutes-the-secret-to-speed-and-relevance","title":"\ud83d\ude80 Build Your First Context-aware Gemini Chatbot in Minutes: The Secret to Speed and Relevance! \u26a1","text":"<p>Scenario: It's Tuesday, July 22, 2025, 12:26 PM CEST. You're a developer, enjoying your morning coffee in Warsaw \u2615, contemplating your next big feature. Suddenly, your director bursts in: \"We need a quick demo of a new, context-aware chatbot for our internal knowledge base \u2013 and the meeting is in 30 minutes! Can you get something ready?\" \ud83e\udd2f</p> <p>Panic? Absolutely not! Not with Gemini and Streamlit. This lesson is your secret weapon to rapidly inject intelligence into your chatbot, focusing on direct, consistent, and reusable context methods that get you up and running with meaningful interactions fast. \ud83d\ude80</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#1-understanding-context-why-its-your-chatbots-superpower","title":"1. Understanding Context: Why It's Your Chatbot's Superpower \ud83e\uddb8\u200d\u2640\ufe0f","text":"<p>Large Language Models (LLMs) like Gemini are incredible, but they're not clairvoyant. Without explicit guidance, their responses can be generic, vague, or even incorrect when faced with specific or domain-sensitive questions. Context is the \"secret sauce\" that transforms a generic LLM into a specialized, helpful chatbot. It's the information you provide to guide the model's understanding and shape its output. \ud83d\udca1</p> <p>Consider a simple chatbot built with Gemini and Streamlit. If you ask it a very specific question without any context, it might struggle.</p> <p>Example: A Generic Chatbot Responding to a Specific Query</p> <p>Let's say your basic Streamlit app simply forwards user input to Gemini. If you ask about an internal project:</p> <pre><code>$ python llm.py\n\nChat session ID: 4383160272\nEnter your question (or 'exit' to quit)\n\nUser: What are the key milestones for Project Alpha in Q3?\n\nModel: I need a little more information to tell you about\n   Project Alpha's Q3 milestones! Could you please tell me \n   more about what \"Project Alpha\" refers to? \ud83d\ude0a\n\n</code></pre> <p>or via chatbot UI:</p> <pre><code>streamlit run streamlit_app.py\n</code></pre> <p></p> <p>This is where context comes in. By providing context, you tell Gemini exactly what \"Project Alpha\" means in your world. \u2728</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#2-in-context-learning-icl-guiding-with-examples-instantly","title":"2. In-Context Learning (ICL): Guiding with Examples, Instantly \u2728","text":"<p>In-Context Learning (ICL) is the quickest way to demonstrate a desired output pattern to Gemini. You provide explicit examples directly within your prompt, and Gemini learns from these patterns without needing any fine-tuning. It's like teaching by showing!</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#one-shot-learning-a-single-guiding-example","title":"One-Shot Learning: A Single Guiding Example \u261d\ufe0f","text":"<p>For simple tasks, one example might be all you need. You show Gemini a single input-output pair, and it follows that pattern for subsequent queries.</p> <p>Use Case: Simple classification, rephrasing, or straightforward translation. \ud83c\udf10</p> <p>Example: One-Shot Translation</p> <pre><code>User: Translate this English to French.\n      English: Hello.\n      French: Bonjour.\n      English: What is your name?\n      French:\nChatbot (With one-shot example): Quel est votre nom?\n</code></pre> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#few-shot-learning-reinforcing-complex-patterns","title":"Few-Shot Learning: Reinforcing Complex Patterns \ud83d\udcda","text":"<p>When the task is more nuanced or requires a specific output format, providing a few examples helps Gemini better grasp the pattern. It's like providing multiple reference points for complex concepts.</p> <p>Use Case: More nuanced categorization, structured data extraction, or adhering to specific stylistic requirements. \ud83d\udccb</p> <p>Example: Few-Shot Sentiment Analysis</p> <pre><code>\nUser: Review: The delivery was fast!\n      Sentiment: Positive.\n\n      Review: The product broke immediately.\n      Sentiment: Negative.\n\n      Review: The customer service was okay, but the delivery was slow.\n      Sentiment:\nChatbot (With few-shot examples): Mixed/Neutral\n</code></pre> <p></p> <p>Considerations: While powerful for quick guidance, ICL consumes tokens with every prompt, which can impact cost and latency for very long examples or many turns. \ud83d\udcb8\ud83d\udc22</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#3-system-instructions-setting-your-chatbots-personality-and-rules","title":"3. System Instructions: Setting Your Chatbot's Personality and Rules \ud83d\udcdc","text":"<p>System instructions define your chatbot's overarching persona, tone, and behavioral guardrails. This is a foundational layer of context that applies to all subsequent user turns in a chat session, making Gemini's responses consistent and aligned with your brand or application's requirements. It's like giving your bot a permanent job description! \ud83e\uddd1\u200d\ud83d\udcbb</p> <p>You define the \"rules of engagement\" for your chatbot, ensuring it behaves predictably. \ud83d\udea6</p> <p>Code Sample: llm.py (LLM Interaction Logic)</p> <p>System instructions are defined in the <code>llm.py</code> file.</p> <pre><code>chat_session = client.chats.create(\n   model=MODEL_NAME,\n   config=GenerateContentConfig(\n      system_instruction=[\n       \"You're a helpful Gemini AI Chatbot.\",\n       \"Answer user's questions and use simple and clear language.\"\n       \"When possible, reply to user's question with a single sentence or a few sentences.\",\n       \"Free to use emojis.\"\n       \"Be open and friendly. Don't be afraid to ask questions or clarify things.\",\n      ]\n  ),\n)\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#4-context-caching-reusing-static-information-efficiently","title":"4. Context Caching: Reusing Static Information Efficiently \ud83d\udce6","text":"<p>Imagine your chatbot needs to be an expert on a fixed set of documents, like internal reports, product manuals, or, in our case, specific research papers. Sending these large documents with every single API call would be slow and expensive. This is where Context Caching becomes a game-changer.</p> <p>Gemini's Context Caching allows you to process and store static, frequently-referenced content once. You then refer to this cached content using a simple, lightweight name in your subsequent API calls. This drastically saves tokens, reduces latency, and lowers costs, especially when dealing with large files.</p> <p>Example Use Case: Efficient retrieval of information from large, static knowledge bases, optimizing token usage, and simplifying your requests to model. \ud83d\udce6</p> <p>Let update our chatbot a chatbot to be expert on the Gemini family of models, using two key research papers as its knowledge base.</p> <ul> <li>Paper 1: Gemini: A Family of Highly Capable Multimodal   Models.(2312.11805v3.pdf)</li> <li>Paper 2: Gemini 1.5: Unlocking multimodal understanding across millions of   tokens of context   (2403.05530.pdf)Instead   of feeding these PDFs to the model repeatedly, we'll cache them and let our   chatbot use that cached knowledge.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#how-it-works-a-two-step-process","title":"How It Works: A Two-Step Process","text":"<p>Step 1: Create the Cache</p> <p>First, you need to upload your static files and create a CachedContent object. This is a one-time operation. You'll save the name of the cache to use in your application later.</p> <p>Full code: https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/genai/content_cache/contentcache_create_with_txt_gcs_pdf.py</p> <pre><code>system_instruction = \"\"\"\nYou are an expert researcher.\nYou always stick to the facts in the sources provided, and never make up new facts.\nNow look at these research papers, and answer the following questions.\n\"\"\"\n\ncache_objects = [\n    Part.from_uri(\n        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n        mime_type=\"application/pdf\",\n    ),\n    Part.from_uri(\n        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n        mime_type=\"application/pdf\",\n    ),\n]\n\ncontent_cache = client.caches.create(\n    model=\"gemini-2.5-flash\",\n    config=CreateCachedContentConfig(\n        contents=[Content(role=\"user\", parts=cache_objects)],\n        system_instruction=system_instruction,\n        display_name=\"example-cache\",\n        ttl=\"86400s\",\n    ),\n)\n</code></pre> <p>Note: Cache is created with a TTL (time to live). After a specific amount of time, the cache will be deleted.</p> <p>Once the cache is created you will a cache name (e.g., cachedContents/f1e2d3c4-a5b6-7890-a1b2-c3d4e5f6a7b8), which we will use with chatbot.</p> <p>Step 2: Use the Cache in Your Chatbot</p> <p>Once the cache is created, you can use it in your chatbot by passing its name in the <code>GenerateContentConfig</code> object. For example:</p> <pre><code>system_instruction = \"...\"\ncache_name = (\"projects/.../locations/us-central1/keyRings/.../cryptoKeys/...\",)\n\nchat_session = client.chats.create(\n    config=GenerateContentConfig(\n        cached_content=cache_name,\n        system_instruction=None if cache_name else system_instruction,\n    )\n)\n</code></pre> <p>Since the Cache has a defined timelimit, it required a little attention to avoid any potential issues. So I hae created a CacheManager to manage the cache, which will automatically clean up expired caches.</p> <p>Here is the content for <code>cache.py</code></p> <p>Examples:</p> <p>Here is the example of not using Context Cache:</p> <p></p> <p>Here is the example of using Context Cache:</p> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#to-deploy-this-application-on-google-cloud-run","title":"To Deploy This Application on Google Cloud Run:","text":"<ol> <li>Clone this repository and navigate to the directory</li> </ol> <pre><code>git clone https://github.com/msampathkumar/msampathkumar.github.io.git\ncd docs/google-cloud-gemini-cookbook/lesson-03\n</code></pre> <ol> <li>Setup your virtual environment and install dependencies:</li> </ol> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre> <ol> <li>Run the application locally to make sure it works as expected.</li> </ol> <pre><code>streamlit run streamlit_app.py\n</code></pre> <ol> <li>To deploy to Google Cloud Run, you can use the <code>deploy.sh</code> script:</li> </ol> <pre><code>bash deploy.sh\n</code></pre> <p>Github: https://github.com/msampathkumar/msampathkumar.github.io/tree/master/docs/google-cloud-gemini-cookbook</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#congratulations","title":"Congratulations \ud83c\udf89 ( \u25e0\u203f\u25e0 )","text":"<p>Congratulations, You've Achieved a Milestone!</p> <p>You successfully deployed a content-aware chatbot application to Google Cloud Run.</p> <p>In the next lesson, we will delve into two more features that will further enhance your chatbot's context awareness:</p> <ol> <li>RAG: Retrieval Augmented Generation (RAG)</li> <li>Grounding: Using Google Search</li> </ol> <p>Let's continue learning and exploring these innovative tools together!</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/","title":"Lesson-04","text":""},{"location":"google-cloud-gemini-cookbook/lesson-04/#blog-post-lesson-04","title":"Blog Post: Lesson 04","text":""},{"location":"google-cloud-gemini-cookbook/lesson-04/#unlock-enterprise-ai-grounding-gemini-with-rag-and-google-cloud-search","title":"\ud83d\udd13 Unlock Enterprise AI: Grounding Gemini with RAG and Google Cloud Search \ud83d\ude80","text":"<p>Welcome back! \ud83d\udc4b In Lesson 03, we built fast, relevant chatbots using direct context methods like In-Context Learning \u2728, System Instructions \ud83d\udcdc, and Context Caching \ud83d\udce6. Those are fantastic for quick demos and consistent persona. But what if your chatbot needs to know the latest company sales figures, details from an obscure internal report, or specific client history? That's where Large Language Models (LLMs) hit their limit \u2013 they hallucinate! \ud83d\ude35\u200d\ud83d\udcab</p> <p>Today, we tackle the next frontier: Grounding your Gemini models in real-time, external, and even proprietary data using Retrieval Augmented Generation (RAG), with Google Cloud Search as your powerful engine. \ud83d\udd0d</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#1-grounding-llms-combating-hallucinations-with-external-knowledge","title":"1. Grounding LLMs: Combating Hallucinations with External Knowledge \ud83d\udee1\ufe0f","text":"<p>LLMs are brilliant pattern matchers, trained on vast amounts of internet data. However, their knowledge is frozen at their last training cut-off date. They don't have real-time access to the internet, nor do they inherently know your company's internal documents, recent sales data, or specific client interactions. When asked about such information, they might:</p> <ul> <li>Refuse to answer: \"I don't have information on that.\" \ud83e\udd37\u200d\u2640\ufe0f</li> <li>Generate generic or outdated information: \"According to general industry   trends...\" \ud83d\uddd3\ufe0f</li> <li>\"Hallucinate\": Make up plausible-sounding but entirely false information.   This is the most dangerous! \ud83d\udea8</li> </ul> <p>Example: A Chatbot Hallucinating on Proprietary Data</p> <p>Let's revisit our chatbot. If you ask about specific, sensitive internal data:</p> <pre><code>User: What was our Q1 revenue for the 'Project Phoenix' initiative for the EMEA region?\nChatbot (Gemini, no grounding): For Project Phoenix in \n\nQ1, the EMEA region generated an estimated $12.5 million, driven by strong performance in software licensing. We anticipate continued growth...\n\n</code></pre> <p>(Imagine a screenshot here: A Streamlit UI with the user's highly specific financial query and a chatbot response that sounds confident but is entirely fabricated, as the data is proprietary and not in Gemini's training. A big red X emoji to emphasize the hallucination.) \u274c</p> <p>This is a problem. In enterprise settings, accuracy is paramount. We need a way to connect Gemini to our actual, verifiable knowledge. This is called grounding. \ud83c\udf31</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#2-introduction-to-retrieval-augmented-generation-rag-your-llms-research-assistant","title":"2. Introduction to Retrieval Augmented Generation (RAG): Your LLM's Research Assistant \ud83e\uddd1\u200d\ud83c\udf93","text":"<p>Retrieval Augmented Generation (RAG) is an architectural pattern that solves the hallucination problem by giving LLMs access to external, up-to-date, and domain-specific information.</p> <p>Think of RAG as giving your LLM a brilliant research assistant: \ud83e\uddd0</p> <ol> <li>Retrieve: When you ask a question, the \"research assistant\" first    searches a vast library (your knowledge base) for relevant documents or    snippets. \ud83d\udcda</li> <li>Augment: It then takes the most relevant findings and gives them to the    LLM. \ud83e\udde9</li> <li>Generate: Finally, the LLM uses this specific, retrieved information    (along with your original query) to formulate an accurate and grounded    response. \u2705</li> </ol> <p>Key Components of a RAG System:</p> <ul> <li>Knowledge Base: Your source of truth \u2013 internal documents, databases,   websites, etc. \ud83d\udcc1</li> <li>Retriever: A system (like a vector database combined with an embedding   model, or a powerful search engine like Google Cloud Search) that can quickly   find the most relevant pieces of information from your knowledge base based   on a query. \ud83d\udd0e</li> <li>Generator: The LLM (Gemini) that synthesizes the answer using the   retrieved context. \ud83e\udde0</li> </ul> <p>Benefits of RAG:</p> <ul> <li>Factuality: Grounding responses in real data dramatically reduces   hallucinations. \u2705</li> <li>Currency: LLMs can answer questions about information that wasn't in   their training data or is constantly changing. \u23f0</li> <li>Domain-Specific Knowledge: Access to proprietary or niche topics. \ud83d\udcbc</li> <li>Attribution: Potential to show users where the information came from   (e.g., \"Source: Policy Manual v2.1\"). \ud83d\udd17</li> </ul> <p>RAG vs. Context Cache: A Crucial Distinction \ud83d\udea8</p> <p>It's vital to differentiate RAG from Context Caching (Lesson 03).</p> <ul> <li>Context Cache: Reuses small, static pieces of pre-loaded or   conversational context. It's about efficiency for fixed data, avoiding   redundant token usage. Think of it as a persistent \"sticky note\" or   short-term memory for repeated instructions or small data blocks. \ud83d\udcdd</li> <li>RAG: Dynamically retrieves specific, often large, and always relevant   chunks of information from a vast, external knowledge base on demand for   each query. It's about expanding the LLM's factual knowledge with new,   current, or private data. \ud83c\udf10</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#3-application-architecture","title":"3. Application Architecture","text":"<p>This application is designed to be a flexible and extensible chatbot that can leverage different grounding techniques. Here's a breakdown of the core components:</p> <pre><code>graph TD\n    subgraph User Interface\n        A[streamlit_app.py]\n    end\n\n    subgraph Core Logic\n        B(llm.py)\n    end\n\n    subgraph Optional Add-ons\n        C[cache.py]\n        D[rag.py]\n    end\n\n    A -- \"Initializes and calls\" --&gt; B\n    B -- \"Optionally uses\" --&gt; C\n    B -- \"Optionally uses\" --&gt; D\n</code></pre> <p>Core Application Logic:</p> <ul> <li><code>streamlit_app.py</code> (UI): This is the user-facing component of the   application, built with Streamlit. It provides the chat interface, handles   user input, and displays the LLM's responses. It's the \"skin\" of our   application.</li> <li><code>llm.py</code> (The Brain): This module is the central nervous system of our   chatbot. It's responsible for all interactions with the Gemini API. It takes   the user's prompt, and based on the selected mode (Default, Context Cache, or   RAG), it constructs the appropriate request to the Gemini model.</li> <li><code>cache.py</code> (Optional Battery): This module manages the Context Cache.   When the \"Use Context Cache\" option is selected, <code>llm.py</code> uses this module to   create and manage a cache of context, which can be reused across   conversations to improve speed and reduce costs.</li> <li><code>rag.py</code> (Optional Battery): This module handles the Retrieval-Augmented   Generation (RAG) functionality. When the \"Use RAG as Tool\" option is   selected, <code>llm.py</code> uses this module to create and manage a RAG corpus. This   allows the LLM to retrieve information from a knowledge base to answer   questions.</li> </ul> <p>Code Links:</p> <ul> <li>streamlit_app.py</li> <li>llm.py</li> <li>cache.py</li> <li>rag.py</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#4-rag-implementation-flow","title":"4. RAG Implementation Flow","text":"<p>Here\u2019s a more detailed look at how the RAG process works within our application when the \"Use RAG as Tool\" option is enabled:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Streamlit UI (streamlit_app.py)\n    participant LLM Brain (llm.py)\n    participant RAG Corpus (rag.py)\n    participant Gemini API\n\n    User-&gt;&gt;Streamlit UI (streamlit_app.py): Enters a prompt\n    Streamlit UI (streamlit_app.py)-&gt;&gt;LLM Brain (llm.py): Sends prompt to get chat session\n    LLM Brain (llm.py)-&gt;&gt;RAG Corpus (rag.py): Initializes RAG corpus\n    RAG Corpus (rag.py)--&gt;&gt;LLM Brain (llm.py): Returns RAG corpus name\n    LLM Brain (llm.py)-&gt;&gt;Gemini API: Creates chat session with RAG tool\n    Gemini API--&gt;&gt;LLM Brain (llm.py): Returns chat session\n    LLM Brain (llm.py)--&gt;&gt;Streamlit UI (streamlit_app.py): Returns chat session\n    Streamlit UI (streamlit_app.py)-&gt;&gt;LLM Brain (llm.py): Sends user prompt\n    LLM Brain (llm.py)-&gt;&gt;Gemini API: Sends prompt to Gemini\n    Gemini API-&gt;&gt;RAG Corpus (rag.py): Retrieves relevant documents\n    RAG Corpus (rag.py)--&gt;&gt;Gemini API: Returns documents\n    Gemini API--&gt;&gt;LLM Brain (llm.py): Generates response based on retrieved documents\n    LLM Brain (llm.py)--&gt;&gt;Streamlit UI (streamlit_app.py): Returns grounded response\n    Streamlit UI (streamlit_app.py)--&gt;&gt;User: Displays response\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#5-application-screenshots","title":"5. Application Screenshots","text":"<p>(Placeholder for screenshots of the Streamlit application in action)</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#conclusion","title":"Conclusion","text":"<p>This lesson demonstrated how to ground Gemini models with external knowledge using RAG. By leveraging RAG, we can build more accurate, factual, and useful AI applications that can reason about private and real-time data.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/","title":"Project Alpha","text":""},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#project-summary","title":"Project Summary","text":"<p>Project Alpha is a next-generation customer relationship management (CRM) platform designed to revolutionize how businesses interact with their customers. It leverages artificial intelligence and machine learning to provide predictive analytics, automate sales workflows, and offer personalized customer experiences.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#goals","title":"Goals","text":"<ul> <li>To increase customer retention by 20% within the first year of launch.</li> <li>To reduce the sales cycle duration by 15%.</li> <li>To improve sales team productivity by 30%.</li> <li>To provide a single, unified view of the customer across all touchpoints.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#team","title":"Team","text":"<ul> <li>Project Manager: Alice Johnson</li> <li>Lead Developer: Bob Williams</li> <li>Frontend Developer: Charlie Brown</li> <li>Backend Developer: Diana Prince</li> <li>UI/UX Designer: Eve Adams</li> <li>QA Engineer: Frank Miller</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#timeline","title":"Timeline","text":"<ul> <li>Phase 1: Discovery &amp; Planning - Q1 2023 (Completed)</li> <li>Phase 2: Design &amp; Prototyping - Q2 2023 (Completed)</li> <li>Phase 3: Development &amp; Implementation - Q3-Q4 2023 (In Progress)</li> <li>Phase 4: Testing &amp; QA - Q1 2024</li> <li>Phase 5: Launch &amp; Deployment - Q2 2024</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#status","title":"Status","text":"<p>Current Status: In Progress</p> <p>We are currently in the middle of Phase 3. The backend team is focused on building out the core APIs for contact management and analytics. The frontend team is developing the main dashboard and reporting components.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#technologies","title":"Technologies","text":"<ul> <li>Frontend: React, Redux, TypeScript</li> <li>Backend: Python, Django, PostgreSQL</li> <li>AI/ML: TensorFlow, scikit-learn</li> <li>Infrastructure: Google Cloud Platform (GCP), Docker, Kubernetes</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/rag_intro/","title":"Rag intro","text":"<p>Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external data sources when generating responses. Here's a breakdown:</p> <p>What it is:</p> <ul> <li>Combining Retrieval and Generation:</li> <li>RAG combines the strengths of information retrieval systems (like search     engines) with the generative power of LLMs.</li> <li>It enables LLMs to go beyond their pre-trained data and access up-to-date     and specific information.</li> <li>How it works:</li> <li>When a user asks a question, the RAG system first retrieves relevant     information from external data sources (e.g., databases, documents, web     pages).</li> <li>This retrieved information is then provided to the LLM as additional     context.</li> <li>The LLM uses this augmented context to generate a more accurate and     informative response.</li> </ul> <p>Why it's helpful:</p> <ul> <li>Access to Up-to-Date Information:</li> <li>LLMs are trained on static datasets, so their knowledge can become     outdated. RAG allows them to access real-time or frequently updated     information.</li> <li>Improved Accuracy and Factual Grounding:</li> <li>RAG reduces the risk of LLM \"hallucinations\" (generating false or     misleading information) by grounding responses in verified external data.</li> <li>Enhanced Contextual Relevance:</li> <li>By providing relevant context, RAG enables LLMs to generate more precise     and tailored responses to specific queries.</li> <li>Increased Trust and Transparency:</li> <li>RAG can provide source citations, allowing users to verify the information     and increasing trust in the LLM's responses.</li> <li>Cost Efficiency:</li> <li>Rather than constantly retraining large language models, RAG allows for the     introduction of new data in a more cost effective way.</li> </ul> <p>In essence, RAG bridges the gap between the vast knowledge of LLMs and the need for accurate, current, and contextually relevant information.</p> <p>Source: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/","title":"This is a re-cap session of what we have lessoned in the previous sessions and revamp of our Application Code.","text":"<p>In this section we will creating a <code>setting.py</code> to store all the configuration variables for our application. We will also create a <code>cache_manager.py</code> file which will handle all.</p>"}]}