{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to My Digital Garden","text":"<p>This is a space where I share my learnings, thoughts, and experiments. You'll find a collection of my personal notes and things I am learning.</p>"},{"location":"#gemini-cookbook","title":"Gemini Cookbook","text":"<p>I'm excited to introduce my \"Cookbook\" series! This is where I'll be sharing step-by-step guides and practical examples for various technologies.</p> <p>The first cookbook in the series is the Google Cloud Gemini Cookbook.</p>"},{"location":"#blog","title":"Blog","text":"<p>I also have a blog where I post short articles and updates. It's a place for more informal thoughts and quick tips.</p> <p>I hope you find these resources helpful!</p>"},{"location":"cookbook/","title":"Google Cloud Gemini Cookbook: A Practical Guide to Learn Fundamentals and Build Applications","text":""},{"location":"cookbook/#vision-executive-summary","title":"Vision &amp; Executive Summary","text":"<p>This project is a cookbook-style series designed to teach developers and AI enthusiasts how to build practical, real-world applications using Google Cloud's Gemini models. Through a series of hands-on blog posts and a central GitHub repository, this guide will provide clear, step-by-step instructions, making generative AI accessible even to those with limited prior experience. The goal is to empower builders, foster a collaborative community, and showcase the power of Gemini.</p>"},{"location":"cookbook/#guiding-principles","title":"Guiding Principles","text":"<ul> <li>Practical First: Focus on hands-on examples and code snippets that solve   real problems.</li> <li>Clarity and Simplicity: Provide clear, step-by-step instructions that are   easy to follow.</li> <li>Gemini Focused: Deep-dive into Google Cloud Gemini, its specific   features, and its ecosystem.</li> <li>Fundamental Concepts: Cover the necessary foundational knowledge to use   Gemini effectively.</li> </ul>"},{"location":"cookbook/#target-audience","title":"Target Audience","text":"<p>This series is for developers, AI enthusiasts, and anyone interested in learning how to build practical AI applications with Gemini.</p>"},{"location":"cookbook/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic Python programming knowledge.</li> <li>A Google Cloud Platform (GCP) account with billing enabled.</li> <li>Familiarity with the command line and GitHub is helpful.</li> </ul>"},{"location":"cookbook/#content-outline-lesson-plan","title":"Content Outline &amp; Lesson Plan","text":"<p>The series will be released as a sequence of lessons, each building upon the last.</p> <ul> <li> <p>Lesson 1: Building a Basic Chatbot with Gemini and Streamlit</p> </li> <li> <p>Objective: Introduce the fundamentals of the Gemini API and build a     simple, interactive chatbot and deploy to Cloud.</p> </li> <li>Core Concepts: API keys, model initialization, generating text,     streaming responses.</li> <li> <p>Tech Stack: Python, <code>google-genai</code> SDK, Streamlit.</p> </li> <li> <p>Lesson 2: Enhancing the Chatbot with Memory and Gemma</p> </li> <li> <p>Objective: Add conversational memory to the chatbot and explore using     open models like Gemma for specific tasks.</p> </li> <li>Core Concepts: Chat history management, context passing, integrating     local/open-source models.</li> <li>Tech Stack: Vertex AI Memory Bank, Gemma, (Optional) Google ADK.</li> </ul>"},{"location":"cookbook/#future-lessons-proposed-agenda","title":"Future Lessons (Proposed Agenda)","text":"<ul> <li> <p>Lesson 3: Unlocking Multimodality with Gemini Pro Vision</p> </li> <li> <p>Objective: Build an application that can understand and analyze     information from both images and text simultaneously.</p> </li> <li>Use Case Example: An app that takes a picture of a whiteboard diagram     and generates code, or identifies products in an image and searches for     them online.</li> <li> <p>Core Concepts: Multimodal prompts, image data handling, combining     visual and text inputs, prompt engineering for vision models.</p> </li> <li> <p>Lesson 4: Building a Knowledge Base Q&amp;A with RAG</p> </li> <li> <p>Objective: Create a Retrieval-Augmented Generation (RAG) system that     answers questions based on a custom document set (e.g., PDFs, text files).</p> </li> <li>Use Case Example: A chatbot that can answer questions about a company\u2019s     internal policy documents.</li> <li> <p>Core Concepts: Vector embeddings, vector databases (e.g., ChromaDB,     Pinecone), document chunking, semantic search.</p> </li> <li> <p>Lesson 5: Advanced RAG with Knowledge Graphs</p> </li> <li> <p>Objective: Go beyond simple vector search by building a RAG system that     understands the relationships between entities in your data, leading to     more accurate and context-aware answers.</p> </li> <li>Use Case Example: A financial analyst bot that can answer complex     queries like \"Which companies in our portfolio have board members who also     sit on the boards of their competitors?\"</li> <li> <p>Core Concepts: Entity and relationship extraction, building a knowledge     graph (e.g., with Neo4j), translating natural language to graph queries     (e.g., Cypher), combining graph retrieval with LLM generation.</p> </li> <li> <p>Lesson 6: Creating Autonomous Agents with Function Calling</p> </li> <li> <p>Objective: Empower Gemini to interact with external tools and APIs to     perform actions in the real world.</p> </li> <li>Use Case Example: A personal assistant that can check the weather, send     an email, or book a meeting by calling external APIs.</li> <li> <p>Core Concepts: Tool definition, function calling, structured data     extraction, handling API errors and responses.</p> </li> <li> <p>Lesson 7: Building Collaborative AI with Multi-Agent Systems</p> </li> <li> <p>Objective: Design a system where multiple specialized AI agents     collaborate to solve a complex problem that a single agent could not handle     alone.</p> </li> <li>Use Case Example: A research team with a \"Web Search\" agent, a \"Data     Analyst\" agent, and a \"Report Writer\" agent that work together to produce a     market analysis.</li> <li> <p>Core Concepts: Agent roles and specialization, inter-agent     communication, task decomposition, state management, and using a     manager/orchestrator agent.</p> </li> <li> <p>Lesson 8: Practical AI Safety and Model Evaluation</p> </li> <li> <p>Objective: Learn to build responsible, reliable AI applications and     objectively measure their performance before they reach production.</p> </li> <li>Use Case Example: Adding a validation step to a customer service bot to     ensure its answers are factually correct and non-toxic before sending them     to a user.</li> <li> <p>Core Concepts: Implementing guardrails, protecting against prompt     injection, detecting and mitigating bias, using evaluation frameworks     (e.g., RAGAs, TruLens) to measure faithfulness and relevance.</p> </li> <li> <p>Lesson 9: Deploying and Scaling on Google Cloud</p> </li> <li> <p>Objective: Take a prototype application and prepare it for production.</p> </li> <li>Core Concepts: Containerizing with Docker, deploying to Cloud Run,     managing API keys securely with Secret Manager, monitoring and logging.</li> </ul>"},{"location":"cookbook/#distribution-community-strategy","title":"Distribution &amp; Community Strategy","text":"<ul> <li>Source of Truth: A public GitHub repository will host all code,   resources, and drafts.</li> <li>Primary Publications: Blog posts will be published on Medium.com and   Dev.to to reach a broad developer audience.</li> <li>Community Engagement: Announcements, key takeaways, and discussions will   be shared on X (formerly Twitter) and LinkedIn to foster community   interaction and feedback.</li> </ul>"},{"location":"cookbook/#potential-impact-success-metrics","title":"Potential Impact &amp; Success Metrics","text":"<ul> <li>Empower Developers: Lower the barrier to entry for building and deploying   AI-powered applications.</li> <li>Foster Community: Create a hub for Gemini users to share knowledge,   collaborate, and get feedback.</li> <li>Showcase Gemini: Highlight the versatility and power of Gemini for   solving real-world problems.</li> <li>Success Metrics: Track GitHub stars/forks, blog post views/claps, social   media engagement, and community contributions.</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/posts/","title":"Blog","text":""},{"location":"blog/posts/advanced-memory-management-agentic-ai/","title":"Advanced Memory Management for Agentic AI Development","text":"<p> Transform your stateless LLMs into intelligent agents with proper memory architecture</p> <p>Imagine building an AI assistant that forgets your name every time you say hello. That's the reality developers face with Large Language Models today\u2014every conversation starts from zero.</p>"},{"location":"blog/posts/advanced-memory-management-agentic-ai/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Challenge 1: Why Agentic Data Explodes</li> <li>Challenge 2: The Master Data Management Challenge</li> <li>Solution: Achieving Agentic MDM</li> <li>Short-Term MDM via ADK</li> <li>Long-Term MDM with Vertex AI</li> <li>Conclusion</li> <li>Get Started Today</li> </ul> <p>You've probably experienced this frustration firsthand: explaining the same context(user preferences) repeatedly to AI Models, watching costs skyrocket as conversations grow longer, or seeing your carefully crafted agent become confused by its own conversation history. These aren't just minor inconveniences\u2014they're fundamental barriers to building truly intelligent AI agents.</p> <p>Large Language Models (LLMs) are inherently stateless, meaning they forget everything from the moment a single API call concludes. This presents a fundamental challenge for developers striving to build personalized, stateful AI agents that can hold long, meaningful interactions. The solution lies in Context Engineering\u2014the discipline of dynamically assembling and managing all necessary information for the LLM to reason and act.</p> <p>The core dilemma is that while past data is essential for intelligence, managing it quickly becomes an overwhelming problem.</p>"},{"location":"blog/posts/advanced-memory-management-agentic-ai/#challenge-1-why-agentic-data-explodes-and-becomes-useless","title":"Challenge 1: Why Agentic Data Explodes (And Becomes Useless)","text":"<p>In agentic applications, every message, tool call, tool output, and intermediate thought is an \"Event\" appended to the active conversation log, or Session. This history rapidly spirals out of control, introducing four major challenges:</p> <ul> <li>Context Window Limits: The conversation transcript can exceed the maximum   token count the LLM can process, causing the API call to fail.</li> <li> <p>Cost and Latency: Most LLM providers charge by tokens sent and received.   Larger contexts increase costs and latency, resulting in a slower response   time for the user.</p> </li> <li> <p>Noise and Context Rot: As the context grows, its quality diminishes. The   LLM's ability to focus on critical information suffers from context rot as   conversational filler and irrelevant details flood the prompt.</p> </li> <li> <p>Reliability Issues: As context approaches limits, agents become unpredictable\u2014   sometimes forgetting critical instructions or failing to complete tasks as the   relevant information gets pushed out of the window.</p> </li> </ul> <p>Consider a simple conversation that quickly explodes:</p> <pre><code>User: \"What's the weather today?\"\nAgent: [Tool call: weather_api] \u2192 500 tokens\nAgent: \"It's 72\u00b0F and sunny...\"\nUser: \"Should I bring an umbrella?\"\nAgent: [Tool call: forecast_api] \u2192 400 tokens\nAgent: \"No rain expected...\"\nUser: \"What about tomorrow?\"\nAgent: [Tool call: extended_forecast] \u2192 600 tokens\n...\n# After 20 interactions: 15,000+ tokens of history!\n</code></pre>"},{"location":"blog/posts/advanced-memory-management-agentic-ai/#challenge-2-the-master-data-management-mdm-challenge-for-agents","title":"Challenge 2: The Master Data Management (MDM) Challenge for Agents","text":"<p>To create a continuous, personalized user experience, an agent must transform the transient chaos of a single Session (the \"workbench\") into highly organized, persistent knowledge (the \"organized filing cabinet\"). This process is a deep challenge akin to managing data coherence in a Master Data Management (MDM) System\u2014it requires maintaining a single, accurate source of truth for user-specific knowledge.</p> <p>Note: Master Data Management (MDM) is a well known discipline in which business and information technology collaborate to ensure the uniformity, accuracy, stewardship, semantic consistency, and accountability of the enterprise's official shared master data asset.</p> <p>The long-term knowledge, called Memory, must be curated to be useful. Otherwise, a simple extraction process results in a noisy, contradictory, and unreliable log. An agent memory manager must tackle the MDM problem by performing consolidation:</p> <ul> <li> <p>Conflict Resolution: Resolving contradictions when a user's preferences   change over time.</p> </li> <li> <p>Deduplication: Merging duplicative entities or facts mentioned in multiple   ways.</p> </li> <li> <p>Information Evolution: Updating initial, simple facts as they become more   nuanced.</p> </li> <li> <p>Forgetting: Proactively pruning old, stale, or low-confidence memories to   keep the knowledge base relevant and efficient.</p> </li> </ul>"},{"location":"blog/posts/advanced-memory-management-agentic-ai/#solution-achieving-agentic-mdm-with-google-adk-and-vertex-ai","title":"Solution: Achieving Agentic MDM with Google ADK and Vertex AI","text":"<p>Effective memory management requires complementary strategies for both short-term (in-session) and long-term (cross-session) memory.</p>"},{"location":"blog/posts/advanced-memory-management-agentic-ai/#i-short-term-mdm-via-adk-content-compaction","title":"i) Short-Term MDM via ADK Content Compaction","text":"<p>For managing the immediate Session history and fitting it within the LLM's context window, the Google Agent Development Kit (ADK) offers compaction techniques. These methods act as short-term MDM by trimming the verbose log while preserving core context:</p> <ul> <li>Token-Based Truncation: Before sending the history, the agent includes   messages starting with the most recent and works backward until a token limit   (e.g., 4000 tokens) is reached, cutting off the rest.</li> <li>Recursive Summarization: Older messages are periodically replaced by an   AI-generated summary, which is then used as a condensed history. For instance,   ADK's EventsCompactionConfig can trigger this LLM-based summarization after a   configured number of turns.</li> </ul> <p>Example: Implementing ADK Compaction</p> <pre><code>import asyncio\nfrom google.adk.runners import InMemoryRunner\nfrom google.adk.agents.llm_agent import Agent\nfrom google.adk.apps.app import App, EventsCompactionConfig\nfrom google.adk.apps.llm_event_summarizer import LlmEventSummarizer\nfrom google.adk.models import Gemini\n\n\nroot_agent = Agent(\n    model=\"gemini-2.5-flash-lite\",\n    name=\"greeter_agent\",\n    description=\"An agent that provides a friendly greeting.\",\n    instruction=\"Be Humble, Respectable and keep your answers short and sweet.\",\n)\n\n# Define the AI model to be used for summarization:\nsummarization_llm = Gemini(model=\"gemini-2.5-flash\")\n\n# Create the summarizer with the custom model:\nmy_summarizer = LlmEventSummarizer(llm=summarization_llm)\n\n\n# Configure the App with the custom summarizer and compaction settings:\napp = App(\n    name=\"root_agent\",\n    root_agent=root_agent,\n    events_compaction_config=EventsCompactionConfig(\n        compaction_interval=3,\n        overlap_size=1,\n        summarizer=my_summarizer,\n    ),\n)\n\n# Set a Runner using the imported application object\nrunner = InMemoryRunner(app=app)\n\n\nasync def main():\n    print(\"\\n################################ START OF RUN\")\n    print(\"Tip: Use `exit` or `quit` to exit the app.\\n---\")\n    user_query = input(\"User: \")\n    while user_query.strip() not in [\"exit\", \"quit\"]:\n        try:\n            await runner.run_debug(user_query)\n        except Exception as e:\n            print(f\"An error occurred during agent execution: {e}\")\n        user_query = input(\"User: \")\n    print(\"#---\")\n    print(\"################################ END OF RUN\\n\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"blog/posts/advanced-memory-management-agentic-ai/#ii-long-term-mdm-with-vertex-ai-memory-bank","title":"ii) Long-Term MDM with Vertex AI Memory Bank","text":"<p>For persistent, cross-session memory management (the true long-term MDM), Google offers Vertex AI Memory Bank (also known as Agent Engine Memory Bank).</p> <p>This managed service is an LLM-driven ETL pipeline designed to automatically manage the entire lifecycle of long-term memory. It ensures the agent is an expert on the user, not just on external facts.</p> <p>Vertex AI Memory Bank solves the MDM problem for long-term memory through:</p> <ul> <li>Extraction and Consolidation: It uses LLMs to intelligently extract   meaningful facts from the conversation history (Sessions) and performs the   critical consolidation step to resolve conflicts and deduplicate   information.</li> <li>Asynchronous Generation: Critically, memory generation and consolidation   run as an asynchronous background process after the agent has responded to   the user, ensuring zero latency on the \"hot path\" of user interaction.</li> <li>Persistent Storage and Retrieval: It durably stores these memories,   linking them to a specific user ID, and makes them available for intelligent,   similarity-search based retrieval in future sessions.</li> </ul> <p>Example: Integrating Vertex AI Memory Bank</p> <pre><code>print(\"\ud83e\udde0 Creating Memory Bank configuration for hotel concierge...\\n\")\n\nbasic_memory_config = MemoryBankConfig(\n    # Which embedding model to use for similarity search\n    similarity_search_config=SimilaritySearchConfig(\n        embedding_model=f\"projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/text-embedding-005\"\n    ),\n    # Which LLM to use for extracting memories from conversations\n    generation_config=GenerationConfig(\n        model=f\"projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/gemini-2.5-flash\"\n    ),\n)\n\nprint(\"\u2705 Memory Bank configuration created!\")\nprint(\"\\n\ud83d\udee0\ufe0f Creating Agent Engine with Memory Bank...\\n\")\nprint(\"\u23f3 This provisions the backend infrastructure for guest memory storage...\")\n\nagent_engine = client.agent_engines.create(\n    config={\"context_spec\": {\"memory_bank_config\": basic_memory_config}}\n)\n\nagent_engine_name = agent_engine.api_resource.name\n\nprint(\"\\n\u2705 Agent Engine created successfully!\")\nprint(f\"   Resource Name: {agent_engine_name}\")\nprint(\"\ud83d\udcac Creating a session for guest check-in...\\n\")\n\n# Generate a unique guest identifier\nguest_id = \"guest_emma_\" + str(uuid.uuid4())[:4]\n\n# Create a session for this guest\nsession = client.agent_engines.sessions.create(\n    name=agent_engine_name,\n    user_id=guest_id,\n    config={\"display_name\": f\"Check-in conversation for {guest_id}\"},\n)\n\nsession_name = session.response.name\n\nprint(\"\u2705 Session created successfully!\")\n</code></pre> <p>Complete code: Link</p> <p>By combining ADK's short-term compaction with the robust, intelligent, and managed capabilities of Vertex AI Memory Bank, developers can confidently build agents that truly remember, adapt, and personalize their interactions without facing the inevitable data explosion challenges.</p>"},{"location":"blog/posts/advanced-memory-management-agentic-ai/#conclusion-from-stateless-to-stateful-ai","title":"Conclusion: From Stateless to Stateful AI","text":"<p>Memory management isn't just a technical optimization\u2014it's the foundation that transforms LLMs from impressive demos into production-ready AI agents. Without proper memory architecture, your agents will forever remain trapped in a cycle of forgetting and re-learning, frustrating users and burning through your budget.</p> <p>The combination of Google ADK's session compaction and Vertex AI Memory Bank provides a complete solution to the agentic MDM challenge:</p> <ul> <li>Immediate wins with token-based truncation and summarization</li> <li>Zero-latency memory through asynchronous processing</li> <li>Intelligent consolidation that resolves conflicts automatically</li> <li>Scalable architecture that grows with your user base</li> </ul>"},{"location":"blog/posts/advanced-memory-management-agentic-ai/#get-started-today","title":"Get Started Today","text":"<p>Ready to build agents that truly remember? Here's your roadmap:</p> <ol> <li>Start Small: Implement ADK compaction in your existing agents to immediately reduce costs and latency</li> <li>Monitor Usage: Track your token consumption and identify memory-intensive workflows</li> <li>Graduate to Memory Bank: Once you need cross-session persistence, integrate Vertex AI Memory Bank</li> <li>Optimize Continuously: Use the consolidation rules to fine-tune what your agents remember</li> </ol> <p>Don't let your AI agents suffer from perpetual amnesia. The tools exist today to build intelligent, stateful applications that deliver the personalized experiences users expect. Your users\u2014and your infrastructure budget\u2014will thank you.</p> <p>Ready to dive deeper? Check out the ADK documentation and start building agents that remember.</p>"},{"location":"blog/posts/advanced-memory-management-agentic-ai/#key-takeaways","title":"Key Takeaways","text":"<p>TL;DR - Transform your stateless LLMs into intelligent, stateful agents with proper memory management:</p> <p>\u2705 The Problem: LLM conversations explode exponentially, causing context limits, high costs, and degraded performance</p> <p>\u2705 The Solution: Implement a two-tier memory architecture:   - Short-term: Use ADK's compaction (truncation + summarization) to manage session context   - Long-term: Use Vertex AI Memory Bank for persistent, cross-session memory with intelligent consolidation</p> <p>\u2705 The Benefits:   - Reduce token costs by up to 80%   - Eliminate context window failures   - Enable truly personalized, continuous user experiences   - Zero-latency memory processing with async architecture</p> <p>\u2705 Next Step: Start with ADK compaction today\u2014it's a single config change that delivers immediate ROI</p> <p>References:</p> <ul> <li>Vertex AI Agent Engine Memory Bank overview</li> <li>Sessions Overview - Agent Development Kit</li> <li>Session Context compression - Agent Development Kit</li> <li>Memory - Agent Development Kit</li> </ul>"},{"location":"blog/posts/beads-for-ai-collaboration/","title":"Beads: The Missing Link for Collaborative AI Agents","text":"<p>The biggest challenge in building sophisticated AI agents isn't just making them smart, it's making them work together on complex, long-running tasks. Most AI agents have the memory of a goldfish, limited to the current conversation. Once the session ends or the context window is full, everything is forgotten. This makes true collaboration impossible.</p> <p>Until now. Meet Beads, the persistent task memory for AI agents.</p>","tags":["beads","ai agent","collaboration","multi-session","persistent memory"]},{"location":"blog/posts/beads-for-ai-collaboration/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>\ud83c\udfaf What Beads is and why it's a game-changer for AI agentics.</li> <li>\ud83d\udca1 How Beads compares to ephemeral task lists like <code>write_todos</code>.</li> <li>\ud83d\ude80 A simple workflow to get your AI agents collaborating with persistent memory.</li> </ul>","tags":["beads","ai agent","collaboration","multi-session","persistent memory"]},{"location":"blog/posts/beads-for-ai-collaboration/#the-collaboration-black-hole","title":"The Collaboration Black Hole","text":"<p>If you've ever tried to make two or more AI agents collaborate on a task that takes more than a few minutes, you've likely run into the \"collaboration black hole\". Agents can't remember what they did, what others did, or what needs to be done next. The work is fragmented, duplicated, and often fails entirely.</p> <p>This is because most task management for agents relies on ephemeral, in-session memory. It's like trying to build a skyscraper with sticky notes that disappear every hour.</p>","tags":["beads","ai agent","collaboration","multi-session","persistent memory"]},{"location":"blog/posts/beads-for-ai-collaboration/#beads-a-git-backed-memory-for-agents","title":"Beads: A Git-Backed Memory for Agents","text":"<p>Beads is a simple yet powerful concept: a git-backed issue tracker for AI agents. Think of it as a shared, persistent to-do list that survives conversation resets, context window limits, and even allows for human-in-the-loop collaboration.</p> <p>Because it's built on Git, Beads provides a robust, decentralized, and auditable history of all work performed by all agents. It's the long-term memory and project management system that the agent ecosystem has been missing.</p>","tags":["beads","ai agent","collaboration","multi-session","persistent memory"]},{"location":"blog/posts/beads-for-ai-collaboration/#how-it-works-the-core-workflow","title":"How it Works: The Core Workflow","text":"<p>The Beads workflow is designed to be simple and intuitive for both agents and their human supervisors.</p> <pre><code>graph TD\n    A[Start Session] --&gt; B{bd ready};\n    B --&gt; C[Select Task];\n    C --&gt; D{bd show &lt;id&gt;};\n    D --&gt; E{bd update --status in_progress};\n    E --&gt; F[Work on Task];\n    F --&gt; G{bd close &lt;id&gt;};\n    G --&gt; H[End Session];\n    H --&gt; I{bd sync};\n    I --&gt; J[Git Repository];\n</code></pre> <p>Here's the simple 5-step process:</p> <ol> <li><code>bd ready</code>: An agent starts its session by asking what tasks are ready to be worked on.</li> <li><code>bd show &lt;id&gt;</code>: The agent gets the full context and history for a specific task.</li> <li><code>bd update &lt;id&gt; --status in_progress</code>: The agent marks the task as in-progress, so other agents know it's being worked on.</li> <li><code>bd close &lt;id&gt;</code>: Once the work is done, the agent closes the task, providing a reason for completion.</li> <li><code>bd sync</code>: At the end of the session, the agent syncs all its changes to the shared git repository, making its work visible to everyone else.</li> </ol>","tags":["beads","ai agent","collaboration","multi-session","persistent memory"]},{"location":"blog/posts/beads-for-ai-collaboration/#beads-vs-write_todos-when-to-use-which","title":"Beads vs. <code>write_todos</code>: When to Use Which?","text":"<p>Beads isn't meant to replace all other task management. For simple, single-session tasks, a tool like <code>write_todos</code> is often sufficient. The key is understanding when to use persistent vs. ephemeral memory.</p> Feature Beads (Persistent) <code>write_todos</code> (Ephemeral) Use Case Multi-session, complex work Single-session, linear tasks Dependencies Handles complex dependencies &amp; blockers Simple checklists Persistence Survives conversation compaction Lost after session Collaboration Git-backed for team sync Local to the current session Best For \"Will I need this context in 2 weeks?\" \"What do I need to do right now?\" <p>Decision test: If you can't imagine needing the context for a task in two weeks, use <code>write_todos</code>. For everything else, use Beads.</p>","tags":["beads","ai agent","collaboration","multi-session","persistent memory"]},{"location":"blog/posts/beads-for-ai-collaboration/#your-next-action","title":"Your Next Action","text":"<p>Ready to supercharge your AI agents with persistent, collaborative memory?</p> <p>\u2192 Explore the Beads documentation</p>","tags":["beads","ai agent","collaboration","multi-session","persistent memory"]},{"location":"blog/posts/beads-for-ai-collaboration/#lets-connect","title":"Let's Connect!","text":"<p>Found this helpful? I'd love to hear your thoughts!</p> <ul> <li>\ud83d\udc26 Share your results on Twitter/X</li> <li>\ud83d\udcbc Connect on LinkedIn</li> <li>\ud83c\udf1f Star the project on GitHub</li> </ul>","tags":["beads","ai agent","collaboration","multi-session","persistent memory"]},{"location":"blog/posts/building-search-for-agentic-ai/","title":"Building Search for Agentic AI: From Data to Expertise","text":"<p>[TODO: Image Idea]</p> <p>Create a hero image that tells a story: An AI agent is shown a simple, closed book. Then, the agent is shown touching a vast, glowing library of data, transforming and becoming more capable. This visually represents the agent gaining expertise.</p> <p>[TODO: Start with a powerful, relatable story to set the stage.]</p> <p>Begin with a scenario: \"Our new AI sales assistant was brilliant... until a customer asked about a product launched last week. The agent, its knowledge frozen in time, could only apologize. It was a stark reminder: an agent's intelligence is useless if its knowledge is incomplete.\"</p> <p>Foundational LLMs are not domain knowledge expertise. Out of the box, LLMs would know deep technical details about your products or data. So we ground them using system like RAG. But what if RAG is not perfect ? </p> <p>Say, you are looking to buy a new Laptop or Iphone. In a e-commorce website and use specific keywords to get latest information. Say, you are young project manager dealing with an advanced project deadline. You will use intent based search to find relevant articles to help you.</p> <p>In the shopping examples, use of Keyword based search is faster. The Traditional SQL Databases have a long history of addressing this specific challenge. In the Project Mangement example, use of new trending Vector Search is the best way go. The Vector Search converts data into high-dimensional vectors to find the closest matches.</p> <p>To build the next generation AI Agents, you have 3 options.</p> <ol> <li>Traditional RDBMS LookUp</li> <li>Vector Search</li> <li>(New) Hybrid Search</li> </ol> <p>This story isn't unique. It's the central challenge in creating AI that can solve real-world problems. An AI agent's power comes not from what it knows, but from what it can access. This post is a journey through the process of building a search capability that turns your agent into a true domain expert.</p>","tags":["Agentic AI","RAG","Vector Search","Architecture","Business Value"]},{"location":"blog/posts/building-search-for-agentic-ai/#the-first-step-the-reliability-of-structured-data","title":"The First Step: The Reliability of Structured Data","text":"<p>When an agent needs a precise, unambiguous answer, there is no substitute for a structured database. Imagine needing to know the price of a specific product SKU. You don't want a conversational answer; you want a number.</p> <p>This is where traditional databases like Cloud SQL excel. They are the bedrock of reliable information.</p> <p>[TODO: Show, don't just tell. Add a code block.]</p> <p>Provide a clear, commented Python snippet. The code should demonstrate the agent receiving a query (e.g., \"What's the stock level for 'SKU-12345'?\"), connecting to a Cloud SQL database, and fetching the exact integer. This illustrates the power of precise, structured lookups.</p> <p>But what happens when the questions aren't so precise? What if a user asks, \"Which of your products are good for outdoor use?\" The answer isn't in a single row or column.</p>","tags":["Agentic AI","RAG","Vector Search","Architecture","Business Value"]},{"location":"blog/posts/building-search-for-agentic-ai/#the-leap-into-understanding-the-magic-of-semantic-search","title":"The Leap into Understanding: The Magic of Semantic Search","text":"<p>This is where we move beyond exact matches and into the realm of semantic understanding. To find answers to conceptual questions, the agent needs to understand the meaning behind the words.</p> <p>This is the role of Vector Search.</p> <p>[TODO: Explain embeddings with a simple, powerful analogy.]</p> <p>Describe embeddings in an accessible way. For example: \"Think of embeddings as a universal translator for concepts. They turn everything\u2014words, sentences, images, documents\u2014into a set of coordinates, placing similar concepts close together in a vast 'idea space'.\"</p> <p>[TODO: Image Idea]</p> <p>Create a diagram illustrating this \"idea space.\" Show the word \"hiking\" as a point. Near it, show points for \"waterproof jacket,\" \"trail boots,\" and a product document for an all-weather backpack. Far away from it, show a point for \"formal dinnerware.\" This makes the concept of semantic closeness tangible.</p> <p>[TODO: Add a developer-focused implementation step.]</p> <p>Provide a Python code snippet that shows how to take a user's query (\"durable gear for mountain trips\"), use a Gemini model to create an embedding for it, and then use Vertex AI Vector Search to find the most relevant documents or products.</p>","tags":["Agentic AI","RAG","Vector Search","Architecture","Business Value"]},{"location":"blog/posts/building-search-for-agentic-ai/#unifying-the-worlds-the-power-of-a-hybrid-strategy","title":"Unifying the Worlds: The Power of a Hybrid Strategy","text":"<p>Relying only on vector search can sometimes miss critical details. A user might search for \"the 'TrekMaster' backpack,\" and a pure semantic search could overlook the exact product match in favor of conceptually similar items.</p> <p>The most powerful solution combines the precision of keyword search with the conceptual reach of vector search. This is Hybrid Search.</p> <p>[TODO: Image Idea]</p> <p>Create a flowchart that visually explains the hybrid process. 1. A user query enters the system. 2. The flow splits into two: \"Keyword Search\" (for exact terms like 'TrekMaster') and \"Vector Search\" (for the concept 'durable backpack for mountains'). 3. The results from both are fed into a \"Re-ranking Engine.\" 4. The final, unified list of results is passed to the agent.</p> <p>[TODO: Explain the business value unlocked by this.]</p> <p>Connect this technical solution to a business outcome. Write a paragraph explaining how this hybrid approach leads to a dramatically better user experience. The agent can now answer complex, multi-faceted questions, leading to higher customer satisfaction, increased conversion rates, and a real return on investment. It's not just better tech; it's a better business.</p>","tags":["Agentic AI","RAG","Vector Search","Architecture","Business Value"]},{"location":"blog/posts/building-search-for-agentic-ai/#conclusion-an-agent-with-expertise","title":"Conclusion: An Agent with Expertise","text":"<p>We've journeyed from a simple, structured lookup to a sophisticated hybrid system. By giving our agent the ability to access and synthesize information from multiple sources, we've transformed it from a clever tool into an expert partner.</p> <p>The goal isn't just to build an AI that can answer questions. It's to build an AI that can solve problems, drive decisions, and create value. And that begins with search.</p> <p>[TODO: Add links to relevant codelabs or documentation.]</p> <ul> <li>Get started with the Vertex AI Vector Search Codelab</li> <li>Read the official Hybrid Search documentation</li> </ul>","tags":["Agentic AI","RAG","Vector Search","Architecture","Business Value"]},{"location":"blog/posts/building-search-for-agentic-ai/#lets-connect","title":"Let's Connect!","text":"<ul> <li>\ud83d\udc26 Share your thoughts on Twitter/X</li> <li>\ud83d\udcbc Connect on LinkedIn</li> <li>\ud83c\udf1f Star the project on GitHub</li> </ul>","tags":["Agentic AI","RAG","Vector Search","Architecture","Business Value"]},{"location":"blog/posts/conductor-gemini-cli/","title":"Give Your AI a Brain: Context-Driven Development with Conductor for Gemini CLI","text":"<p>Stop coding in \"YOLO Mode\". Learn how to give Gemini CLI the full context of your project for better, more accurate results</p> <p>[Header Image Idea: A split screen. Left side: A messy desk with loose post-it notes labeled \"YOLO Mode\". Right side: A neat desk with a structured binder labeled \"Conductor - CDD\".]</p> <p>\"Failing to plan, is planning to fail.\"</p> <p>Software development isn't just typing code. It\u2019s planning, tracking, executing, testing, and iterating. For large corporate projects, we have heavyweights like Jira. But what about pet projects, POCs, or solo features? We are often stuck in the middle ground\u2014Jira is overkill, but having zero structure leaves us vulnerable to scope creep and losing direction.</p> <p>Enter the era of AI-assisted coding. If we humans need structure to stay focused, our AI tools need it even more. If you\u2019re just firing random prompts at Gemini CLI, you are coding in \"YOLO Mode.\" The AI has no memory of what you asked five minutes ago and no idea what the end goal is.</p> <p>How do we bring structured planning to the \"AI Way\" of doing things?</p>"},{"location":"blog/posts/conductor-gemini-cli/#introducing-conductor-for-gemini-cli","title":"Introducing Conductor for Gemini CLI","text":"<p>Conductor is an open-source, optional extension for the Gemini CLI. It bridges the gap between unstructured prompting and full-blown project management tools.</p> <p>You can think of Conductor as a lightweight project manager that lives directly inside your terminal and repository. It introduces Context-Driven Development (CDD) to the Gemini CLI experience.</p> <ul> <li>Website: Google Developers Blog Introduction</li> <li>Repo: GitHub (Apache-2.0 License)</li> </ul>"},{"location":"blog/posts/conductor-gemini-cli/#the-why-context-is-king","title":"The \"Why\": Context is King","text":"<p>Why should you care about another tool? Because context is what separates a generic code snippet from a perfectly integrated feature.</p> <p>Conductor adds a strict protocol used to specify, plan, and implement software features. By maintaining files in your repository that define the product guidelines, tech stack, and current feature specifications, Conductor ensures that every prompt you send to Gemini is automatically enriched with the bigger picture.</p> <p>It stops the AI from guessing your tech stack and forces it to adhere to your agreed-upon plan.</p>"},{"location":"blog/posts/conductor-gemini-cli/#the-how-a-practical-walkthrough","title":"The \"How\": A Practical Walkthrough","text":"<p>Let's look at how Conductor actually works on a local machine.</p>"},{"location":"blog/posts/conductor-gemini-cli/#step-1-installation","title":"Step 1: Installation","text":"<p>Installing Conductor is as simple as adding any other Gemini extension.</p> <pre><code>gemini extensions install https://github.com/gemini-cli-extensions/conductor\n\n</code></pre>"},{"location":"blog/posts/conductor-gemini-cli/#step-2-initialization-giving-the-ai-context","title":"Step 2: Initialization (Giving the AI Context)","text":"<p>Just like you <code>git init</code> a repo, you need to initialize Conductor in your project root.</p> <p>(Recommended Screenshot/GIF: Run this command and show the output indicating files were created)</p> <pre><code># Start a Gemini session\ngemini\n\n# Inside the session run:\nconductor init\n</code></pre> <p>As you do this, Gemini CLI works to fulfill the requirements and creates a new folder named <code>conductor/</code> populated with guideline files.</p> <p>(Recommended Screenshot: Show your iTerm file tree showing this structure)</p> <pre><code>conductor/\n\u251c\u2500\u2500 product-guidelines.md  &lt;-- How we build things here\n\u251c\u2500\u2500 product.md             &lt;-- What this product is\n\u251c\u2500\u2500 tech-stack.md          &lt;-- The tools we use (React, Python, etc.)\n\u2514\u2500\u2500 tracks/                &lt;-- Where active work lives\n\n</code></pre> <p>These files are accessible by both you and Gemini. You should fill out <code>tech-stack.md</code> and <code>product.md</code> first. This is the baseline context Gemini will now use for everything in this project.</p>"},{"location":"blog/posts/conductor-gemini-cli/#step-3-the-workflow-tracks-specs-and-plans","title":"Step 3: The Workflow (Tracks, Specs, and Plans)","text":"<p>Conductor organizes work into \"Tracks\" (like a feature branch or a Jira ticket).</p> <p>1. Create a Track Let's say we want to add a chatbot feature.</p> <p>(Recommended Screenshot: The output of creating a track)</p> <pre><code># Setup conductor\n# Note: Conductor will create a folder structure for you.\n/conductor:setup\n\n# Create a new track.\n# Example: To create blog post on Quantum Physics\n# Note: Conductor will ask a few follow-up questions to create a plan and an implementation track\n/conductor:newTrack create blog post on Quantum Physics\n\n# Check all open tracks for this project.\n/conductor:status\n\n# To start work on all open tracks for this project.\n/conductor:implement\n</code></pre> <p>This creates a new folder structure under <code>conductor/tracks/chatbot_feature</code> containing a <code>spec</code> file.</p> <p>2. Define the Spec You (the human) edit the <code>spec</code> file in that folder to define what you want. You don't say how to code it, just define the requirements.</p> <p>3. Generate the Plan This is where the magic happens. You ask Conductor to look at your spec, your tech stack, and your product guidelines, and generate a step-by-step implementation plan.</p> <p>(Recommended Video Clip: Record the terminal as Gemini generates the plan. It's satisfying to watch it list the steps.)</p> <pre><code># Inside gemini session, ensuring you have the track selected\nconductor plans generate\n\n</code></pre> <p>Gemini will generate a detailed <code>plan.md</code> file listing numbered tasks required to build your spec.</p> <p>4. Execute the Tasks Now, instead of randomly prompting, you tell Conductor to execute a specific step from the plan it just created.</p> <pre><code># Inside gemini session\nconductor tasks execute 1\n\n</code></pre> <p>Gemini now has the ultimate context: It knows the overall product goal, the specific feature spec, the exact step in the plan it needs to do, and the tech stack rules. It will write the code, and then update the plan to mark task #1 as complete.</p>"},{"location":"blog/posts/conductor-gemini-cli/#conclusion","title":"Conclusion","text":"<p>If you enjoy working with Gemini CLI, Conductor sharpens its focus significantly. It turns an AI coding assistant into an AI development partner that understands the \"definition of done.\"</p> <p>Stop YOLO prompting. Give your AI some context.</p>"},{"location":"blog/posts/conductor-gemini-cli/#part-2-the-youtube-video-script","title":"Part 2: The YouTube Video Script","text":"<p>Video Concept: A fast-paced, \"over-the-shoulder\" look at using Conductor. The tone should be enthusiastic and practical. Estimated Duration: 3-5 minutes.</p> Time Visual Idea Script (Voiceover/Host) 0:00 Face to camera (or engaging terminal intro screen). Text overlay: \"AI Coding: YOLO Mode vs. God Mode\" Are you just throwing random prompts at your terminal hoping for working code? That\u2019s coding in YOLO mode. Your AI has no memory and no context. Today, we\u2019re fixing that. 0:20 Screen share: Google Blog Post about Conductor. Highlight \"Context-Driven Development\". Google recently introduced \"Conductor\" for the Gemini CLI. It introduces \"Context-Driven Development.\" 0:35 Graphic: Simple diagram showing: YOU -&gt; (Prompts) -&gt; GEMINI -&gt; (Random Code?). Then change it to: YOU + CONDUCTOR FILES -&gt; (Rich Context) -&gt; GEMINI -&gt; (Structured Features). Think of Conductor as a lightweight project manager that lives right inside your repo. It forces you to define what you want before the AI starts guessing how to write it. 0:55 Screen Share: iTerm on Macbook. Show an empty project folder. Run the install command. Let\u2019s look at it in action on my local machine. Installing it is just a standard Gemini extension command. 1:10 Screen Share: Terminal. Run <code>gemini start</code> then <code>conductor init</code>. Crucial: Zoom in on the file tree updating on the left. Once we start Gemini, we run <code>conductor init</code>. Watch the file tree on the left. Boom. 1:25 Screen Share: Briefly open the generated markdown files in VS Code or Vim. Point out <code>tech-stack.md</code>. It created a <code>conductor</code> folder. This is the AI's brain for your project. You define your <code>tech-stack</code> here once, and Gemini never has to guess again. 1:45 Screen Share: Terminal. Run <code>conductor tracks create new-landing-page</code>. Show the new folders created. Work happens in \"Tracks\". Let's create a track for a new landing page. Conductor sets up the structure for us. 2:05 Screen Share: Briefly show the <code>spec</code> file. Then back to terminal. Run <code>conductor plans generate</code>. Let the recording run while Gemini types out the plan. I\u2019ve put my requirements into the spec file. Now, I\u2019ll ask Conductor to generate the plan. Look at that\u2014it\u2019s reading my spec, my tech stack, and creating a step-by-step checklist. 2:35 Screen Share: Show the generated <code>plan.md</code>. Point to task #1. Back to terminal: run <code>conductor tasks execute 1</code>. We have a plan. Now we just execute. \"Conductor, do task one.\" 2:55 Screen Share: Show Gemini generating the actual code files for task 1. Because it has all that context, it knows exactly which files to create and what stack to use, without me having to re-explain it in the prompt. 3:15 Face to camera. Conductor turns Gemini CLI from a smart autocomplete into an actual junior developer that understands your project's plan. The links to install it are in the description below. Stop YOLO coding, and give your AI some brainpower."},{"location":"blog/posts/documentation-driven-development/","title":"Proposal: Documentation-Driven Development (D3) - A New Pattern for the AI Era","text":"<p>If you have worked in IT for any length of time, you know one thing for sure: user documentation is often seen as a undervalued chore. In the standard Software Development Life Cycle (SDLC) process, documentation is often an afterthought, usually started just before a product release. As an expert in software development, I can confirm that creating human-readable documentation has always been a significant challenge.</p> <p>However, over the last 18 months, LLMs and Agents have shifted this perspective. Slowly but surely documentation is now starting to become a forethought. This change builds on earlier ideas about Documentation-Driven Development (DDD). The core principle of DDD suggests that if an idea is thoroughly documented, then it can evolve into a product. Similarly, a feature must be drafted, to be developed, tested and then deployed; otherwise, it is considered a bug.</p> <p>Given the rising prevalence of Vibe coding and AI-assisted development, documentation has arguably become a central concern. Reflecting on my own experience (a self-evaluation), I pose this question to you: Are you, in essence, practicing Documentation-Driven Development (D3) while coding with AI?</p> <p>Let's discuss this in the comments. Is documentation still a 'boring chore' for your team, or has it become your primary development tool? I\u2019m seeing a massive shift toward D3\u2014are you seeing it too?</p>"},{"location":"blog/posts/documentation-driven-development/#ai-softwareengineering-d3-vibecoding-gemini","title":"AI #SoftwareEngineering #D3 #VibeCoding #Gemini","text":""},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/","title":"Exploring MESOP Web Framework with Google ADK","text":"<p>Are you looking to build AI-powered web applications with a fresh approach?</p> <p>The Google Agent Development Kit (ADK) offers immense potential for creating intelligent agents. But how do you give these agents a compelling web interface? In this post, we'll explore MESOP, a promising new web framework, and demonstrate how to seamlessly integrate it with the Google ADK to bring your AI applications to life.</p>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>\ud83c\udfaf What is MESOP and why is it gaining traction?</li> <li>\ud83d\udca1 How to set up your development environment for MESOP and Google ADK.</li> <li>\ud83d\ude80 Practical steps to build a simple AI agent interface using MESOP with ADK.</li> </ul>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#the-problemcontext-why-should-they-care","title":"The Problem/Context (Why Should They Care?)","text":"<p>Developing web applications for AI agents often involves juggling complex frontend and backend technologies. This can slow down development and make it challenging to iterate quickly on agent interactions. Traditional frameworks might be overkill for rapid prototyping and deployment of agent-centric UIs.</p>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#the-solutionmain-content","title":"The Solution/Main Content","text":"","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#1-introduction-to-mesop-a-new-paradigm-for-web-development","title":"1. Introduction to MESOP: A New Paradigm for Web Development","text":"<p>MESOP aims to simplify web development, especially for Python developers. It provides a component-based approach, allowing you to focus on application logic rather than intricate frontend details.</p> <p>Pro tip: MESOP's Python-first approach can significantly reduce context-switching for teams already proficient in Python.</p> <pre><code># Basic MESOP application structure (example)\nimport mesop as me\n\n@me.page(path=\"/\")\ndef app():\n    me.text(\"Hello from MESOP!\")\n\n</code></pre>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#2-understanding-the-google-agent-development-kit-adk","title":"2. Understanding the Google Agent Development Kit (ADK)","text":"<p>The Google ADK provides tools and libraries to build, deploy, and manage AI agents. It abstracts away much of the complexity of interacting with large language models and other AI services.</p> <p>\ud83d\udcac Synergy: Combining MESOP's intuitive UI capabilities with ADK's powerful agent functionalities creates a robust platform for modern AI applications.</p>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#3-integrating-mesop-with-google-adk-a-step-by-step-guide","title":"3. Integrating MESOP with Google ADK: A Step-by-Step Guide","text":"","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#31-setting-up-your-environment","title":"3.1 Setting Up Your Environment","text":"<p>Ensure you have Python installed and create a virtual environment. Install MESOP and the necessary Google ADK libraries.</p> <pre><code>pip install mesop google-generativeai # Example libraries\n</code></pre>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#32-designing-your-agents-interface-with-mesop","title":"3.2 Designing Your Agent's Interface with MESOP","text":"<p>We'll create a simple chat interface where users can interact with our ADK-powered agent.</p> <p></p> <p>Remember: Every 300-500 words should have a visual element.</p>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#33-connecting-mesop-components-to-adk-agent-logic","title":"3.3 Connecting MESOP Components to ADK Agent Logic","text":"<p>This section will detail how to pass user input from MESOP to your ADK agent and display the agent's responses back in the MESOP UI.</p> <pre><code># Example of connecting MESOP input to ADK agent (conceptual)\n# Assume 'my_adk_agent' is initialized\n# and 'mesop.input' captures user text\n\n# @me.button(on_click=handle_submit)\n# def submit_button():\n#     me.text(\"Send to Agent\")\n\n# def handle_submit(e: me.ClickEvent):\n#     user_query = get_mesop_input_value() # Placeholder for getting input\n#     agent_response = my_adk_agent.send_query(user_query)\n#     display_mesop_output(agent_response) # Placeholder for displaying output\n</code></pre>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#real-world-examplecase-study","title":"Real-World Example/Case Study","text":"<p>!!! example \"Interactive Content Generator\"     Imagine an agent built with ADK that helps content creators brainstorm ideas and generate drafts. Using MESOP, we can build a sleek web interface where users input prompts, and the agent provides structured suggestions, all managed through a simple, Python-native workflow.</p>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":"<ol> <li>Overlooking State Management: With interactive agents, managing conversation state is crucial.</li> <li> <p>Fix: Design clear state management patterns, perhaps leveraging MESOP's reactive capabilities or ADK's session management.</p> </li> <li> <p>Ignoring User Feedback Loops: Agents can be unpredictable.</p> </li> <li> <p>Fix: Implement clear loading indicators, error messages, and opportunities for users to correct or refine agent interactions.</p> </li> <li> <p>Complex Deployments: Combining multiple new technologies can lead to deployment headaches.</p> </li> <li>Fix: Start with simple deployment strategies (e.g., Docker) and gradually introduce more sophisticated CI/CD pipelines.</li> </ol>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#quick-implementation-guide","title":"Quick Implementation Guide","text":"","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#step-1-initialize-a-mesop-project","title":"Step 1: Initialize a MESOP project","text":"<pre><code>mesop create my_agent_app\n</code></pre>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#step-2-integrate-google-adk-client-libraries","title":"Step 2: Integrate Google ADK client libraries","text":"<p>Add <code>google-generativeai</code> (or relevant ADK components) to your <code>requirements.txt</code> and install.</p>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#step-3-develop-your-mesop-ui-for-agent-interaction","title":"Step 3: Develop your MESOP UI for agent interaction","text":"<p>Use <code>me.input</code>, <code>me.button</code>, and <code>me.text</code> (or richer components) to build your interface.</p>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#step-4-connect-ui-events-to-adk-agent-calls","title":"Step 4: Connect UI events to ADK agent calls","text":"<p>Write Python functions to handle user input, call your ADK agent, and update the MESOP UI with responses.</p>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 MESOP Simplifies UI: Build web UIs for your AI agents using a Python-native approach. \u2705 ADK Powers Intelligence: Leverage Google's Agent Development Kit for robust agent capabilities. \u2705 Seamless Integration: Combine these tools for rapid development and deployment of interactive AI applications.</p>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#your-next-action","title":"Your Next Action","text":"<p>\u2192 Explore the MESOP Documentation | Learn more about Google ADK | Start building your first MESOP+ADK app!</p>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#resources-further-reading","title":"Resources &amp; Further Reading","text":"<ul> <li>\ud83d\udcd6 MESOP GitHub Repository</li> <li>\ud83d\udee0\ufe0f Google AI for Developers</li> <li>\ud83d\udcda Introduction to Google ADK</li> </ul>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/exploring-mesop-web-framework-google-adk/#lets-connect","title":"Let's Connect!","text":"<p>Found this helpful? I'd love to hear your thoughts!</p> <ul> <li>\ud83d\udc26 Share your results on Twitter/X</li> <li>\ud83d\udcbc Connect on LinkedIn</li> <li>\ud83c\udf1f Star the project on GitHub</li> </ul>","tags":["MESOP","Google ADK","Web Framework","AI","Python"]},{"location":"blog/posts/gemini-2.5-flash-image-preview/","title":"Gemini 2.5 - A New Era for AI Image Generation","text":""},{"location":"blog/posts/gemini-2.5-flash-image-preview/#gemini-25-flash-vs-gemini-20-flash-a-new-era-for-ai-image-generation","title":"Gemini 2.5 Flash vs. Gemini 2.0 Flash: A New Era for AI Image Generation","text":"<p>Google has once again pushed the boundaries of generative AI with the release of Gemini 2.5 Flash Image (Preview), a significant upgrade to its predecessor, Gemini 2.0 Flash Image. While both models offer impressive image generation capabilities, Gemini 2.5 Flash introduces a suite of new features and enhancements that provide you with unprecedented creative control and higher-quality outputs. In this blog post, we will delve into the key differences between these two models, highlighting the advancements that make Gemini 2.5 Flash a game-changer for creators and developers.</p>"},{"location":"blog/posts/gemini-2.5-flash-image-preview/#key-advancements-in-gemini-25-flash","title":"Key Advancements in Gemini 2.5 Flash","text":"<p>Conversational Editing and Iterative Refinement: One of the most significant improvements in Gemini 2.5 Flash is its ability to engage in conversational editing. This means you can now refine and edit images through a natural language dialogue. Instead of starting from scratch with a new prompt for every minor adjustment, you can provide iterative feedback. For instance, you can ask the model to \"make the sky a deeper blue\" or \"add a dog in the background,\" and it will make the changes while preserving the rest of the image. This feature was more limited in Gemini 2.0, making the creative process in 2.5 much more fluid and intuitive.</p> <p></p> <p>You can find my final image at the link.</p> <p>Multi-Image Fusion and Composition: Gemini 2.5 Flash introduces multi-image fusion, a powerful feature that allows you to combine up to three images to create a single, seamless visual. This opens up a world of creative possibilities, from placing a product in a new environment to creating surrealist art. While Gemini 2.0 had some capabilities in this area, 2.5 takes it a step further with more sophisticated blending and composition, resulting in more photorealistic and coherent images.</p> <p></p> <p>Character and Style Consistency: Maintaining a consistent character or style across multiple images has always been a challenge for AI image generators. Gemini 2.5 Flash addresses this with enhanced character and style consistency. You can now generate images of the same character in different scenes, outfits, or poses, and the model will maintain their key features and appearance. This is a massive leap forward for storytelling, branding, and any application that requires a consistent visual identity.</p> <p>Improved Visual Quality(Aspect ratio) and Text Rendering: Gemini 2.5 Flash boasts a noticeable improvement in overall visual quality. The images generated are more detailed, vibrant, and photorealistic than those from Gemini 2.0. Furthermore, the model's ability to render text within images has been significantly enhanced. This means you can create posters, invitations, and other designs with clear, legible text, a task that was often a struggle for previous models.</p>"},{"location":"blog/posts/gemini-2.5-flash-image-preview/#_1","title":"Gemini 2.5 - A New Era for AI Image Generation","text":""},{"location":"blog/posts/gemini-2.5-flash-image-preview/#the-verdict-a-leap-forward-in-creative-control","title":"The Verdict: A Leap Forward in Creative Control","text":"<p>While Gemini 2.0 Flash was a powerful tool in its own right, Gemini 2.5 Flash represents a significant evolution in AI image generation. The introduction of conversational editing, multi-image fusion, and enhanced consistency gives creators an unparalleled level of control over the final output. These advancements, combined with the improvements in visual quality and text rendering, make Gemini 2.5 Flash an indispensable tool for anyone looking to push the boundaries of their creativity. As AI-powered image generation continues to evolve, Gemini 2.5 Flash has set a new standard for what's possible.</p>"},{"location":"blog/posts/gemini-2.5-flash-image-preview/#links","title":"Links:","text":"<ul> <li>Gemini 2.5 Flash Image Preview</li> <li>Gemini 2.0 Flash Image</li> </ul>"},{"location":"blog/posts/gemini-as-your-culinary-guide/","title":"Gemini: Your Culinary Guide in Italy \ud83c\uddee\ud83c\uddf9 \ud83c\udf55","text":"<p>Savoring Italy, One Personalized Recommendation at a Time</p> <p>Last week I had a wonderful trip to Bologna, Italy. Since I don\u2019t speak italian \ud83c\uddee\ud83c\uddf9my biggest concern was how do I ask for Indian vegetarian food ? Indian Vegetarian also known as lato-vegeterian. In short, they are vegans who also eat any milk products.</p> <p>Usually my trips are planned ahead of time, but this trip was n\u2019t. Luckily the Gemini proved invaluable. Instead of manually searching for answers to various inquiries, I leveraged its capabilities for real-time recommendations.</p> <p>To Gemini, I'd provide restaurant details I am in, and it would swiftly analyze reviews to offer a curated list of top choices based on my criteria. I visited 3-4 restaurants, and by maintaining the same chat thread, Gemini developed a clear understanding of my preferences, ensuring each recommendation was highly tailored.</p> <p>With Gemini assistance, I am looking forward to visiting more Italian restaurants \ud83c\uddee\ud83c\uddf9\ud83c\udf55\ud83d\ude0b #LLM #AI #Gemini #EverydayTech #TravelTips #Innovation</p>"},{"location":"blog/posts/gemini-cli-cheatsheet-printouts/","title":"Gemini CLI Cheatsheet Printout","text":"<p>Google's Gemini models are a powerful family of multimodal AI models. For developers and power users who live in the terminal, <code>gcloud</code> provides a robust command-line interface to interact with Gemini on Vertex AI.</p> <p>To help you keep the most common commands at your fingertips, I've created a handy CLI cheatsheet available in both light and dark themes. Download your preferred version and get started!</p>"},{"location":"blog/posts/gemini-cli-cheatsheet-printouts/#light-theme","title":"Light Theme","text":"<p>Download Links: PNG &amp; PDF.</p>"},{"location":"blog/posts/gemini-cli-cheatsheet-printouts/#dark-theme","title":"Dark Theme","text":"<p>Download Links: PNG &amp; PDF.</p>"},{"location":"blog/posts/gemini-cli-cheatsheet-printouts/#getting-started","title":"Getting Started","text":"<p>Before you can use these commands, you need to set up your local environment and Google Cloud project. Follow the Gemini Cookbook Setup guide to get everything configured correctly.</p> <p>Happy coding!</p>"},{"location":"blog/posts/gemini-cli-top-commands/","title":"Master Your Workflow: Top Gemini CLI Commands You Should Know","text":""},{"location":"blog/posts/gemini-cli-top-commands/#master-your-workflow-top-gemini-cli-commands-you-should-know","title":"Master Your Workflow: Top Gemini CLI Commands You Should Know","text":"<p>Picture this: you're deep in your coding workflow. You need to refactor a complex function, write a clear and concise commit message for your latest changes, and then figure out an obscure shell command to deploy your work. Each task requires switching context\u2014moving from your editor to a browser, then to your Git client, and back again. It's a constant juggling act that breaks your focus.</p> <p>What if you could do all of this from one place, your terminal? The Gemini CLI brings the power of Google's state-of-the-art AI directly to your command line, acting as a seamless, context-aware pair programmer. It's designed to understand your project, streamline your tasks, and keep you in the flow.</p>"},{"location":"blog/posts/gemini-cli-top-commands/#getting-started-your-first-steps","title":"\ud83d\ude80 Getting Started: Your First Steps","text":"<p>Before you can master your workflow, you need to get set up and oriented. These commands are the foundation.</p> <ul> <li> <p>/init \ud83c\udfac: This is where your journey should begin. Running <code>/init</code> in your project directory allows Gemini to analyze your codebase and create a <code>GEMINI.md</code> file.</p> </li> <li> <p>/about \u2139\ufe0f: Curious about your setup? This command quickly displays your Gemini CLI version, the underlying model being used, and your current authentication method.</p> </li> <li> <p>/auth \ud83d\udd11: Security and access are paramount. The <code>/auth</code> command lets you configure how you authenticate with Google's AI services.</p> </li> <li> <p>/help &amp; /docs \ud83d\udcda: The <code>/help</code> command is your go-to for a quick overview. For more details, <code>/docs</code> opens the full documentation in your browser.</p> </li> <li> <p>/quit \ud83d\udc4b: When you're done for the day, simply use <code>/quit</code> to exit the Gemini CLI application.</p> </li> </ul>"},{"location":"blog/posts/gemini-cli-top-commands/#core-workflow-commands","title":"\ud83d\udee0\ufe0f Core Workflow Commands","text":"<p>These are the commands you'll use day-to-day to interact with Gemini and your codebase.</p> <ul> <li> <p><code>!</code> (Shell Commands) \ud83d\udc1a: This is one of the most powerful features. Press <code>SHIFT + 1</code> to enter \"shell mode,\" allowing you to execute shell commands or describe them in natural language.</p> </li> <li> <p>/tools \ud83e\uddf0: Ever wonder what capabilities Gemini has? The <code>/tools</code> command lists all available tools Gemini can use to help you.</p> </li> <li> <p>/editor \u270d\ufe0f: Pro-Tip: For complex, multi-line prompts, use <code>/editor</code> to set your preferred external editor (like Vim or VS Code), then use <code>Ctrl+X</code> to open it.</p> </li> </ul>"},{"location":"blog/posts/gemini-cli-top-commands/#managing-your-conversations","title":"\ud83d\udcac Managing Your Conversations","text":"<p>A conversation with Gemini is a valuable asset. Here's how to manage it effectively.</p> <ul> <li> <p>/chat (save, resume, list, delete, share) \ud83d\udcbe: The <code>/chat</code> command is a powerful tool for managing your session history. <code>save</code> a conversation, <code>resume</code> it later, or <code>share</code> it as a file.</p> </li> <li> <p>/clear \ud83e\uddf9: Need a fresh start? <code>/clear</code> will wipe the screen and your current conversation history.</p> </li> <li> <p>/compress \ud83e\udde0: When a conversation gets long, <code>/compress</code> intelligently summarizes the context to keep things focused.</p> </li> </ul>"},{"location":"blog/posts/gemini-cli-top-commands/#customization-and-stats","title":"\ud83c\udfa8 Customization and Stats","text":"<p>Tailor the CLI to your liking and keep an eye on your usage.</p> <ul> <li> <p>/theme \ud83d\udd8c\ufe0f: Customize your experience. The <code>/theme</code> command allows you to change the look and feel of the Gemini CLI.</p> </li> <li> <p>/stats \ud83d\udcca: Curious about your usage? <code>/stats</code> provides statistics for your current session, including model and tool usage.</p> </li> </ul>"},{"location":"blog/posts/gemini-cli-top-commands/#gemini-cli-keyboard-shortcuts-you-should-know","title":"\u2328\ufe0f Gemini CLI Keyboard Shortcuts You Should Know","text":"<p>To work even faster, it's worth learning a few essential keyboard shortcuts.</p> <ul> <li><code>ESC</code>: Cancel a long-running task or clear your input.</li> <li><code>Ctrl+C</code>: Quit the application (press twice).</li> <li><code>Ctrl+X</code> / <code>Meta+Enter</code>: Open your input in an external editor.</li> <li><code>Enter</code>: Send your query to Gemini.</li> </ul>"},{"location":"blog/posts/gemini-cli-top-commands/#common-use-cases-to-try-today","title":"\u2728 Common Use Cases to Try Today","text":"<p>Here are a few practical ideas to get you started:</p> <ul> <li>Quick Refactoring: Use <code>@</code> to provide a file as context (e.g., <code>@src/utils.py</code>) and ask Gemini: \"Refactor the <code>calculate_total</code> function in this file to be more readable.\"</li> <li>Commit Message Generation: Run <code>!git diff --staged</code> and then ask Gemini: \"Based on the diff, write a conventional commit message.\"</li> <li>Command-Line Coach: Paste a complex shell command and ask: <code>tar -xzvf archive.tar.gz</code> \"What do each of these flags do?\"</li> </ul> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 &gt;   tar -xzvf archive.tar.gz what does each flag do                                                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\u2726 Of course. The command tar -xzvf archive.tar.gz is used to decompress and extract files from a gzipped tar archive.\n\n  Here is a breakdown of what each flag does:\n\n   * -x: eXtract. This tells tar to get files out of the archive.\n   * -z: gZip. This tells tar to decompress the archive using gzip. This is necessary for files ending in .gz.\n   * -v: Verbose. This makes tar list each file as it is being extracted, so you can see the progress.\n   * -f: File. This flag must be followed by the name of the archive file you want to process (in this case, archive.tar.gz).\n\n  You can think of the command as saying: \"eXtract the gZipped contents Verbosely from the File named archive.tar.gz.\"\n</code></pre>"},{"location":"blog/posts/gemini-cli-top-commands/#takeaway-youre-one-step-ahead","title":"\ud83c\udf89 Takeaway: You're One Step Ahead!","text":"<p>Congratulations on making it through this guide! By familiarizing yourself with these commands and workflows, you're already one step ahead of many developers who might not be tapping into the full potential of having an AI assistant directly in their terminal.</p> <p>Thank you for investing the time to level up your skills. We encourage you to start integrating these commands into your daily routine. You'll be surprised at how much time you can save and how much more you can accomplish.</p> <p>Now we'd love to hear from you! What new command did you learn that you're most excited to try? Share your thoughts in the comments below!</p>"},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/","title":"Kubernetes for AI: Introducing Gas Town &amp; The MEOW Stack","text":"<p>Are you drowning in the complexity of managing multiple AI agents? If you've scaled beyond single-agent interactions with tools like Claude Code or Gemini CLI, you know the challenge: coordination chaos. What if there was a 'Kubernetes for AI' that brought order to this new world? Enter Gas Town.</p>","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>\ud83c\udfaf  Grasp the \"Context Wall\": Understand the core challenge of managing state in multi-agent AI systems.</li> <li>\ud83d\udca1  Discover Gas Town: See how this \"Kubernetes for AI\" orchestrates complex agentic workflows.</li> <li>\ud83d\ude80  Unpack the MEOW Stack: Dive into Beads, Molecules, and GUPP \u2013 the architecture enabling persistent AI work.</li> <li>\ud83e\uddd1\u200d\ud83d\udcbb  Meet the Gas Town Crew: Learn about the specialized roles like the Mayor, Polecats, and Refinery.</li> <li>\ud83d\udcc8  Shift Your Paradigm: Move from simple AI chat to orchestrating powerful AI agent factories for maximum throughput.</li> </ul>","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#the-problem-the-context-wall","title":"The Problem: The \"Context Wall\"","text":"<p>You've probably felt it: the frustration when your AI agent's context window overflows or a session inexplicably crashes. All that precious \"state\" of your work? Gone. If you're ambitious enough to run 5 or 10 agents concurrently, you'll quickly discover a new challenge: \"yak shaving\" \u2013 more time spent on manual coordination than actual coding. This is the Context Wall.</p>","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#the-solution-the-meow-stack","title":"The Solution: The MEOW Stack","text":"<p>Forget fragile chat sessions. Gas Town\u2019s power comes from a radical new architecture: the MEOW stack (Molecular Expression of Work). This isn't just a wrapper; it's a fundamental shift designed to bring persistence and coordination to your AI agent factory. Here\u2019s how it works:</p> <ul> <li>Beads: The Persistent Unit of Work. Imagine a Git-backed issue tracker where every task is a \"Bead\"\u2014a persistent JSON object. If an agent crashes, the work doesn't disappear; it lives in Git, ready to be picked up.</li> <li>Molecules: Orchestrating Complex Workflows. Instead of vague commands like \"build this app,\" you provide a \"Molecule\"\u2014a sequenced chain of interconnected Beads. Think of it as a blueprint for autonomous workflows (e.g., Design \u2192 Plan \u2192 Implement \u2192 Review \u2192 Test).</li> <li>GUPP (Universal Propulsion Principle): The Engine of Autonomy. This simple but powerful rule states: If an agent has work on its \"hook,\" it must run it. This principle empowers agents to work autonomously, even across sessions, driving your projects forward relentlessly.</li> </ul>","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#the-crew-whos-doing-the-work","title":"The \"Crew\": Who\u2019s Doing the Work?","text":"<p>In Gas Town, you're not juggling individual chats; you're orchestrating a specialized team. This isn't a flat structure; it's a dynamic hierarchy of roles, each with a crucial mission:</p> <ol> <li>The Mayor (Your Chief of Staff): The central point of contact. You interact directly with the Mayor to initiate large-scale projects and define high-level goals.</li> <li>Polecats (The Temporary Task Force): These are the agile, specialized agents. They spin up for a specific coding task, deliver a Merge Request, and then gracefully disappear, minimizing resource consumption.</li> <li>The Refinery (The Merge Manager): Imagine an agent solely dedicated to maintaining code integrity. The Refinery intelligently manages the \"Merge Queue,\" preventing conflicts and ensuring your army of agents doesn't step on each other's toes.</li> <li>The Witness &amp; Deacon (The Patrol Agents): These vigilant agents are the guardians of productivity. They patrol the system, ensuring agents remain \"awake\" and responsive, preventing them from getting stuck in endless loops.</li> </ol>","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#why-should-you-care-the-empire-builders-edge","title":"Why Should You Care? The Empire Builder's Edge.","text":"<p>If you're already wrangling a multitude of agents and local models, you've hit what Yegge calls \"Stage 7.\" You're at the absolute frontier of what's manually manageable. This is where Gas Town becomes your strategic advantage.</p> <p>Gas Town isn't about incremental gains; it's engineered for massive throughput. It empowers you to \"vibe code\"\u2014to transcend the limitations of typing line-by-line and instead act as a visionary Product Manager. Imagine slinging high-level designs to an entire factory of autonomous agents, rapidly bringing complex visions to life. This is the future of scaling AI development, and it's your path to building an empire.</p>","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#the-current-state-frontier-technology","title":"The Current State: Frontier Technology","text":"<p>Gas Town is cutting-edge, and like all frontier technology, it comes with its own set of characteristics:</p> <ul> <li>UI: Tmux-Powered Control. Currently, the interface is driven by tmux, offering a powerful, if raw, command-line experience. This is where the magic happens.</li> <li>Cost: An Investment in Scale. Yes, it's token-intensive; keeping 10-30 agents running continuously means slurping up tokens. But view this as an investment in unparalleled productivity and autonomous scaling.</li> <li>Status: Rapid Evolution. It\u2019s incredibly early\u2014\"vibe coded\" and less than a month old. Yet, its nascent stage underscores its disruptive potential, marking the definitive shift from merely chatting with AI to intelligently orchestrating entire AI factories.</li> </ul>","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#_1","title":"Kubernetes for AI: Introducing Gas Town & The MEOW Stack","text":"","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#_2","title":"Kubernetes for AI: Introducing Gas Town & The MEOW Stack","text":"","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#_3","title":"Kubernetes for AI: Introducing Gas Town & The MEOW Stack","text":"","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#_4","title":"Kubernetes for AI: Introducing Gas Town & The MEOW Stack","text":"","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#_5","title":"Kubernetes for AI: Introducing Gas Town & The MEOW Stack","text":"","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#_6","title":"Kubernetes for AI: Introducing Gas Town & The MEOW Stack","text":"","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#_7","title":"Kubernetes for AI: Introducing Gas Town & The MEOW Stack","text":"","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/kubernetes-for-ai-gas-town-meow-stack/#the-empires-foundation-a-new-era-of-ai-development","title":"The Empire's Foundation: A New Era of AI Development","text":"<p>The future of AI development isn't merely about a better chatbot. It's about architecting a persistent, Git-backed infrastructure\u2014a true factory\u2014where dozens, even hundreds, of specialized agents can seamlessly collaborate on complex workflows. This is the era where human hand-holding gives way to autonomous orchestration, and the scale of what's possible with AI expands exponentially. This is the foundation upon which your AI empire will be built.</p>","tags":["AI agents","Kubernetes","MEOW Stack","Gas Town","AI workflow","Developer tools"]},{"location":"blog/posts/miyamoto-musashi-10-principles/","title":"10 principles of Miyamoto Musashi","text":"<p>??? About Miyamoto Musashi</p> <pre><code>Miyamoto Musashi, a legendary figure in the annals of Japanese history, stands as a symbol of unparalleled mastery in the way of the sword. Renowned not only for his exceptional martial skills but also for his profound philosophical insights, Musashi is a name that resonates through the ages. Born in the late 16th century, during a tumultuous period of Japan\u2019s history, Musashi\u2019s life was marked by a relentless pursuit of perfection in the art of swordsmanship. His remarkable journey led him to become the author of the timeless classic, \u201cThe Book of Five Rings,\u201d which continues to inspire warriors and thinkers alike, transcending both time and culture. This introductory glimpse into the life and legacy of Miyamoto Musashi only scratches the surface of his remarkable story, a tale of discipline, relentless ambition, and the quest for martial and spiritual enlightenment.\n</code></pre> <ol> <li>Accept everything just the way it is.</li> <li>Do not seek pleasure for its own sake.</li> <li>Do not under any circumstances, depend on a partial feeling.</li> <li>Think lightly of yourself and deeply of the world.</li> <li>Be detached from desire your whole life long.</li> <li>Do not regret what you have done.</li> <li>Never be jealous.</li> <li>Never let yourself be surrounded by a separation.</li> <li>Resentment and complaint are appropriate neither for oneself nor others.</li> <li>Do not let yourself be guided by the feeling of lust or love.</li> </ol> <p>Source: Link</p>"},{"location":"blog/posts/poem-sky-sea-truth/","title":"Sky, Sea and Truth","text":"<p>A vast blue canvas, Sea and sky, a mirror's gleam, Moments, swiftly pass.</p> <p>Life's a rushing stream, We stand still, in silent thought, Seeds grow, futures bloom.</p> <p>Birth and death entwined, Happiness, a passing shade, Truth, a fleeting breath.</p> <p>Sea and sky reflect, Constant change, yet ever the same, The now, the only truth.</p>"},{"location":"blog/posts/quantum-ai-era/","title":"The Quantum Leap in AI: Faster, Greener, and More Powerful","text":""},{"location":"blog/posts/quantum-ai-era/#the-quantum-leap-in-ai-faster-greener-and-more-powerful","title":"The Quantum Leap in AI: Faster, Greener, and More Powerful","text":"<p>The world is buzzing with the incredible power of Artificial Intelligence. From writing emails to creating stunning images, AI models like Gemini are transforming our digital lives. But this explosion in AI capability comes with a cost \u2013 a voracious appetite for energy and computational power that is pushing the boundaries of our current technology. What if there was a way to not only meet these demands but to unlock a new level of intelligence altogether?</p> <p>Enter the strange and wonderful world of quantum computing. It\u2019s a field that sounds like science fiction, but it holds the potential to revolutionize everything we know about computation. This isn't just about faster computers; it's about a fundamentally new way of processing information. In this post, we'll explore the exciting synergies between quantum computing and AI, and how this powerful partnership could lead to a future with faster, greener, and more powerful artificial intelligence.</p>"},{"location":"blog/posts/quantum-ai-era/#the-ai-race-and-its-hurdles","title":"The AI Race and Its Hurdles","text":"<p>The artificial intelligence landscape is evolving at an unprecedented pace. Large Language Models (LLMs) and generative AI are no longer concepts from sci-fi movies; they are here, assisting us, creating for us, and even conversing with us. This rapid advancement, however, comes with significant challenges. Training these sophisticated AI models requires immense computational power, often running on vast data centers filled with thousands of powerful Graphics Processing Units (GPUs). This process consumes staggering amounts of energy, contributing to growing concerns about environmental impact and sustainability. Beyond energy, there's also the sheer computational hurdle: for certain incredibly complex problems, even the most powerful classical supercomputers struggle to find solutions within practical timeframes. The sheer scale and complexity of the data involved can push classical computing architectures to their limits.</p>"},{"location":"blog/posts/quantum-ai-era/#quantum-computing-to-the-rescue","title":"Quantum Computing to the Rescue","text":"<p>So, what is this \"quantum computing\" that promises to be a game-changer? Forget the bits (0s and 1s) of classical computers. Quantum computers use \"qubits,\" which can represent both 0 and 1 simultaneously, thanks to phenomena like superposition and entanglement. This allows quantum computers to process vast amounts of information in parallel and explore many possibilities at once. While still in its early stages, this technology has the potential to solve certain types of problems that are practically impossible for even the fastest conventional supercomputers.</p> <p>How does this apply to AI? Imagine an AI trying to sift through an astronomical number of possibilities to find the optimal solution \u2013 be it designing a new drug molecule, optimizing a global logistics network, or even recognizing complex patterns in chaotic data. A classical computer approaches this by trying options sequentially or through clever heuristics. A quantum computer, by leveraging its unique properties, could potentially explore these possibilities in a fundamentally different and much faster way.</p>"},{"location":"blog/posts/quantum-ai-era/#the-quantum-ai-synergy","title":"The Quantum-AI Synergy","text":"<p>The marriage of quantum computing and AI isn't just about making existing AI faster; it's about making it smarter and more sustainable.</p> <ul> <li> <p>Faster AI: Quantum machine learning (QML) algorithms can accelerate the training of AI models, particularly deep learning networks. Tasks like complex matrix operations and processing vast datasets, which are bottlenecks for classical AI, could be handled with unprecedented speed on quantum processors. This means AI models could learn more efficiently and deliver insights faster.</p> </li> <li> <p>More Powerful AI: Quantum algorithms are particularly adept at solving complex optimization problems. For AI, this translates into more precise predictions, better decision-making in intricate scenarios, and the ability to tackle problems currently deemed intractable. Fields like drug discovery, financial modeling, and materials science, where AI struggles with immense combinatorial complexities, could see revolutionary breakthroughs.</p> </li> <li> <p>Greener AI: Perhaps one of the most compelling aspects is the potential for significant energy savings. While classical AI models consume megawatts of power, quantum computers generally operate at kilowatt levels for their core computations. Although the cryogenic cooling required for many quantum systems does consume energy, the actual information processing is highly efficient. Furthermore, advancements in room-temperature quantum computing are emerging, which could further reduce the overall energy footprint. By performing complex calculations more efficiently, quantum AI could drastically reduce the energy demands associated with training and running the AI models of the future, helping to mitigate the environmental impact of the burgeoning AI industry.</p> </li> </ul>"},{"location":"blog/posts/quantum-ai-era/#gemini-and-the-future","title":"Gemini and the Future","text":"<p>Consider the capabilities of a cutting-edge AI model like Google's Gemini \u2013 capable of multimodal understanding, complex reasoning, and creative generation. Now, imagine a \"Quantum Gemini.\" If Gemini's underlying architecture could be accelerated and enhanced by quantum processing units, its ability to learn from vast datasets, understand nuanced context, and generate highly sophisticated responses could reach new heights. Such a system could process information with unparalleled efficiency, learn from fewer examples, and even explore solution spaces that are currently beyond our grasp. This is not a certainty, but rather a compelling expectation \u2013 a potential path for explorers in both quantum computing and AI to navigate, leading to truly groundbreaking advancements.</p>"},{"location":"blog/posts/quantum-ai-era/#the-dawn-of-a-new-intelligence","title":"The Dawn of a New Intelligence","text":"<p>The journey into the quantum realm of AI is just beginning. What we've discussed today is not a guaranteed roadmap, but a glimpse into a future brimming with possibilities. The synergy between quantum computing and artificial intelligence has the potential to redefine what's possible, making AI not only more powerful and intelligent but also more sustainable and aligned with our planet's needs. As we stand at the crossroads of these two revolutionary technologies, one thing is certain: the future of intelligence\u2014be it human, artificial, or quantum\u2014is set to be an extraordinary adventure.</p>"},{"location":"blog/posts/spot-fakes-leaders/","title":"How to Spot an Ineffective Manager or Leader","text":"<p>Identifying poor leadership is a critical skill for career growth and mental well-being. In many corporate environments, a person's position or title can mask their true leadership capabilities. Ineffective leaders often exploit communication gaps and their perceived authority, creating a toxic environment for their team.</p> <p>Here are three common warning signs.</p>"},{"location":"blog/posts/spot-fakes-leaders/#1-they-stifle-autonomy-and-micromanage","title":"1. They Stifle Autonomy and Micromanage","text":"<p>A classic sign of a bad leader is the tendency to suffocate their team's freedom and autonomy. They take broad, collaborative goals and twist them into high-pressure, individual tasks.</p> <p>For example, a company-wide directive like \"we need to improve application performance\" becomes a direct, isolating order to a single developer: \"Your application's performance is not satisfactory. Fix it.\" What should be a team effort becomes a solitary burden, setting the individual up for failure and burnout.</p>"},{"location":"blog/posts/spot-fakes-leaders/#2-they-withhold-recognition","title":"2. They Withhold Recognition","text":"<p>Ineffective leaders are often the last to acknowledge your hard work and the first to take credit for it. While your peers might celebrate your achievements, a bad manager will either ignore them or offer empty praise that feels disingenuous, often followed by piling on more responsibilities.</p> <p>These managers are deeply concerned with their own reputation. They enjoy the benefits of your team's successes to bolster their own image, but they don't genuinely care about or understand the work you do. This behavior leads to a feeling of being overworked, underappreciated, and overlooked for growth opportunities.</p>"},{"location":"blog/posts/spot-fakes-leaders/#3-they-lack-accountability-and-offer-fake-support","title":"3. They Lack Accountability and Offer Fake Support","text":"<p>Trust is the foundation of any healthy team. Bad leaders erode this trust with a pattern of dishonesty and avoidance. They are quick to make promises they can't or won't keep.</p> <p>They might demand you work weekends to fix a critical issue but will not invest their own time to understand the technology or the root cause of the problem. They are not problem-solvers; they are masters of evasion. This lack of genuine support breaks your confidence in their leadership and leaves you feeling isolated.</p>"},{"location":"blog/posts/spot-fakes-leaders/#what-should-you-do","title":"What Should You Do?","text":"<p>If these signs resonate with your experience, it's essential to take action to protect your career and well-being.</p>"},{"location":"blog/posts/spot-fakes-leaders/#1-prioritize-your-well-being-consider-your-options","title":"1. Prioritize Your Well-being: Consider Your Options","text":"<p>If you feel consistently undermined and unsupported, the best long-term solution may be to find a new role. Ineffective managers are often skilled at applying pressure and may even resort to bad-mouthing team members when things go wrong.</p> <p>Leaving a toxic environment is not a sign of failure. While the thought of changing jobs can be daunting, your mental health and financial future are better served in a supportive workplace. Many people suffer severe burnout and trauma from a lack of support at work. Not everyone has the luxury to quit immediately, but it's crucial to start thinking about a way out.</p>"},{"location":"blog/posts/spot-fakes-leaders/#2-plan-a-strategic-exit","title":"2. Plan a Strategic Exit","text":"<p>Because these managers often lack deep technical expertise, they tend to focus on project-level timelines and deliverables, expecting you to handle all the details. They often lay out work in two- to three-month cycles. Use this time to your advantage. Plan your exit, update your resume, and start networking.</p>"},{"location":"blog/posts/spot-fakes-leaders/#3-document-everything","title":"3. Document Everything","text":"<p>Keep a clear record of your accomplishments, tasks, and any relevant interactions. Use email to confirm conversations and decisions. This documentation is not just for performance reviews; it's a way to protect yourself and maintain a clear, factual account of your contributions.</p> <p>Lastly, don't forget to remember that you're in control of your career path. Don't let the one bad environment dictate your future.</p>"},{"location":"blog/posts/spot-fakes-leaders/#4-stay-positive","title":"4. Stay Positive","text":"<p>Remember, this is not an end; it's a new beginning. You have the power to shape your own destiny and create a better work environment for yourself and others.</p>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/","title":"Python in Google Cloud: More Than Just Examples for Gemini","text":"<p>Is Python the ultimate language for Google Cloud development? Is it Google's official recommendation, or do alternatives like Java, Go, or Rust hold sway in specific scenarios? These are crucial questions for any developer building on Google Cloud, particularly when diving into Generative AI with Gemini. This article offers an opinionated, yet fact-driven, perspective on Python's prominent role, clarifying why you often encounter Python examples in Google Cloud documentation, especially for Gemini features.</p>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/#the-genesis-of-a-preference-pythons-rise-in-my-early-career","title":"The Genesis of a Preference: Python's Rise in My Early Career","text":"<p>My journey into advanced programming and data analysis began long before \"Data Science\" became a household term. Before an internship at Standard Chartered Bank in 2012, my programming toolkit was dominated by C and Java, with Python being more of a personal project companion.</p> <p>The internship presented a unique challenge: sifting through mountains of Unix system health and performance data to predict maintenance needs and identify system improvements. My initial inclination was to leverage Java, given my familiarity. However, my mentor, Asjorn, encouraged me to explore Python. This pivotal decision unveiled Python's unexpected power and laid the groundwork for its consistent presence in my projects.</p> <p>Over the years, Python became the language of choice for almost all data science initiatives I led, driven by several compelling advantages:</p> <ol> <li>Code Readability and Maintainability: Python's clean, intuitive syntax    drastically lowered the learning curve, making code easier to understand,    write, and debug. This is a benefit Google itself recognizes and    inspired Go language    development in certain ways.</li> <li>Efficiency for Lean Teams: Python's conciseness allowed smaller    development teams to achieve significant outcomes, often comparable to what    larger teams using more complex or \"low-code\" solutions might accomplish.    Its extensive ecosystem contributes to rapid development.</li> <li>Prioritizing Availability Over Raw Performance: For many projects, the    immediate availability of services, scripts, and tools proved more critical    than bleeding-edge performance. With compute power becoming increasingly    affordable, Python enabled rapid deployment and iteration. This is    particularly true for scripting and prototyping.</li> <li>\"Batteries Included\" for Data-Intensive Tasks: Python boasts a rich    standard library and a vast ecosystem of third-party packages, particularly    beneficial for data manipulation and analysis. This extensive library    support is a consistent benefit for cloud development.</li> </ol>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/#pythons-enduring-reign-in-data-science-and-cloud-computing","title":"Python's Enduring Reign in Data Science and Cloud Computing","text":"<p>Python's appeal in data science continued to grow for two additional reasons, deeply intertwined with cloud development:</p> <ol> <li>Rapid Prototyping: The interactive interpreter and tools like    Google Colab make Python invaluable for agile    data analytics workflows. This capability is essential for quickly testing    ideas and developing proofs of concept.</li> <li>Robust Community and Specialized Libraries: Public libraries such as    NumPy, Pandas, and Scikit-learn, despite often having performance-critical    core logic written in lower-level languages like C or Fortran, offer    user-friendly Python interfaces. Google taps into Python's extensive library    ecosystem, with machine learning and AI projects benefiting from libraries    like TensorFlow.</li> </ol>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/#decoding-pythons-presence-in-google-clouds-generative-ai-gemini","title":"Decoding Python's Presence in Google Cloud's Generative AI (Gemini)","text":"<p>This brings us to Python's specific role within Google Cloud's Generative AI, particularly with Gemini. You'll find a significant number of Python examples for Gemini features, and this isn't by accident.</p> <p>The primary reason for the prevalence of Python samples is its readability and speed of prototyping. For quickly demonstrating capabilities, enabling developers to get started swiftly, and iterating on ideas, Python excels. The official Google GenAI SDK is indeed available for Python, alongside JavaScript/TypeScript, Go, and Java. The Python GenAI SDK provides an interface to integrate Google's generative models into Python applications, supporting both the Gemini Developer API and Vertex AI APIs.</p> <p>While Python is a widely-used, high-level programming language known for its simplicity and readability, it's crucial to understand the nuance: these Python examples are provided for ease of use and quick experimentation, not as an exclusive recommendation for all production environments. Google Cloud itself supports a variety of popular languages, each suited for different development needs.</p> <p>Lastly, about the absence of official GenAI SDKs for languages like Rust or C# is often a matter of demand and ecosystem focus, though Google Cloud generally supports a broad range of languages for various services. In fact, Google Cloud Service Extension uses a very interesting mix of GO, Java, Python and Web Assembly.</p>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/#beyond-python-a-polyglot-approach-to-cloud-development","title":"Beyond Python: A Polyglot Approach to Cloud Development","text":"<p>Google Cloud's philosophy embraces a polyglot environment, recognizing that no single language fits every use case. While Python shines in data science, scripting, and rapid development, other languages are vital:</p> <ul> <li>Go: Developed by Google, Go is a natural fit for GCP, known for   performance, simplicity, and efficiency in cloud computing, distributed   systems, and microservices.</li> <li>Java: A robust, object-oriented language, Java is a powerhouse for   enterprise applications, offering strong performance and security features,   integrating seamlessly with GCP services.</li> <li>Node.js (JavaScript): Excellent for web applications, real-time   applications, and API services, Node.js allows JavaScript to run on the   server side efficiently.</li> </ul> <p>Google's Gemini Code Assist even supports a wide array of programming languages for AI-assisted development, including Python, Java, JavaScript, C++, Go, and Rust, highlighting Google's commitment to diverse developer ecosystems.</p>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/#choosing-your-cloud-language-a-strategic-decision","title":"Choosing Your Cloud Language: A Strategic Decision","text":"<p>Ultimately, the \"best\" programming language for Google Cloud \u2013 or for any* cloud platform \u2013 depends on your specific use case, project requirements, performance needs, and team expertise. Python's ease of use, extensive libraries, and strong community make it an excellent choice for (initial) development, data-intensive tasks, and rapid prototyping, especially within the Generative AI space. However, for highly performant, scalable, or enterprise-grade applications, Go, Java, or Node.js could be more suitable.</p> <p>Google Cloud provides the tools and flexibility to choose the language that best empowers your team to build innovative solutions. So, while Python examples for Gemini are abundant and incredibly useful for getting started, remember to select the language that truly aligns with your project's long-term goals.</p>"},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/","title":"The Six Essential Protocols Powering the AI Agent Ecosystem","text":"<p>TL;DR: Six protocols are emerging as the foundation of AI agent architecture: MCP (tool access), A2A (agent collaboration), AG-UI (UI communication), A2UI (UI generation), AP2 (payments), and UCP (commerce). Together, they enable agents to interact seamlessly with tools, other agents, users, and commerce systems. Understanding these protocols today positions you at the forefront of the AI agent revolution.</p> Protocol Purpose Key Benefit MCP Agent \u2192 Tools One connector for every tool A2A Agent \u2192 Agent Discover, coordinate, delegate AG-UI Agent \u2192 Frontend Secure state &amp; approval flows A2UI Agent \u2192 UI Generation Rich interfaces, not just text AP2 Agent \u2192 Payments Cryptographic transaction safety UCP Agent \u2192 Commerce One standard, every retailer <p>As AI agents become increasingly sophisticated, the need for standardized communication protocols has never been more critical. Just as the internet was built on protocols like HTTP and TCP/IP, the emerging AI agent ecosystem is being shaped by a new generation of protocols that enable seamless interaction between agents, tools, users, and commerce systems.</p> <p>Having worked extensively with AI agents over the past year, I've identified six foundational protocols that are shaping how agents interact with the world around them. Each protocol addresses a specific challenge in the agent ecosystem, and together they form the backbone of what I believe will be the standard architecture for AI agent applications.</p>"},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/#the-protocol-stack-from-tools-to-commerce","title":"The Protocol Stack: From Tools to Commerce","text":""},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/#1-mcp-model-context-protocol-agent-to-tools","title":"1. MCP: Model Context Protocol (Agent \ud83e\udd16 to Tools \u2699\ufe0f)","text":"<p>Purpose: Connects agents to external tools and data sources seamlessly.</p> <p>Key Concept: \"One connector. Every tool.\"</p> <p>The Model Context Protocol solves a fundamental challenge: how do agents interact with the thousands of tools and data sources available today? Rather than building custom integrations for each tool, MCP provides a unified interface that allows agents to connect to databases, APIs, files, calendars, calculators, and virtually any other tool through a single protocol.</p> <p></p> <p>Why it matters: Without MCP, every agent would need custom code for every tool integration. With MCP, agents can instantly access any MCP-compatible tool, dramatically reducing development time and expanding capabilities.</p>"},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/#2-a2a-agent-to-agent-protocol-agent-to-agent","title":"2. A2A: Agent-to-Agent Protocol (Agent \ud83e\udd16 to Agent \ud83e\udd16)","text":"<p>Purpose: Enables agents to discover, coordinate with, and delegate tasks to other agents.</p> <p>Key Actions: Discover, Coordinate, Delegate.</p> <p>The A2A Protocol addresses the reality that no single agent can do everything. In complex workflows, agents need to collaborate. For example, an Inventory Agent might need to work with a Research Agent to understand market trends and a Pricing Agent to set optimal prices.</p> <p></p> <p>Real-world scenario: Imagine a supply chain optimization task where multiple specialized agents (logistics, inventory, demand forecasting) need to coordinate to minimize costs while maintaining service levels.</p>"},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/#3-ag-ui-agent-to-frontend-protocol-agent-to-frontend","title":"3. AG-UI: Agent-to-Frontend Protocol (Agent \ud83e\udd16 to Frontend \ud83d\udcf1)","text":"<p>Purpose: Manages the critical interaction layer between agents and user interfaces.</p> <p>Key Functions: Streaming state updates, handling approval workflows, maintaining synchronization.</p> <p>AG-UI an open, lightweight, event-based protocol that standardizes how AI agents connect to user-facing applications. AG-UI is designed to be the general-purpose, bi-directional connection between a user-facing application and any agentic backend.</p> <p>Example: When an agent needs to access sensitive data or perform a high-stakes action, AG-UI manages the approval flow, presenting the request to the user and waiting for confirmation before proceeding.</p>"},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/#4-a2ui-agent-to-user-interface-agent-to-user-interface","title":"4. A2UI: Agent-to-User Interface (Agent \ud83e\udd16 to User Interface \ud83d\udcf1)","text":"<p>Purpose: Empowers agents to create dynamic, context-appropriate user interface elements.</p> <p>Key Concept: Moving beyond plain text to rich, interactive experiences.</p> <p>A2UI is an open standard and set of libraries that allows agents to \"speak UI.\" Agents send a declarative JSON format describing the intent of the UI. The client application then renders this using its own native component library (Flutter, Angular, Lit, etc.)</p> <p>Benefit: This creates a more intuitive and efficient user experience. For instance, instead of asking users to type out complex configuration options, an agent can generate a form with appropriate validation and helpful tooltips.</p> <p>AG-UI vs A2UI: Think of A2UI as the blueprint for UI widgets and AG-UI as the secure communication channel moving them between the AI (backend) and your app (frontend).</p>"},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/#5-ap2-agent-payment-protocol-agent-to-payment-interface","title":"5. AP2: Agent Payment Protocol (Agent \ud83e\udd16 to Payment Interface \ud83d\udcb3)","text":"<p>Purpose: Enables secure, auditable financial transactions for autonomous agents.</p> <p>Key Features: Cryptographic proof at every step, granular permission controls.</p> <p>The AP2 Protocol tackles one of the most challenging aspects of autonomous agents: handling money. It implements a three-stage mandate system: - Intent Mandate: What the agent is authorized to do (e.g., \"Buy shoes under $150\") - Cart Mandate: What the agent has selected - Payment Mandate: The actual transaction execution</p> <p>Security focus: Every step is cryptographically signed and auditable, ensuring agents can only act within their authorized boundaries.</p>"},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/#6-ucp-universal-commerce-protocol-agent-to-commerce","title":"6. UCP: Universal Commerce Protocol (Agent \ud83e\udd16 to Commerce \ud83d\udecd)","text":"<p>Purpose: Standardizes commerce interactions across different retailers and platforms.</p> <p>Key Concept: \"One standard. Every retailer.\"</p> <p>UCP solves the fragmentation problem in e-commerce. Instead of agents needing to understand dozens of different checkout flows, product schemas, and API formats, UCP provides a unified interface for product search, cart management, checkout, and delivery tracking across any compatible retailer. (UCP is built on top of AP2.)</p> <p>Impact: This enables true shopping agent experiences where a single agent can help users find and purchase products from multiple stores without custom integrations for each retailer.</p>"},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/#the-bigger-picture-an-integrated-ecosystem","title":"The Bigger Picture: An Integrated Ecosystem","text":"<p>These six protocols aren't just independent protocols\u2014they work together to create a strong ecosystem:</p> <p></p>"},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/#real-world-impact-how-these-protocols-transform-daily-work","title":"Real-World Impact: How These Protocols Transform Daily Work","text":"<p>Let me share an example. Say my friend runs a small marketing agency and recently started using AI agents. Here's what changed:</p> <p>Before protocols: She had separate tools for project management (Asana), analytics (Google Analytics), client communication (Slack), and invoicing (QuickBooks). Her AI assistant could only help with basic tasks in isolation.</p> <p>After MCP adoption: Her AI assistant now connects to all these tools through a single protocol. When a client emails about campaign performance, the assistant automatically: - Pulls the latest analytics data - Checks project status in Asana - Drafts a comprehensive update email with actual metrics - Suggests next steps based on the data</p> <p>The game-changer? She didn't need to build custom integrations for each tool. The MCP-compatible assistant just worked with everything out of the box.</p> <p>The multiplier effect with A2A: When she needs deeper analysis, her main assistant delegates to specialized agents\u2014one for competitive analysis, another for content optimization. They collaborate behind the scenes and return unified recommendations.</p> <p>This isn't some far-off vision. It's happening now. The protocols are removing the friction that kept AI assistants from being truly useful in real business workflows.</p>"},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/#looking-ahead-the-protocol-first-future","title":"Looking Ahead: The Protocol-First Future","text":"<p>As someone who's been building with AI agents, I'm convinced that these protocols\u2014or something very similar\u2014will become as fundamental to AI applications as REST APIs are to web services today. The teams building on these protocols now are positioning themselves at the forefront of the AI agent revolution.</p> <p>A question for the community: Which of these protocols do you see as most critical for your use cases? Are there gaps that need addressing? I'd love to hear about your experiences building agent-based systems and the integration challenges you've faced.</p>"},{"location":"blog/posts/the-six-essential-protocols-powering-the-ai-agent-ecosystem/#aiagents-protocols-aidevelopment-mcp-a2a-agenteconomy-aiintegration","title":"AIAgents #Protocols #AIDevelopment #MCP #A2A #AgentEconomy #AIIntegration","text":""},{"location":"blog/posts/two-lenses-of-leadership/","title":"The Two Lenses of Leadership: Operational Fixes vs. Strategic Bets","text":"<p>We all want to be \"data-driven.\" It\u2019s the mantra of modern business. Yet, one of the most thrilling moments in my professional journey wasn't just about discovering data; it was the profound realization that the same exact metrics can narrate two entirely different stories, simply by changing the lens through which you view them.</p>"},{"location":"blog/posts/two-lenses-of-leadership/#the-bakery-analogy-a-tale-of-two-lenses","title":"The Bakery Analogy: A Tale of Two Lenses","text":"<p>Consider a hypothetical scenario to illustrate this dichotomy.</p> <p>Imagine you own a bustling downtown bakery. You analyze your sales data and find a clear, undeniable trend: 80% of your customers want your signature sourdough piping hot at exactly 8:00 AM.</p> <p>This demand creates chaos. The kitchen is overwhelmed, delivery drivers are stressed, and if anything goes wrong at 7:55 AM, customer satisfaction plummets.</p>"},{"location":"blog/posts/two-lenses-of-leadership/#lens-1-the-operational-fix","title":"Lens 1: The Operational Fix","text":"<p>If you wear an Operational Efficiency lens, you look at that data and see a bottleneck. The logical conclusion is to smooth out the demand curve. Your thought process might lead to: \"Let's offer a free cookie to anyone picking up their sourdough after 10:00 AM.\" The aim is to alleviate operational stress by subtly shifting customer behavior\u2014a smart, efficient, and cost-effective solution.</p>"},{"location":"blog/posts/two-lenses-of-leadership/#lens-2-the-strategic-bet","title":"Lens 2: The Strategic Bet","text":"<p>But if you wear a Strategic Growth lens, you look at that same data and see massive, untapped desire. You don't see a bottleneck; you see your biggest competitive advantage. Your conclusion isn't to shift demand; it's to meet it. Instead, your rallying cry might be: \"Forget the cookies! Let's invest in a second oven and more morning staff to double our 8:00 AM sourdough sales.\"</p> <p>Thus, one perspective seeks to save by optimizing the existing framework, while the other aims to generate by aggressively scaling what truly resonates with customers.</p> <p></p>"},{"location":"blog/posts/two-lenses-of-leadership/#the-real-world-realization","title":"The Real-World Realization","text":"<p>I share this story because I recently went through my own version of the \"8:00 AM sourdough rush\" on a project.</p> <p>We had clear data showing that a specific service protocol was overwhelmingly driving our success, but it was operationally taxing to execute. My immediate instinct\u2014my operational lens\u2014was to find ways to incentivize customers toward an easier alternative to relieve the pressure on our team.</p> <p>Fortunately, a senior leader challenged me with the strategic lens. They argued that if this difficult protocol was the key to delighting our users, we shouldn't try to minimize it\u2014we should figure out how to scale it aggressively.</p>"},{"location":"blog/posts/two-lenses-of-leadership/#the-takeaway","title":"The Takeaway","text":"<p>This marked a thrilling moment of perspective shift. While neither approach was inherently 'wrong,' it unveiled an invaluable lesson: </p> <p>Sometimes the path forward isn't merely to alleviate an operational pinch point by redirecting demand. Sometimes, the path forward is to lean into the difficulty, because it could be where the magic \ud83c\udf1f happens.</p>"},{"location":"blog/posts/vibe-coding-dos-donts/","title":"Vibe Coding - Dos and Donts","text":""},{"location":"blog/posts/vibe-coding-dos-donts/#dos-and-donts-with-vibe-coding","title":"DO's and DOnt's with Vibe Coding","text":"<p>Summary notes of https://www.infoworld.com/article/4029093/9-habits-of-the-highly-ineffective-vibe-coder.html</p> <ul> <li> <p>Do's Do experiment with different models. Different LLMs have varying   strengths and weaknesses, and you might need to try a few to find the right   one for your specific coding problem.</p> </li> <li> <p>Do treat the AI as a search tool. LLMs excel at finding and recalling   information from their vast training data, such as obscure documentation or   code snippets.</p> </li> <li> <p>Do use the AI for simple, repetitive tasks. The article suggests AI can be   helpful for basic programming chores.</p> </li> <li> <p>Don'ts Don't trust the LLM completely. The article warns that LLMs can   provide incorrect information and even \"hallucinate\" non-existent code or   URLs.</p> </li> <li> <p>Don't treat the LLM like a \"dumpster.\" Avoid dumping large blocks of code on   the AI, as this can be inefficient, costly, and even confuse the model.</p> </li> <li> <p>Don't assume all models are the same. Each LLM has a unique internal   structure, parameter count, and training data, which can significantly affect   its performance.</p> </li> <li> <p>Don't ignore the costs. AI tools charge by the token, and repetitive requests   or large inputs can lead to surprisingly high costs.</p> </li> <li> <p>Don't hand over full control. The article highlights the risk of trusting an   LLM with critical tasks, as their inherent randomness can lead to   unpredictable and destructive outcomes.</p> </li> <li> <p>Don't expect the AI to \"think\" like a human. LLMs are clever mimics and   excellent at information retrieval, but they are not always good at deep   synthesis or providing novel insights.</p> </li> <li> <p>Don't create inconsistent code. The randomness in LLM output can lead to a   \"patchwork quilt\" of different coding styles, making the codebase messy and   hard to maintain.</p> </li> </ul>"},{"location":"blog/posts/vibe-coding-intro/","title":"Vibe Coding - Intro","text":""},{"location":"blog/posts/vibe-coding-intro/#vibe-code-the-what-why-and-how-and-when-to-hit-the-brakes","title":"Vibe Code? The What, Why, and How (And When to Hit the Brakes)","text":"<p>What is Vibe Coding? At its heart, vibe coding is about using AI to generate code from a high-level intent or \"vibe.\" Think of it less like a co-pilot and more like an automated sidekick. You describe a function or a script, the AI generates the code, and you run it. You're less focused on the syntax and more on the desired outcome. It\u2019s a workflow of describe-generate-run-refine.</p> <p>This is a stark contrast to responsible AI-assisted development, where the human developer remains firmly in the driver's seat, meticulously reviewing and guiding every line of code. Vibe coding is fast, fluid, and often done with the explicit goal of getting a quick result, sometimes with the intent to throw the code away later.</p> <p>When to Ride the Vibe \ud83c\udfc4 Not all projects are created equal. Vibe coding is a superpower for the right task.</p> <p>For Developers: Rapid Prototyping: Got a weekend idea for a simple web app or a data script? Vibe code it. You can spin up a proof-of-concept in hours, not days.</p> <p>Automating Repetitive Tasks: Need a Python script to rename files or parse some logs? A simple prompt like \"write a Python function to read a CSV file\" can save you the boilerplate.</p> <p>Learning a New Library: Want to see how a new library works? Ask an AI to generate a simple example. It's like having a personalized, instant documentation assistant.</p> <p>When to Hit the Brakes \ud83d\uded1 Just as a sports car is a bad choice for a family road trip, vibe coding has its limits. This is where it gets critical for tech leads and project managers.</p> <p>For Tech Leads and Project Managers: Critical Systems: Never, ever vibe code mission-critical or security-sensitive applications. The code generated might have unknown flaws, and the cost of debugging or a security breach will far outweigh the speed benefits.</p> <p>Long-term Projects: If a project needs to be maintained for months or years, a vibe-coded mess will become a nightmare. It will accumulate technical debt that cripples the team and makes scaling impossible.</p> <p>Ensuring Code Quality: Vibe-coded solutions often lack documentation, modularity, and proper error handling. This can lead to an \"entropy loop\" where every fix introduces more problems.</p> <p>Common Vibe-Coding Mistakes (And How to Fix Them) The true danger isn't the AI\u2014it's the over-reliance on it. Here are some pitfalls to watch out for, with actionable advice for everyone on the team.</p> <ol> <li>The Security Trap \ud83d\udd12 The Mistake: Blindly accepting AI-generated code that    contains vulnerabilities. In one notable case, an AI assistant naively used    eval() on user input, creating a critical arbitrary code execution    vulnerability. Another common mistake is hardcoding API keys directly into a    script.</li> </ol> <p>Fix: Developers, maintain a \"human in the loop\" mindset. Always review code    for common security flaws like insecure input handling. Tech leads, mandate    static analysis tools and code reviews for any AI-generated code, no matter    how small.</p> <ol> <li>The Technical Debt Vortex \ud83c\udf2a\ufe0f The Mistake: Treating a vibe-coded prototype as    a production-ready solution. The code works, but it's a tangled mess that's    impossible to debug or extend.</li> </ol> <p>Fix: Developers, refactor aggressively. If a prototype is promising, treat    it as pseudo-code and rewrite it with proper structure. Project managers,    plan for a \"refactoring phase\" in your sprints. The AI got you 80% there;    now build the last, most crucial 20% responsibly.</p> <ol> <li>The Skills Erosion \ud83e\udde0 The Mistake: Over-relying on AI to the point where    developers stop understanding the fundamentals. You lose the ability to    debug complex issues because you never truly learned how the code works.</li> </ol> <p>Fix: Developers, use AI as a tool to explore, not a crutch to lean on.    Always ask the AI why it made a certain choice. Tech leads, foster a culture    of learning. Encourage pair programming and discussions on how to improve    AI-generated code.</p>"},{"location":"blog/posts/gemini-cli-cheatsheet-printout/google-cloud-docs-mcp/","title":"Google cloud docs mcp","text":"<pre><code>sampathm-mac:~ sampathm$\nsampathm-mac:~ sampathm$ # Enable the Developer Knowledge API\nsampathm-mac:~ sampathm$ gcloud services enable developerknowledge.googleapis.com --project=gemini-demo-project-2141\nOperation \"operations/acat.p2-465664842892-870a709c-a3cd-4142-803b-003f9234be5e\" finished successfully.\nsampathm-mac:~ sampathm$\nsampathm-mac:~ sampathm$ # Create an API key\nsampathm-mac:~ sampathm$ gcloud services api-keys create --project=gemini-demo-project-2141 --display-name=\"DK API Key\"\nOperation [operations/akmf.p7-465664842892-7f9d0052-cd5e-4bdd-b72d-96780d48e14c] complete. Result: {\n    \"@type\":\"type.googleapis.com/google.api.apikeys.v2.Key\",\n    \"createTime\":\"2026-02-11T09:18:06.981920Z\",\n    \"displayName\":\"DK API Key\",\n    \"etag\":\"W/\\\"M+VbUnMbRT7Mr8hTB+nn+g==\\\"\",\n    \"keyString\":\"AIzaSyBeT41Y8mU1daxrWd3pI3urCJFiq1x6r18\",\n    \"name\":\"projects/465664842892/locations/global/keys/597e7886-a7b7-4cb5-b154-49faa1072a76\",\n    \"uid\":\"597e7886-a7b7-4cb5-b154-49faa1072a76\",\n    \"updateTime\":\"2026-02-11T09:18:07.006738Z\"\n}\nsampathm-mac:~ sampathm$\nsampathm-mac:~ sampathm$ # Enable the Developer Knowledge MCP\nsampathm-mac:~ sampathm$ gcloud beta services mcp enable developerknowledge.googleapis.com \\\n&gt;     --project=gemini-demo-project-2141\nThe MCP endpoint for service developerknowledge.googleapis.com has been enabled for the resource projects/gemini-demo-project-2141.\n\n\nsampathm-mac:~ sampathm$\nsampathm-mac:~ sampathm$ # Configure your AI application\nsampathm-mac:~ sampathm$ gemini mcp add -t http -H \"X-Goog-Api-Key: AIzaSyBeT41Y8mU1daxrWd3pI3urCJFiq1x6r18\" google-developer-knowledge https://developerknowledge.googleapis.com/mcp --scope user\nLoaded cached credentials.\nMCP server \"google-developer-knowledge\" added to user settings. (http)\n\nsampathm-mac:~ sampathm$\nsampathm-mac:~ sampathm$\nsampathm-mac:~ sampathm$ gemini mcp list\nLoaded cached credentials.\nConfigured MCP servers:\n\u2717 google-developer-knowledge: https://developerknowledge.googleapis.com/mcp (http) - Disconnected\n\u2713 google-workspace (from google-workspace): node scripts/start.js (stdio) - Connected\n\u2717 nanobanana (from nanobanana): node /Users/sampathm/.gemini/extensions/nanobanana/mcp-server/dist/index.js (stdio) - Disconnected\n\nsampathm-mac:~ sampathm$\nsampathm-mac:~ sampathm$\n</code></pre> <pre><code>\n\n \u2588\u2588\u2588            \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\n\u2591\u2591\u2591\u2588\u2588\u2588         \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\n  \u2591\u2591\u2591\u2588\u2588\u2588      \u2588\u2588\u2588     \u2591\u2591\u2591  \u2591\u2588\u2588\u2588  \u2588 \u2591  \u2591\u2588\u2588\u2588\u2591\u2588\u2588\u2588\u2588\u2588\u2591\u2588\u2588\u2588  \u2591\u2588\u2588\u2588  \u2591\u2588\u2588\u2588\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588  \u2591\u2588\u2588\u2588\n    \u2591\u2591\u2591\u2588\u2588\u2588   \u2591\u2588\u2588\u2588          \u2591\u2588\u2588\u2588\u2588\u2588\u2588    \u2591\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588  \u2591\u2588\u2588\u2588  \u2591\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2588\u2591\u2588\u2588\u2588  \u2591\u2588\u2588\u2588\n     \u2588\u2588\u2588\u2591    \u2591\u2588\u2588\u2588    \u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2588    \u2591\u2588\u2588\u2588 \u2591\u2591\u2591  \u2591\u2588\u2588\u2588  \u2591\u2588\u2588\u2588  \u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588  \u2591\u2588\u2588\u2588\n   \u2588\u2588\u2588\u2591      \u2591\u2591\u2588\u2588\u2588  \u2591\u2591\u2588\u2588\u2588  \u2591\u2588\u2588\u2588 \u2591   \u2588 \u2591\u2588\u2588\u2588      \u2591\u2588\u2588\u2588  \u2591\u2588\u2588\u2588  \u2591\u2588\u2588\u2588  \u2591\u2591\u2588\u2588\u2588\u2588\u2588  \u2591\u2588\u2588\u2588\n \u2588\u2588\u2588\u2591         \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588     \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588  \u2591\u2591\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\n\u2591\u2591\u2591            \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591  \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591     \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591    \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591\n\n\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\n &gt; How do I list Cloud Storage buckets?\n\n\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u2713  search_documents (google-developer-knowledge MCP Server) {\"quer\u2026 \u2502\n\u2502 {                                                                   \u2502\n\u2502   \"results\": [                                                      \u2502\n\u2502     {                                                               \u2502\n\u2502       \"parent\":                                                     \u2502\n\u2502 \"documents/docs.cloud.google.com/storage/docs/samples/storage-list- \u2502\n\u2502 buckets\",                                                           \u2502\n\u2502       \"id\": \"c1\",                                                   \u2502\n\u2502       \"content\": \"List the Cloud Storage buckets in a               \u2502\n\u2502 project.\\n\\n## Explore further\\n\\nFor detailed documentation that   \u2502\n\u2502 includes this code sample, see the following:\\n\\n* [Listing         \u2502\n\u2502 buckets](/storage/docs/listing-buckets)\\n\\n\"                        \u2502\n\u2502 ....                                                                \u2502\n\u2502 ....                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2726 To list Cloud Storage buckets, you can use the gcloud CLI command:\n\n   1 gcloud storage ls\n\n\n  This command will list all Cloud Storage buckets in your default\n  project.\n</code></pre> <p>Source: https://developers.google.com/knowledge/mcp#gcloud-cli</p>"},{"location":"events/","title":"Events","text":""},{"location":"events/#professional-speaking-engagements-and-technical-leadership","title":"Professional Speaking Engagements and Technical Leadership","text":"<p>As a Developer Programs Engineer at Google Cloud, I have actively contributed to major AI and cloud computing events across Europe. These engagements demonstrate my expertise in AI agent development, memory management, and the Google Agent Development Kit (ADK).</p>"},{"location":"events/#2026","title":"2026","text":""},{"location":"events/#2026-02-17-accelerate-ai-with-cloud-run-warsaw","title":"2026-02-17: Accelerate AI with Cloud Run Warsaw","text":"<p>Role: Event Support (Facilitator/TA)</p> <p>I supported this event as part of the Facilitators/TA team to help attendees with the hands-on labs. Remigiusz Samborski was the event lead and delivered the introduction and presentations. I also participated in the \"Know Before You Go\" (KBYG) preparation session on February 5, 2026.</p>"},{"location":"events/#2025","title":"2025","text":""},{"location":"events/#2025-11-10-to-2025-11-14-kaggle-ai-days-5-day-ai-agents-intensive-course","title":"2025-11-10 to 2025-11-14: Kaggle AI Days - 5-Day AI Agents Intensive Course","text":"<p>Role: Course Codelabs Lead for Day 3: Context Engineering &amp; Memory</p> <p>I served as the technical lead for Day 3 of Google's flagship AI Agents Intensive Course, which attracted over 1.5M participants globally. My responsibilities included: - Review the course codelabs content on context engineering and memory management for AI agents - Creating two comprehensive Colab notebooks demonstrating short-term and long-term memory implementations</p>"},{"location":"events/#2025-11-05-google-cloud-ai-day-sweden","title":"2025-11-05: Google Cloud AI Day - Sweden","text":"<p>Location: M\u00fcnchenbryggeriet, Stockholm Role: Technical Speaker - Developer Track</p> <p>I delivered 2 technical sessions at Google Cloud AI Day Sweden, engaging with 200+ developers and IT leaders: - Session 1: Building Intelligent Agents: Gemini, Google ADK, and Memory Management - Session 2: Five Key Takeaways to Enhance Your Gemini Apps - Impact: Successfully demonstrated practical implementations of AI agents using Google ADK</p>"},{"location":"events/#2025-10-28-google-cloud-ai-day-denmark","title":"2025-10-28: Google Cloud AI Day - Denmark","text":"<p>Location: Copenhagen Role: Technical Speaker - Developer Track</p> <p>I presented a technical session on \"Building Intelligent Agents: Gemini, Google ADK, and Memory Management\" as part of the developer track, contributing to Google Cloud's Nordic AI initiative.</p>"},{"location":"events/#2025-04-09-google-cloud-next-25","title":"2025-04-09: Google Cloud Next '25","text":"<p>Location: Las Vegas, U.S. Role(s): Go-To-Market DevRel Lead for GenAI SDKs(Python &amp; Go), Presenter of Advanced Development Tools in Vertex AI Demo and one of the speaker for Agent Starter Pack (ASP) at Google Cloud Next 2025.</p>"},{"location":"events/#2025-03-04-google-devcloud-day-warsaw","title":"2025-03-04: Google /dev/cloud day Warsaw","text":"<p>Role: Workshop Co-Lead</p> <p>I participated as a workshop co-lead, running the \"Gemini Workshop: Multimodality and Long Context\" alongside Laurent Picard. Remigiusz Samborski was heavily involved, delivering the Welcome, Keynote, and a presentation on Gemini 2.0 for developers.</p>"},{"location":"events/#2024","title":"2024","text":""},{"location":"events/#2024-11-06-google-cloud-summit-poland-2024","title":"2024-11-06: Google Cloud Summit Poland 2024","text":"<p>Role: Speaker &amp; Demo Booth Lead</p> <p>I participated as a speaker presenting \"Inside Google Cloud: How GenAI is Transforming our Docs &amp; Code Samples\". I also successfully ran the \"Load Balancing Blitz\" (LB Blitz) game demo booth for attendees. Remigiusz was a co-speaker with me for the presentation and collaborated on the LB Blitz booth.</p>"},{"location":"events/#older-events","title":"Older Events","text":"<ul> <li>2024-03-14: Duet AI for Developers Roadshow Warsaw - Event Support (Facilitator)</li> <li>2023-10-26: Google Cloud Summit Poland 2023 - Google Cloud Experts Booths (Facilitator)</li> </ul>"},{"location":"events/2025-04-09-cloud-next/","title":"Google Cloud Next 2025 (April 9 to Apr 13)","text":""},{"location":"events/2025-04-09-cloud-next/#devrel-gtm-tl-gen-ai-sdk-launch-python-go","title":"DevRel GTM TL: Gen AI SDK Launch (Python &amp; Go)","text":"<p>DevRel Go To Market Lead for the Google Gen AI SDK in Python. I was responsible for: - Support the user onboarding for the SDK launch at Google Cloud Next 2025 - Providing technical artifacts that helped developers get started with the SDK.</p>"},{"location":"events/2025-04-09-cloud-next/#cloud-next-demo-booth-advanced-development-tools-in-vertex-ai-demo","title":"Cloud Next Demo Booth: Advanced Development Tools in Vertex AI Demo","text":"<p>I was a Technical Lead for the \"Advanced Development Tools in Vertex AI Demo\" at Google Cloud Next 2025. - Presented the advanced development tools in Vertex AI to 250+ attendees. - Developed a demo app to shown advanced uses of Gemini Controlled Generations, Caching and other feature to build Knowledge Graphs on Books.</p>"},{"location":"events/2025-04-09-cloud-next/#presenter","title":"Presenter:","text":""},{"location":"events/2025-04-09-cloud-next/#i-was-one-of-the-presenter-for-google-agent-starter-pack-asp","title":"I was one of the Presenter for Google Agent Starter Pack (ASP).","text":""},{"location":"events/2025-10-28-ai-day-denmark/","title":"AI Day Denmark","text":""},{"location":"events/2025-10-28-ai-day-denmark/#google-cloud-ai-day-denmark-2025","title":"Google Cloud AI Day Denmark 2025","text":"<p>Event Date: October 28, 2025 Location: Copenhagen, Denmark Event Website: https://cloudonair.withgoogle.com/events/google-cloud-ai-day-denmark-2025</p>"},{"location":"events/2025-10-28-ai-day-denmark/#my-contribution","title":"My Contribution","text":""},{"location":"events/2025-10-28-ai-day-denmark/#speaker-profile","title":"Speaker Profile","text":"<ul> <li>Name: Sampath Kumar</li> <li>Title: Developer Programs Engineer, Google Cloud</li> <li>Track: Developer Track</li> <li>Session Time: 4:00 PM CET</li> </ul>"},{"location":"events/2025-10-28-ai-day-denmark/#session-details","title":"Session Details","text":"<p>Title: Building Intelligent Agents: Gemini, Google ADK, and Memory Management</p> <p>Abstract: In this session, I provided developers with a comprehensive understanding of how to leverage Google Gemini models with the Agent Development Kit (ADK) to build intelligent agents. The session focused particularly on implementing and managing agent memory for more sophisticated and persistent interactions.</p> <p>Key Topics Covered: - Transforming stateless LLMs into stateful, intelligent agents - Session and Memory (SAM) management in Google ADK - Practical implementation patterns for agent memory - Best practices for production-ready AI agents</p> <p>Audience Impact: - 150+ working professionals attendees the session. - Engaged with developers through hands-on demonstrations and Q&amp;A</p>"},{"location":"events/2025-10-28-ai-day-denmark/#event-context","title":"Event Context","text":"<p>Google Cloud AI Day Denmark 2025 was part of Google Cloud's Nordic AI initiative, featuring: - 15+ sessions with demos and hands-on labs - Parallel tracks for IT/Business Leaders and Developers - Focus on the \"agentic era\" of AI and practical enterprise applications</p> <p>The event emphasized how organizations can harness AI to revolutionize operations, with particular focus on autonomous AI agents that boost productivity and resilience.</p>"},{"location":"events/2025-10-28-ai-day-denmark/#professional-impact","title":"Professional Impact","text":"<p>This speaking engagement allowed me to: - Share expertise in AI agent development with the Nordic developer community - Demonstrate practical applications of Google's latest AI technologies - Network with industry leaders and developers implementing AI solutions</p>"},{"location":"events/2025-10-28-ai-day-denmark/#related-events","title":"Related Events","text":"<p>This was the third event in Google Cloud's Nordic AI Days series: - Helsinki (October 7) - Oslo (October 14) - Copenhagen (October 28) - My speaking engagement - Stockholm (November 5) - Also featured as speaker </p>"},{"location":"events/2025-11-05-ai-day-sweden/","title":"AI Day Sweden","text":""},{"location":"events/2025-11-05-ai-day-sweden/#google-cloud-ai-day-sweden-2025","title":"Google Cloud AI Day Sweden 2025","text":"<p>Event Date: November 5, 2025 Location: M\u00fcnchenbryggeriet, Stockholm Event Website: https://cloudonair.withgoogle.com/events/google-cloud-ai-day-sweden-2025</p>"},{"location":"events/2025-11-05-ai-day-sweden/#my-contribution","title":"My Contribution","text":""},{"location":"events/2025-11-05-ai-day-sweden/#speaker-profile","title":"Speaker Profile","text":"<ul> <li>Name: Sampath Kumar</li> <li>Title: Developer Programs Engineer, Google Cloud</li> <li>Track: Developer Track (10:45 AM CET)</li> <li>Sessions Delivered: 2 technical presentations</li> </ul>"},{"location":"events/2025-11-05-ai-day-sweden/#session-details","title":"Session Details","text":"<p>Session 1: Building Intelligent Agents: Gemini, Google ADK, and Memory Management</p> <p>Session 2: Five Key Takeaways to Enhance Your Gemini Apps</p> <p>Format: Extended Q&amp;A and interactive discussions with post-session networking</p> <p>Key Technical Topics Presented: - Memory Management fundamentals for AI Agents - Google ADK SessionManagers for short-term memory management - MemoryManagement implementations for long-term memory persistence - Context engineering best practices for Gemini models - Data privacy considerations in agent memory systems</p> <p>Audience Engagement: - Successfully engaged with 200+ developers and technical leaders - Utilized the mnemonic \"SAM \u2192 Session and Memory\" to reinforce key concepts - Extended Q&amp;A sessions based on high audience interest.</p>"},{"location":"events/2025-11-05-ai-day-sweden/#technical-insights-shared","title":"Technical Insights Shared","text":""},{"location":"events/2025-11-05-ai-day-sweden/#memory-architecture-in-google-adk","title":"Memory Architecture in Google ADK","text":"<ul> <li>Short-term Memory: Demonstrated SessionManagers for maintaining conversational context</li> <li>Long-term Memory: Showcased persistent memory implementations for complex agent behaviors</li> <li>Context Optimization: Explained strategies for maximizing Gemini's context window efficiency</li> </ul>"},{"location":"events/2025-11-05-ai-day-sweden/#best-practices-emphasized","title":"Best Practices Emphasized","text":"<ol> <li>Proper context management for improved agent performance</li> <li>Data privacy and security in memory systems</li> <li>Leveraging ADK base classes for custom implementations</li> <li>Production-ready patterns for agent deployment</li> </ol>"},{"location":"events/2025-11-05-ai-day-sweden/#event-context","title":"Event Context","text":"<p>Google Cloud AI Day Sweden featured: - Dual tracks: Developer Track (08:00-19:30) and IT &amp; Business Leader Track (12:30-19:30) - 15 sessions with hands-on labs and live demonstrations - Focus on the \"agentic era\" and practical AI implementations - Part of Google Cloud's Nordic AI initiative</p>"},{"location":"events/2025-11-05-ai-day-sweden/#professional-impact","title":"Professional Impact","text":"<p>This speaking engagement resulted in: - Deep technical discussions with Nordic developer community - Valuable feedback incorporated into Google ADK documentation.</p>"},{"location":"events/2025-11-05-ai-day-sweden/#open-source-contribution","title":"Open Source Contribution","text":"<p>Emphasized Google ADK's open-source nature and encouraged community contributions, highlighting: - Extensibility of ADK base classes - Community-driven development opportunities - Real-world implementation patterns</p>"},{"location":"events/2025-11-05-ai-day-sweden/#related-materials","title":"Related Materials","text":"<ul> <li>https://github.com/msampathkumar/google-adk-sam</li> </ul>"},{"location":"events/2025-11-10-kaggle-ai-days/","title":"Kaggle AI Days","text":""},{"location":"events/2025-11-10-kaggle-ai-days/#kaggle-ai-days-2025-5-day-ai-agents-intensive-course","title":"Kaggle AI Days 2025: 5-Day AI Agents Intensive Course","text":"<p>Event Dates: November 10-14, 2025 Format: Online, Global Official Website: https://rsvp.withgoogle.com/events/google-ai-agents-intensive_2025 Kaggle Learn Guide: https://www.kaggle.com/learn-guide/5-day-agents</p>"},{"location":"events/2025-11-10-kaggle-ai-days/#my-role-and-contribution","title":"My Role and Contribution","text":""},{"location":"events/2025-11-10-kaggle-ai-days/#technical-leadership","title":"Technical Leadership","text":"<ul> <li>Position: Course Lead for Day 3: Context Engineering &amp; Memory</li> <li>Name: (Sampath Kumar](https://www.kaggle.com/maddula)</li> <li>Title: Developer Programs Engineer, Google Cloud</li> <li>Responsibility: Led the curriculum development and implementation for context management in AI agents</li> </ul>"},{"location":"events/2025-11-10-kaggle-ai-days/#deliverables-created","title":"Deliverables Created","text":"<ol> <li>Colab Notebook 1: Short-term Memory Implementation</li> <li>Demonstrated session management techniques</li> <li>Practical examples using Google ADK SessionManagers</li> <li> <p>https://www.kaggle.com/code/kaggle5daysofai/day-5b-agent-deployment</p> </li> <li> <p>Colab Notebook 2: Long-term Memory Implementation</p> </li> <li>Showcased persistent memory patterns</li> <li>Integration with vector databases</li> <li>Production-ready memory architectures</li> <li>https://www.kaggle.com/code/kaggle5daysofai/day-5a-agent2agent-communication</li> </ol>"},{"location":"events/2025-11-10-kaggle-ai-days/#course-overview","title":"Course Overview","text":"<p>This flagship Google AI course attracted over 420,000 registrations globally, following the success of the previous year's Gen AI Intensive Course. The course focused on building autonomous AI agents that go beyond simple chatbots.</p>"},{"location":"events/2025-11-10-kaggle-ai-days/#5-day-curriculum-structure","title":"5-Day Curriculum Structure","text":"<ol> <li>Day 1: Introduction to Agents &amp; Agentic Architectures</li> <li>Day 2: Agent Tools &amp; Interoperability with MCP</li> <li>Day 3: Context Engineering &amp; Memory (My Leadership Area)</li> <li>Day 4: Quality, Logging &amp; Evaluation</li> <li>Day 5: Prototype to Production</li> </ol>"},{"location":"events/2025-11-10-kaggle-ai-days/#day-3-focus-context-engineering-memory","title":"Day 3 Focus: Context Engineering &amp; Memory","text":"<p>As the Day 3 lead, I covered: - Short-term Memory: Conversation context and session management - Long-term Memory: Persistent knowledge and learning mechanisms - Memory Architecture Patterns: Best practices for production systems - Google ADK Integration: Practical implementation using ADK's memory components</p>"},{"location":"events/2025-11-10-kaggle-ai-days/#impact-and-reach","title":"Impact and Reach","text":"<ul> <li>Community Engagement: Active participation in Discord discussions</li> <li>Capstone Projects: Participants used Day 3 concepts in their final projects</li> </ul>"},{"location":"events/2025-11-10-kaggle-ai-days/#technical-stack","title":"Technical Stack","text":"<ul> <li>Primary Framework: Google Agent Development Kit (ADK)</li> <li>Language: Python</li> <li>Tools: Colab notebooks, Gemini API, Vector databases</li> <li>Platforms: Kaggle, YouTube (live sessions), Discord (community)</li> </ul>"},{"location":"events/2025-11-10-kaggle-ai-days/#key-takeaways-for-participants","title":"Key Takeaways for Participants","text":"<p>Through my Day 3 curriculum, participants learned: 1. How to implement stateful AI agents using memory systems 2. Best practices for context management in production 3. Practical patterns for short-term and long-term memory 4. Integration techniques with Google ADK</p> <p>This course represents Google's commitment to democratizing AI agent development, and I'm proud to have contributed to educating the next generation of AI developers.</p>"},{"location":"google-cloud-gemini-cookbook/","title":"Google Cloud - Gemini Cookbook","text":""},{"location":"google-cloud-gemini-cookbook/#introduction","title":"Introduction","text":"<p>This cookbook is a collection of recipes for using the Google Cloud platform.</p>"},{"location":"google-cloud-gemini-cookbook/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Google account with access to the Google Cloud Platform.</li> <li>A valid API key or service account credentials.</li> <li>A working Python environment (Python &gt;= 3.8).</li> </ul>"},{"location":"google-cloud-gemini-cookbook/#tools","title":"Tools","text":"<ul> <li>Google Cloud Gemini SDK to access the Gemini API.</li> <li>Streamlit for building the user interface.</li> <li>Cloud Run to deploy the app.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/#cookbook-lessons","title":"Cookbook Lessons","text":"<ul> <li>01: Build a <code>Hello World</code> app with Gemini, Streamlit &amp; Google Cloud Run \ud83d\ude80</li> <li>02: Deploy Your AI Chatbot to Google Cloud Run: Go Live! \u2601\ufe0f</li> <li>03 Build Your First Context-aware Gemini Chatbot in Minutes: The Secret to Speed and Relevance! \u26a1</li> <li>04: \ud83d\udd13Unlock Enterprise AI: Grounding Gemini with RAG and Google Cloud Search</li> <li>05: Review - Five Takeaways to enhance your Gemini Apps</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-01/","title":"Lesson 1: Basic Chatbot with Streamlit","text":""},{"location":"google-cloud-gemini-cookbook/lesson-01/#cookbook-lesson-01-build-a-hello-world-app-with-gemini-streamlit-google-cloud-run","title":"Cookbook Lesson 01: \ud83d\ude80 Build a <code>Hello World</code> app with Gemini, Streamlit &amp; Google Cloud Run","text":"<p>Welcome to the Google Cloud Gemini Cookbook! In this very first lesson, we're going to embark on an exciting journey: taking your Python code from a simple idea to a live web application in minutes. Forget complex setups; with Streamlit and Google Cloud Run, deploying your first web app is incredibly fast and fun! \u2728</p> <p>This lesson is part of the Google Cloud - Gemini Cookbook (GitHub Link).</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#what-youll-learn","title":"What You'll Learn \ud83c\udf93","text":"<p>This lesson focuses on the essentials of getting a web application up and running quickly:</p> <ol> <li>Build a \"Hello World\" with Streamlit: Discover how effortlessly you can    create interactive web apps using just Python. Streamlit handles all the    front-end magic for you! \ud83d\udc0d</li> <li>Deploy to Google Cloud Run: Learn to take your Streamlit app and deploy    it as a scalable, serverless container on Google Cloud Run. This means your    app can handle traffic effortlessly, and you only pay for what you use! \u2601\ufe0f\ud83d\udcb8</li> </ol> <p>By the end of this lesson, you'll have a fully functional web application accessible via a URL, demonstrating the incredible speed of modern cloud development. \ud83d\ude80</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#prerequisites","title":"Prerequisites \ud83d\udee0\ufe0f","text":"<p>Before we begin, ensure you have the following:</p> <ul> <li>A Google Cloud Project with billing enabled.</li> <li>The <code>gcloud</code> CLI installed and configured.</li> <li>Python 3.8+ installed on your local machine.</li> <li><code>pip</code> (Python package installer).</li> </ul> <p>For development, we recommend using the Google Cloud Shell, which comes pre-installed with the necessary tools.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#what-is-streamlit","title":"What is Streamlit?","text":"<p>Streamlit is an open-source Python framework that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few lines of code, you can build and deploy powerful data apps.</p> <p>While Streamlit is not as feature-rich as full-fledged web frameworks like Django or Flask, its strength lies in its simplicity and ability to create highly interactive applications quickly. This makes it an excellent choice for building demos and prototypes.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#what-is-google-cloud-run","title":"What is Google Cloud Run?","text":"<p>Cloud Run is a fully managed serverless platform that enables you to run stateless containers that are invocable via web requests or Pub/Sub events. You can deploy your code to Cloud Run, and it will automatically scale up or down based on traffic.</p> <p>Here are some of the benefits of using Cloud Run:</p> <ul> <li>Easy to use: Deploy your application with a single command.</li> <li>Serverless: No infrastructure to manage.</li> <li>Scalable: Automatically scales to meet demand.</li> <li>Cost-effective: Pay only for the resources you use.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#local-development-your-streamlit-hello-world","title":"\ud83d\udcbb Local Development: Your Streamlit \"Hello World\"","text":"<p>Let's start by creating a simple Streamlit application locally. Here's how you can do it:</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#1-set-up-your-environment","title":"1. Set up your environment","text":"<p>Create a virtual environment and install the required dependencies:</p> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#2-create-a-simple-streamlit-app","title":"2. Create a simple Streamlit app","text":"<p>Create a file named <code>streamlit_app.py</code> with the following content:</p> <pre><code>import streamlit as st\n\nst.title(\"Sample AI App\")\n\nst.text(\"This is a sample app.\")\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#3-run-the-app-locally","title":"3. Run the app locally","text":"<p>To run the app locally, use the following command:</p> <pre><code>streamlit run streamlit_app.py --server.port 8080\n</code></pre> <p>You should see your Streamlit app open in your browser and navigating to <code>http://localhost:8080</code>. Interact with it! This is your app running locally. \ud83d\ude80</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#4-deploy-to-cloud-run","title":"4. Deploy to Cloud Run","text":"<p>To deploy your app to Cloud Run, you'll need to create a <code>Procfile</code> and a <code>deploy.sh</code> script.</p> <p>Procfile</p> <p>Create a file named <code>Procfile</code> with the following content:</p> <pre><code>web: streamlit run streamlit_app.py --server.port=8080 --server.address=0.0.0.0 --server.enableCORS=false --browser.gatherUsageStats=false\n</code></pre> <p>This file tells Cloud Run how to start your application.</p> <p>deploy.sh</p> <p>Create a file named <code>deploy.sh</code> with the following content:</p> <pre><code>#!/bin/bash\n# Purpose: To deploy the App to Cloud Run.\n\n# Google Cloud Project ID\nPROJECT=\"your-gcp-project-id\"\n\n# Google Cloud Region\nLOCATION=\"us-central1\"\n\n# Deploy app from source code\ngcloud run deploy simple-app --source . --region=$LOCATION --project=$PROJECT --allow-unauthenticated\n</code></pre> <p>Important: Replace <code>\"your-gcp-project-id\"</code> with your actual Google Cloud Project ID.</p> <p>Now, run the deployment script:</p> <pre><code>bash deploy.sh\n</code></pre> <p>This command will build a container image from your source code, push it to the container registry, and deploy it to Cloud Run. Once the deployment is complete, you'll see a service URL in the output.</p> <p>Congratulations \ud83c\udf89! You have successfully deployed your Streamlit app to Cloud Run.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#cleanup","title":"Cleanup","text":"<p>To avoid incurring future charges, delete the resources you created:</p> <ul> <li>Go to the Cloud Run console and   delete your application.</li> <li>Go to the Container Registry and   delete the container image.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#learn-more","title":"Learn More","text":"<ul> <li>Cloud Run Documentation</li> <li>Streamlit Documentation</li> <li>Authenticating to Cloud Run</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-02/","title":"Lesson 2: Chatbot with Memory & Gemma","text":""},{"location":"google-cloud-gemini-cookbook/lesson-02/#cookbook-lesson-02-deploy-your-ai-chatbot-to-google-cloud-run-go-live","title":"Cookbook Lesson 02: Deploy Your AI Chatbot to Google Cloud Run: Go Live! \u2601\ufe0f","text":"<p>Welcome to the second lesson in our Gemini Cookbook series! This time, we're diving into the exciting world of conversational AI. You'll learn to build a smart, interactive chatbot using the power of Streamlit and Google's Gemini 2.5 Flash model. We'll be using the official Google Cloud Vertex AI SDK, which has powerful features like chat sessions that give your bot a memory.</p> <p>This lesson is part of the Google Cloud - Gemini Cookbook (GitHub Link).</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#what-youll-create","title":"What You'll Create","text":"<p>Get ready to build a sleek, web-based chatbot. With Streamlit as our frontend, your chatbot will connect to the mighty Gemini 1.5 Flash model, enabling you to have dynamic and stateful conversations. It's like having your own personal AI assistant!</p> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#what-youll-need","title":"What You'll Need","text":"<p>To get started, make sure you have the following essentials:</p> <ul> <li>A Google Cloud project with the Vertex AI API ready to go.</li> <li>Python 3.8 or a newer version.</li> <li>The <code>pip</code> package manager for installing our dependencies.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#mandatory-steps","title":"Mandatory steps","text":"<p>This is a mandatory steps to access Gemini Models from your Google Cloud Project.</p> <p>I have installed the Gcloud tool and used Application Default Credentials to get the credentials. If you want to run the code in Google Cloud project, then you need to update respective service-account with the required permissions. For details, check out this user-managed service account article.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#lets-get-building","title":"Let's Get Building!","text":"<ol> <li>Get the Code: First, clone the repository and hop into the right    directory:</li> </ol> <p><code>bash    git clone https://github.com/msampathkumar/msampathkumar.github.io.git    cd msampathkumar.github.io/docs/google-cloud-gemini-cookbook/lesson-02</code></p> <ol> <li>Set Up Your Workspace: Create a virtual environment to keep things tidy:</li> </ol> <p><code>bash    python3 -m venv .venv    source .venv/bin/activate</code></p> <ol> <li>Install the Magic: Time to install the necessary Python packages:</li> </ol> <p><code>bash    pip install -r requirements.txt</code></p> <ol> <li>Connect to Google Cloud: Authenticate your local environment to use    Google Cloud services:</li> </ol> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#a-look-under-the-hood","title":"A Look Under the Hood","text":"<p>Let's take a peek at the code that makes our chatbot tick.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#the-chatbot-ui-streamlit_apppy","title":"The Chatbot UI: <code>streamlit_app.py</code>","text":"<p>This file is the heart of our Streamlit app. It's responsible for:</p> <ul> <li>Providing a chat interface for user input.</li> <li>Displaying the response from the model.</li> <li>Maintaining the conversation history.</li> </ul> <p>While you would typically use Streamlit's <code>session_state</code> to store the conversation history manually, the Vertex AI SDK simplifies this. We'll use a <code>ChatSession</code> object from the SDK, which automatically handles the history for us. We just need to store this one object in <code>st.session_state</code> to make our chat stateful.</p> <p>You can see the core logic below:</p> <pre><code>import streamlit as st\nimport llm\n\n# Initialize chat session in Streamlit's session state.\n# This will be run only once, on the first run of the session.\nif \"chat_session\" not in st.session_state:\n    st.session_state.chat_session = llm.start_chat()\n\n# Display chat history from the session state\nfor message in st.session_state.chat_session.history:\n    with st.chat_message(\"assistant\" if message.role == \"model\" else \"user\"):\n        st.markdown(message.parts[0].text)\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#the-brains-of-the-operation-llmpy","title":"The Brains of the Operation: <code>llm.py</code>","text":"<p>This file handles all the communication with the Gemini 2.5 Flash model. As we are using GenAI SDK, we can use environment variables to set up the required details for authentication. Also, GenAI SDK provides us with <code>Client</code> class which we can use to create a chat session and send messages to the Gemini Model and receive.</p> <pre><code>from google import genai\n\n# Using environment variables to pass essential parameters to the client.\nclient = genai.Client()\n\n# Create chat session\nchat_session = client.chats.create(\"gemini-2.0-flash-lite-001\")\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#to-run-chatbot-in-cli","title":"To run chatbot in CLI","text":"<pre><code>python llm.py\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#to-run-streamlit-chatbot","title":"To run streamlit chatbot","text":"<pre><code>streamlit run streamlit_app.py\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#deploy-the-application","title":"Deploy the application","text":"<p>You can deploy your chatbot to Google Cloud Run and share it with the world.</p> <p>Use the <code>deploy.sh</code> script to package your app into a Docker image and send it to the Google Artifact Registry.</p> <pre><code>./deploy.sh\n</code></pre> <p>The script will then deploy your app to Cloud Run, making it live on the web.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#you-did-it","title":"You Did It!","text":"<p>High five! You've built and deployed a fully functional chatbot with Streamlit and Gemini Pro. You've seen how to use the new Generative AI SDK and its chat features to create a more natural and engaging conversational experience. Now, go ahead and have a chat with your new AI friend!</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/","title":"Cookbook Lesson 03 \ud83d\ude80 Build Your First Context-aware Gemini Chatbot in Minutes: The Secret to Speed and Relevance! \u26a1","text":"<p>Scenario: It's Tuesday, July 22, 2025, 12:26 PM CEST. You're a developer, enjoying your morning coffee in Warsaw \u2615, contemplating your next big feature. Suddenly, your director bursts in: \"We need a quick demo of a new, context-aware chatbot for our internal knowledge base \u2013 and the meeting is in 30 minutes! Can you get something ready?\" \ud83e\udd2f</p> <p>Panic? Absolutely not! Not with Gemini and Streamlit. This lesson is your secret weapon to rapidly inject intelligence into your chatbot, focusing on direct, consistent, and reusable context methods that get you up and running with meaningful interactions fast.</p> <p>This lesson is part of the 5 part series Google Cloud - Gemini Cookbook (GitHub Link).</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#1-understanding-context-why-its-your-chatbots-superpower","title":"1. Understanding Context: Why It's Your Chatbot's Superpower \ud83e\uddb8\u200d\u2640\ufe0f","text":"<p>Large Language Models (LLMs) like Gemini are incredible, but they're not clairvoyant. Without explicit guidance, their responses can be generic, vague, or even incorrect when faced with specific or domain-sensitive questions. Context is the \"secret sauce\" that transforms a generic LLM into a specialized, helpful chatbot. It's the information you provide to guide the model's understanding and shape its output. \ud83d\udca1</p> <p>Consider a simple chatbot built with Gemini and Streamlit. If you ask it a very specific question without any context, it might struggle.</p> <p>Example: A Generic Chatbot Responding to a Specific Query</p> <p>Let's say your basic Streamlit app simply forwards user input to Gemini. If you ask about an internal project:</p> <pre><code>$ python llm.py\n\nChat session ID: 4383160272\nEnter your question (or 'exit' to quit)\n\nUser: What are the key milestones for Project Alpha in Q3?\n\nModel: I need a little more information to tell you about\n   Project Alpha's Q3 milestones! Could you please tell me \n   more about what \"Project Alpha\" refers to? \ud83d\ude0a\n\n</code></pre> <p>or via chatbot UI:</p> <pre><code>streamlit run streamlit_app.py\n</code></pre> <p></p> <p>This is where context comes in. By providing context, you tell Gemini exactly what \"Project Alpha\" means in your world. \u2728</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#2-in-context-learning-icl-guiding-with-examples-instantly","title":"2. In-Context Learning (ICL): Guiding with Examples, Instantly \u2728","text":"<p>In-Context Learning (ICL) is the quickest way to demonstrate a desired output pattern to Gemini. You provide explicit examples directly within your prompt, and Gemini learns from these patterns without needing any fine-tuning. It's like teaching by showing!</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#one-shot-learning-a-single-guiding-example","title":"One-Shot Learning: A Single Guiding Example \u261d\ufe0f","text":"<p>For simple tasks, one example might be all you need. You show Gemini a single input-output pair, and it follows that pattern for subsequent queries.</p> <p>Use Case: Simple classification, rephrasing, or straightforward translation. \ud83c\udf10</p> <p>Example: One-Shot Translation</p> <pre><code>User: Translate this English to French.\n      English: Hello.\n      French: Bonjour.\n      English: What is your name?\n      French:\nChatbot (With one-shot example): Quel est votre nom?\n</code></pre> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#few-shot-learning-reinforcing-complex-patterns","title":"Few-Shot Learning: Reinforcing Complex Patterns \ud83d\udcda","text":"<p>When the task is more nuanced or requires a specific output format, providing a few examples helps Gemini better grasp the pattern. It's like providing multiple reference points for complex concepts.</p> <p>Use Case: More nuanced categorization, structured data extraction, or adhering to specific stylistic requirements. \ud83d\udccb</p> <p>Example: Few-Shot Sentiment Analysis</p> <pre><code>\nUser: Review: The delivery was fast!\n      Sentiment: Positive.\n\n      Review: The product broke immediately.\n      Sentiment: Negative.\n\n      Review: The customer service was okay, but the delivery was slow.\n      Sentiment:\nChatbot (With few-shot examples): Mixed/Neutral\n</code></pre> <p></p> <p>Considerations: While powerful for quick guidance, ICL consumes tokens with every prompt, which can impact cost and latency for very long examples or many turns. \ud83d\udcb8\ud83d\udc22</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#3-system-instructions-setting-your-chatbots-personality-and-rules","title":"3. System Instructions: Setting Your Chatbot's Personality and Rules \ud83d\udcdc","text":"<p>System instructions define your chatbot's overarching persona, tone, and behavioral guardrails. This is a foundational layer of context that applies to all subsequent user turns in a chat session, making Gemini's responses consistent and aligned with your brand or application's requirements. It's like giving your bot a permanent job description! \ud83e\uddd1\u200d\ud83d\udcbb</p> <p>You define the \"rules of engagement\" for your chatbot, ensuring it behaves predictably. \ud83d\udea6</p> <p>Code Sample: llm.py (LLM Interaction Logic)</p> <p>System instructions are defined in the <code>llm.py</code> file.</p> <pre><code>chat_session = client.chats.create(\n   model=MODEL_NAME,\n   config=GenerateContentConfig(\n      system_instruction=[\n       \"You're a helpful Gemini AI Chatbot.\",\n       \"Answer user's questions and use simple and clear language.\"\n       \"When possible, reply to user's question with a single sentence or a few sentences.\",\n       \"Free to use emojis.\"\n       \"Be open and friendly. Don't be afraid to ask questions or clarify things.\",\n      ]\n  ),\n)\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#4-context-caching-reusing-static-information-efficiently","title":"4. Context Caching: Reusing Static Information Efficiently \ud83d\udce6","text":"<p>Imagine your chatbot needs to be an expert on a fixed set of documents, like internal reports, product manuals, or, in our case, specific research papers. Sending these large documents with every single API call would be slow and expensive. This is where Context Caching becomes a game-changer.</p> <p>Gemini's Context Caching allows you to process and store static, frequently-referenced content once. You then refer to this cached content using a simple, lightweight name in your subsequent API calls. This drastically saves tokens, reduces latency, and lowers costs, especially when dealing with large files.</p> <p>Example Use Case: Efficient retrieval of information from large, static knowledge bases, optimizing token usage, and simplifying your requests to model. \ud83d\udce6</p> <p>Let update our chatbot a chatbot to be expert on the Gemini family of models, using two key research papers as its knowledge base.</p> <ul> <li>Paper 1: Gemini: A Family of Highly Capable Multimodal   Models.(2312.11805v3.pdf)</li> <li>Paper 2: Gemini 1.5: Unlocking multimodal understanding across millions of   tokens of context   (2403.05530.pdf)Instead   of feeding these PDFs to the model repeatedly, we'll cache them and let our   chatbot use that cached knowledge.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#how-it-works-a-two-step-process","title":"How It Works: A Two-Step Process","text":"<p>Step 1: Create the Cache</p> <p>First, you need to upload your static files and create a CachedContent object. This is a one-time operation. You'll save the name of the cache to use in your application later.</p> <p>Full code: https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/genai/content_cache/contentcache_create_with_txt_gcs_pdf.py</p> <pre><code>system_instruction = \"\"\"\nYou are an expert researcher.\nYou always stick to the facts in the sources provided, and never make up new facts.\nNow look at these research papers, and answer the following questions.\n\"\"\"\n\ncache_objects = [\n    Part.from_uri(\n        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n        mime_type=\"application/pdf\",\n    ),\n    Part.from_uri(\n        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n        mime_type=\"application/pdf\",\n    ),\n]\n\ncontent_cache = client.caches.create(\n    model=\"gemini-2.5-flash\",\n    config=CreateCachedContentConfig(\n        contents=[Content(role=\"user\", parts=cache_objects)],\n        system_instruction=system_instruction,\n        display_name=\"example-cache\",\n        ttl=\"86400s\",\n    ),\n)\n</code></pre> <p>Note: Cache is created with a TTL (time to live). After a specific amount of time, the cache will be deleted.</p> <p>Once the cache is created you will a cache name (e.g., cachedContents/f1e2d3c4-a5b6-7890-a1b2-c3d4e5f6a7b8), which we will use with chatbot.</p> <p>Step 2: Use the Cache in Your Chatbot</p> <p>Once the cache is created, you can use it in your chatbot by passing its name in the <code>GenerateContentConfig</code> object. For example:</p> <pre><code>system_instruction = \"...\"\ncache_name = (\"projects/.../locations/us-central1/keyRings/.../cryptoKeys/...\",)\n\nchat_session = client.chats.create(\n    config=GenerateContentConfig(\n        cached_content=cache_name,\n        system_instruction=None if cache_name else system_instruction,\n    )\n)\n</code></pre> <p>Since the Cache has a defined timelimit, it required a little attention to avoid any potential issues. So I hae created a CacheManager to manage the cache, which will automatically clean up expired caches.</p> <p>Here is the content for <code>cache.py</code></p> <p>Examples:</p> <p>Here is the example of not using Context Cache:</p> <p></p> <p>Here is the example of using Context Cache:</p> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#to-deploy-this-application-on-google-cloud-run","title":"To Deploy This Application on Google Cloud Run:","text":"<ol> <li>Clone this repository and navigate to the directory</li> </ol> <pre><code>git clone https://github.com/msampathkumar/msampathkumar.github.io.git\ncd docs/google-cloud-gemini-cookbook/lesson-03\n</code></pre> <ol> <li>Setup your virtual environment and install dependencies:</li> </ol> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre> <ol> <li>Run the application locally to make sure it works as expected.</li> </ol> <pre><code>streamlit run streamlit_app.py\n</code></pre> <ol> <li>To deploy to Google Cloud Run, you can use the <code>deploy.sh</code> script:</li> </ol> <pre><code>bash deploy.sh\n</code></pre> <p>Github: https://github.com/msampathkumar/msampathkumar.github.io/tree/master/docs/google-cloud-gemini-cookbook</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#congratulations","title":"Congratulations \ud83c\udf89 ( \u25e0\u203f\u25e0 )","text":"<p>Congratulations, You've Achieved a Milestone!</p> <p>You successfully deployed a content-aware chatbot application to Google Cloud Run.</p> <p>In the next lesson, we will delve into two more features that will further enhance your chatbot's context awareness:</p> <ol> <li>RAG: Retrieval Augmented Generation (RAG)</li> <li>Grounding: Using Google Search</li> </ol> <p>Let's continue learning and exploring these innovative tools together!</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/","title":"Cookbook Lesson 04: \ud83d\udd13 Unlock Enterprise AI: Grounding Gemini with RAG and Google Cloud Search \ud83d\ude80","text":"<p>Welcome back! \ud83d\udc4b In Lesson 03, we built fast, relevant chatbots using direct context methods like In-Context Learning \u2728, System Instructions \ud83d\udcdc, and Context Caching \ud83d\udce6. Those are fantastic for quick demos and consistent persona. But what if your chatbot needs to know the latest company sales figures, details from an obscure internal report, or specific client history? That's where Large Language Models (LLMs) hit their limit \u2013 they hallucinate! \ud83d\ude35\u200d\ud83d\udcab</p> <p>Today, we tackle the next frontier: Grounding your Gemini models in real-time, external, and even proprietary data using Retrieval Augmented Generation (RAG), with Google Cloud Search as your powerful engine. \ud83d\udd0d</p> <p>This lesson is part of the 5 part series Google Cloud - Gemini Cookbook (GitHub Link).</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#1-grounding-llms-combating-hallucinations-with-external-knowledge","title":"1. Grounding LLMs: Combating Hallucinations with External Knowledge \ud83d\udee1\ufe0f","text":"<p>LLMs are brilliant pattern matchers, trained on vast amounts of internet data. However, their knowledge is frozen at their last training cut-off date. They don't have real-time access to the internet, nor do they inherently know your company's internal documents, recent sales data, or specific client interactions. When asked about such information, they might:</p> <ul> <li>Refuse to answer: \"I don't have information on that.\" \ud83e\udd37\u200d\u2640\ufe0f</li> <li>Generate generic or outdated information: \"According to general industry   trends...\" \ud83d\uddd3\ufe0f</li> <li>\"Hallucinate\": Make up plausible-sounding but entirely false information.   This is the most dangerous! \ud83d\udea8</li> </ul> <p>Example: A Chatbot Hallucinating on Project Status</p> <p>Let's revisit our chatbot. If you ask about specific, latest information:</p> <pre><code>User: What is the status of our CRM project - Alpha?\n\nModel: I'm sorry, but I don't have access to specific project details like the status of your CRM project Alpha. \ud83d\ude1f I can only provide general information.\n\nUser: What is today's date ?\n\nModel: Today's date is June 16, 2024. \ud83d\udcc5\n</code></pre> <p>This is a problem. In enterprise settings, accuracy is paramount. We need a way to connect Gemini to our actual, verifiable knowledge. This is called grounding. \ud83c\udf31</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#2-introduction-to-retrieval-augmented-generation-rag-your-llms-research-assistant","title":"2. Introduction to Retrieval Augmented Generation (RAG): Your LLM's Research Assistant \ud83e\uddd1\u200d\ud83c\udf93","text":"<p>Retrieval Augmented Generation (RAG) is an architectural pattern that solves the hallucination problem by giving LLMs access to external, up-to-date, and domain-specific information.</p> <p>Think of RAG as giving your LLM a brilliant research assistant: \ud83e\uddd0</p> <ol> <li>Retrieve: When you ask a question, the \"research assistant\" first    searches a vast library (your knowledge base) for relevant documents or    snippets. \ud83d\udcda</li> <li>Augment: It then takes the most relevant findings and gives them to the    LLM. \ud83e\udde9</li> <li>Generate: Finally, the LLM uses this specific, retrieved information    (along with your original query) to formulate an accurate and grounded    response. \u2705</li> </ol> <p>Key Components of a RAG System:</p> <ul> <li>Knowledge Base: Your source of truth \u2013 internal documents, databases,   websites, etc. \ud83d\udcc1</li> <li>Retriever: A system (like a vector database combined with an embedding   model, or a powerful search engine like Google Cloud Search) that can quickly   find the most relevant pieces of information from your knowledge base based   on a query. \ud83d\udd0e</li> <li>Generator: The LLM (Gemini) that synthesizes the answer using the   retrieved context. \ud83e\udde0</li> </ul> <p>Benefits of RAG:</p> <ul> <li>Factuality: Grounding responses in real data dramatically reduces   hallucinations. \u2705</li> <li>Currency: LLMs can answer questions about information that wasn't in   their training data or is constantly changing. \u23f0</li> <li>Domain-Specific Knowledge: Access to proprietary or niche topics. \ud83d\udcbc</li> <li>Attribution: Potential to show users where the information came from   (e.g., \"Source: Policy Manual v2.1\"). \ud83d\udd17</li> </ul> <p>RAG vs. Context Cache: A Crucial Distinction \ud83d\udea8</p> <p>It's vital to differentiate RAG from Context Caching (Lesson 03).</p> <ul> <li>Context Cache: Reuses small, static pieces of pre-loaded or   conversational context. It's about efficiency for fixed data, avoiding   redundant token usage. Think of it as a persistent \"sticky note\" or   short-term memory for repeated instructions or small data blocks. \ud83d\udcdd</li> <li>RAG: Dynamically retrieves specific, often large, and always relevant   chunks of information from a vast, external knowledge base on demand for   each query. It's about expanding the LLM's factual knowledge with new,   current, or private data. \ud83c\udf10</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#3-application-architecture","title":"3. Application Architecture","text":"<p>This application is designed to be a flexible and extensible chatbot that can leverage different grounding techniques. Here's a breakdown of the core components:</p> <pre><code>graph TD\n    subgraph User Interface\n        A[streamlit_app.py]\n    end\n\n    subgraph Core Logic\n        B(llm.py)\n    end\n\n    subgraph Optional Add-ons\n        C[cache.py]\n        D[rag.py]\n    end\n\n    A -- \"Initializes and calls\" --&gt; B\n    B -- \"Optionally uses\" --&gt; C\n    B -- \"Optionally uses\" --&gt; D\n</code></pre> <p>Core Application Logic:</p> <ul> <li><code>streamlit_app.py</code> (UI): This is the user-facing component of the   application, built with Streamlit. It provides the chat interface, handles   user input, and displays the LLM's responses. It's the \"skin\" of our   application.</li> <li><code>llm.py</code> (The Brain): This module is the central nervous system of our   chatbot. It's responsible for all interactions with the Gemini API. It takes   the user's prompt, and based on the selected mode (Default, Context Cache, or   RAG), it constructs the appropriate request to the Gemini model.</li> <li><code>cache.py</code> (Optional Battery): This module manages the Context Cache.   When the \"Use Context Cache\" option is selected, <code>llm.py</code> uses this module to   create and manage a cache of context, which can be reused across   conversations to improve speed and reduce costs.</li> <li><code>rag.py</code> (Optional Battery): This module handles the Retrieval-Augmented   Generation (RAG) functionality. When the \"Use RAG as Tool\" option is   selected, <code>llm.py</code> uses this module to create and manage a RAG corpus. This   allows the LLM to retrieve information from a knowledge base to answer   questions.</li> </ul> <p>Code Links:</p> <ul> <li>streamlit_app.py</li> <li>llm.py</li> <li>cache.py</li> <li>rag.py</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#4-rag-implementation-flow","title":"4. RAG Implementation Flow","text":"<p>Here\u2019s a more detailed look at how the RAG process works within our application when the \"Use RAG as Tool\" option is enabled:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Streamlit UI (streamlit_app.py)\n    participant LLM Brain (llm.py)\n    participant RAG Corpus (rag.py)\n    participant Gemini API\n\n    User-&gt;&gt;Streamlit UI (streamlit_app.py): Enters a prompt\n    Streamlit UI (streamlit_app.py)-&gt;&gt;LLM Brain (llm.py): Sends prompt to get chat session\n    LLM Brain (llm.py)-&gt;&gt;RAG Corpus (rag.py): Initializes RAG corpus\n    RAG Corpus (rag.py)--&gt;&gt;LLM Brain (llm.py): Returns RAG corpus name\n    LLM Brain (llm.py)-&gt;&gt;Gemini API: Creates chat session with RAG tool\n    Gemini API--&gt;&gt;LLM Brain (llm.py): Returns chat session\n    LLM Brain (llm.py)--&gt;&gt;Streamlit UI (streamlit_app.py): Returns chat session\n    Streamlit UI (streamlit_app.py)-&gt;&gt;LLM Brain (llm.py): Sends user prompt\n    LLM Brain (llm.py)-&gt;&gt;Gemini API: Sends prompt to Gemini\n    Gemini API-&gt;&gt;RAG Corpus (rag.py): Retrieves relevant documents\n    RAG Corpus (rag.py)--&gt;&gt;Gemini API: Returns documents\n    Gemini API--&gt;&gt;LLM Brain (llm.py): Generates response based on retrieved documents\n    LLM Brain (llm.py)--&gt;&gt;Streamlit UI (streamlit_app.py): Returns grounded response\n    Streamlit UI (streamlit_app.py)--&gt;&gt;User: Displays response\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#5-application-screenshots","title":"5. Application Screenshots","text":"<p>Youtube: https://youtu.be/JIx4Fr4V6Mw</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#conclusion","title":"Conclusion","text":"<p>This lesson demonstrated how to ground Gemini models with external knowledge using RAG. By leveraging RAG, we can build more accurate, factual, and useful AI applications that can reason about private and real-time data.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/","title":"Project Alpha","text":""},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#project-summary","title":"Project Summary","text":"<p>Project Alpha is a next-generation customer relationship management (CRM) platform designed to revolutionize how businesses interact with their customers. It leverages artificial intelligence and machine learning to provide predictive analytics, automate sales workflows, and offer personalized customer experiences.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#goals","title":"Goals","text":"<ul> <li>To increase customer retention by 20% within the first year of launch.</li> <li>To reduce the sales cycle duration by 15%.</li> <li>To improve sales team productivity by 30%.</li> <li>To provide a single, unified view of the customer across all touchpoints.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#team","title":"Team","text":"<ul> <li>Project Manager: Alice Johnson</li> <li>Lead Developer: Bob Williams</li> <li>Frontend Developer: Charlie Brown</li> <li>Backend Developer: Diana Prince</li> <li>UI/UX Designer: Eve Adams</li> <li>QA Engineer: Frank Miller</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#timeline","title":"Timeline","text":"<ul> <li>Phase 1: Discovery &amp; Planning - Q1 2023 (Completed)</li> <li>Phase 2: Design &amp; Prototyping - Q2 2023 (Completed)</li> <li>Phase 3: Development &amp; Implementation - Q3-Q4 2023 (In Progress)</li> <li>Phase 4: Testing &amp; QA - Q1 2024</li> <li>Phase 5: Launch &amp; Deployment - Q2 2024</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#status","title":"Status","text":"<p>Current Status: In Progress</p> <p>We are currently in the middle of Phase 3. The backend team is focused on building out the core APIs for contact management and analytics. The frontend team is developing the main dashboard and reporting components.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#technologies","title":"Technologies","text":"<ul> <li>Frontend: React, Redux, TypeScript</li> <li>Backend: Python, Django, PostgreSQL</li> <li>AI/ML: TensorFlow, scikit-learn</li> <li>Infrastructure: Google Cloud Platform (GCP), Docker, Kubernetes</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/rag_intro/","title":"Rag intro","text":"<p>Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external data sources when generating responses. Here's a breakdown:</p> <p>What it is:</p> <ul> <li>Combining Retrieval and Generation:</li> <li>RAG combines the strengths of information retrieval systems (like search     engines) with the generative power of LLMs.</li> <li>It enables LLMs to go beyond their pre-trained data and access up-to-date     and specific information.</li> <li>How it works:</li> <li>When a user asks a question, the RAG system first retrieves relevant     information from external data sources (e.g., databases, documents, web     pages).</li> <li>This retrieved information is then provided to the LLM as additional     context.</li> <li>The LLM uses this augmented context to generate a more accurate and     informative response.</li> </ul> <p>Why it's helpful:</p> <ul> <li>Access to Up-to-Date Information:</li> <li>LLMs are trained on static datasets, so their knowledge can become     outdated. RAG allows them to access real-time or frequently updated     information.</li> <li>Improved Accuracy and Factual Grounding:</li> <li>RAG reduces the risk of LLM \"hallucinations\" (generating false or     misleading information) by grounding responses in verified external data.</li> <li>Enhanced Contextual Relevance:</li> <li>By providing relevant context, RAG enables LLMs to generate more precise     and tailored responses to specific queries.</li> <li>Increased Trust and Transparency:</li> <li>RAG can provide source citations, allowing users to verify the information     and increasing trust in the LLM's responses.</li> <li>Cost Efficiency:</li> <li>Rather than constantly retraining large language models, RAG allows for the     introduction of new data in a more cost effective way.</li> </ul> <p>In essence, RAG bridges the gap between the vast knowledge of LLMs and the need for accurate, current, and contextually relevant information.</p> <p>Source: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/","title":"Lesson 5: Advanced RAG with Knowledge Graphs","text":""},{"location":"google-cloud-gemini-cookbook/lesson-05/#cookbook-lesson-05-review-five-takeaways-to-enhance-your-gemini-apps","title":"Cookbook Lesson 05: Review - Five Takeaways to enhance your Gemini Apps","text":"<p>Welcome to Lesson 05 of my Gemini Cookbook series :)</p> <p>The potential of Gemini is limitless, but unlocking its true power requires more than just a \u2018hello world\u2019 app. It demands a strategic approach to performance, cost, and design. In this lesson 05, we\u2019ll transform that potential into reality by exploring 5\ufe0f\u20e3 key takeaways that will elevate your Gemini applications to the next level.</p> <p>In Lessons 01 through 04, we covered essential topics, from building a \u201chello world\u201d app to creating powerful, context-aware chatbots. In this Lesson 05, we\u2019ll do a quick recap of those earlier lessons and then dive into some important tips and takeaways for building great Gemini features applicable to most users.</p> <p>This lesson is the last part of the 5 part series Google Cloud - Gemini Cookbook (GitHub Link).</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#takeaway-01-multi-layered-application","title":"Takeaway 01: Multi-Layered Application","text":"<p>Let me quote from a Google Developer Advocate.</p> <p>\u201cTo effectively monitor your agent, it\u2019s best to adopt a practical, layered approach. Here\u2019s a guide to setting up observability at every stage of the development lifecycle.\u201d - Alvin Prayuda Juniarta Dwiyantoro</p> <p>Building an application and its features can become a cascading challenge, especially when you aim for an app that can be used, maintained, updated, and deployed to production environments. As mentioned in the quote, building applications with a layered approach can help streamline the overall development flow.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#flow-chart","title":"Flow chart","text":"<p>Lesson 04 Github-Link</p> <p></p> <p>This tutorial series built a sophisticated chatbot over four lessons.</p> <ul> <li>Lesson 01: We started with streamlit_app.py to create a basic \u201chello   world\u201d page and learned deployment fundamentals.</li> <li>Lesson 02: We developed llm.py, the chatbot\u2019s \u201cbrain,\u201d using Google\u2019s   GenAI SDK, and designed the user interface for interactive chat sessions.</li> <li>Lesson 03: We added contextual awareness using System Instructions and a   Context Cache to maintain coherent conversations.</li> <li>Lesson 04: We expanded the chatbot\u2019s context to external resources like   Google Search and external data stores via Retrieval Augmented Generation   (RAG).</li> </ul> <p>To maintain simplicity, Lessons 03 and 04 utilize a layered architecture (for code), adding new features as distinct modules. This actually differs with the initial approach, where a single file facilitated early development. As features expanded, I realized the benefits of dedicated modules like RAG.py and Cache.py over modifying existing files, which significantly streamlined development, testing, and debugging.</p> <p>This layered approach proved effective in our application. The following sections detail further takeaways from my experiments and explorations.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#takeaway-02-improve-your-speed","title":"Takeaway 02: Improve your speed","text":"<p>Large Language Models (LLMs) are extensive software applications that utilize significant CPU, memory, and network resources. Much like services such as BigQuery or Cloud SQL, these models are globally hosted, serving users worldwide. In this section, we will observe how you can improve model responses speed.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#021-global-endpoints-ge","title":"021 Global Endpoints (GE):","text":"<p>This section presents my personal explorations and hypotheses.</p> <p>It\u2019s common to assume that proximity to the LLM\u2019s physical location ensures optimal performance. While reduced network latency is a factor, it doesn\u2019t account for the entire picture. LLMs operate as shared services, inherently utilizing a first-in, first-out queuing system. Therefore, the ideal location selection hinges on the equation: min_response(Nearby locations + shorter queue). When incorporating additional variables like time and potential datacenter disruptions (e.g., maintenance, power failures, or even submarine cable issues), the optimal solution evolves to min_response(Nearby *available* locations + shorter queue).</p> <p>However, forecasting all these conditions is challenging. Continuously querying every Gemini model globally to assess response times could paradoxically impede overall performance. This is precisely where Global Endpoints demonstrate their value! They intelligently determine which LLM model will deliver the quickest response, thereby accelerating your interactions.</p> <p>Back in March 2024, I whipped up a demo app that hit a global endpoint instead of a specific regional one, like us-central1 or europe-central2. Since I was working from my place in Warsaw, Poland, with an old router, I figured there might be some lag. But honestly, the huge performance difference between the global endpoint and my go-to europe-west1 was pretty wild-in a good way!</p> <p></p> <p>Based on my personal explorations and practical experience, employing Global Endpoints(GE) can significantly enhance the speed of your Gemini application. While GE are an excellent choice, they do have some limitations.</p> <ol> <li>Data Localization: If your app needs to keep data in a specific place (like    for GDPR), GE might not be the right choice.</li> <li>LLM\u2019s Features Limitation: Certain features, such as RAG (Vector Search)    built in specific locations (e.g., Europe or US-Central), may not be    available for use.</li> </ol>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#022-using-gemini-lite-models","title":"022 Using Gemini-Lite Models","text":"<p>Upon the initial announcement of the Gemini Live API, I was quite enthusiastic, particularly regarding the integration of WebSockets, which I find to be remarkably efficient compared to standard HTTPS requests. Following exploration, I determined that a text-based chat proved to be the optimal choice for my needs, rather than audio or video conferencing.</p> <p>Although the Live API offers robust transcription options for both input and output (https://cloud.google.com/vertex-ai/generative-ai/docs/live-api#transcribe_audio), I found text-based interaction more effective(simple and fast enough) as it allowed me to review my queries before submission.</p> <p>These two considerations led me to consistently utilize the gemini-flash-lite models for straightforward chatbot applications.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#takeaway-03-cost-saving","title":"Takeaway 03: Cost Saving","text":""},{"location":"google-cloud-gemini-cookbook/lesson-05/#031-use-context-caching","title":"031 Use Context Caching","text":"<p>When leveraging Gemini for token usage tracking through observability, you might encounter a pleasant surprise: Gemini\u2019s Implicit Context Caching (ICC) and Explicit Context Caching (ECC) features. By default, ICC is active but can be disabled if necessary.</p> <p>As with any caching mechanism, the effectiveness is determined by the cache-hit to cache-miss ratio. With ICC, cost savings are not guaranteed. For instance, if you process 100\u2013200 distinct queries across various languages, a common token cache is not feasible, and Gemini cannot provide cost savings in such scenarios.</p> <p>In contrast to ICC, ECC offers a guaranteed cost reduction. While ECC is a powerful feature, its implementation is a design choice. Achieving cost-effectiveness with ECC, akin to reaching escape velocity, requires hitting a specific mathematical threshold. It\u2019s not magic, just pure mathematics!</p> <p>My recommendation is to consider using ECC when you have about 200+ queries and need a large context (dataset) in a short duration. For example, (1) you are building an Astro-Science Chatbot knowledgeable in a dozen physics, chemistry, and astrophysics books (2) You have to deal with really smart kids (I hope AI could handle those queries).</p> <p>Here is a quick recap of takeaway 031.</p> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#032-provision-throughput-pt","title":"032 Provision Throughput (PT)","text":"<p>This is Vertex AI only feature.</p> <p>Provisioned Throughput is a fixed-cost, fixed-term subscription available in several term-lengths that reserves throughput for supported generative AI models on Vertex AI. To reserve your throughput, you must specify the model and available locations in which the model runs.</p> <p>It is important to note that exceeding your Provisioned Throughput limit will not halt your requests. While this ensures uninterrupted service by preventing errors, it also means you cannot cap your Gemini costs if budget constraints are a concern.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#takeaway-04-context-awareness","title":"Takeaway 04: Context Awareness","text":"<p>Whether it\u2019s Gemini or some other LLMs, it\u2019s always the same rule. People are calling context the king but when I was learning data science, I heard a similar quote about Data / Information.</p> <p>Let me take a personal example here. Back in 2015, my colleagues had a simple classification challenge but the difficult part was the data distribution. Client has provided 1 TB of data positive data and 10 MB of negative data. (1,000,000 MB of positive data set and 10 MB is -tive dataset). The substantial disparity in data distribution made all our initial models predominantly classified queries as positive, achieving approximately 99.999% accuracy. However it was not the solution we needed.</p> <p>When I say context, it is the data relevancy. If your queries are not relevant to the context or theme you have set for the model, then you may be doing something fundamentally wrong. Imagine studying poetry to improve painting.</p> <p>So to keep your model responses relative to what users need, with gemini I have tested the following approaches</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#041-use-system-instructions","title":"041 Use System Instructions:","text":"<p>Define all the critical details that your model needs to remember. For example, \u201cYou are Bill Nye, the science guy. You are an American science communicator, television presenter, and former mechanical engineer. You are best known as the host of the science education television show Bill Nye the Science Guy and as a science educator in pop culture.\u201d (Copied from wikipedia)</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#042-use-grounding-with-google-search","title":"042 Use Grounding with Google Search:","text":"<p>To the scope limited, many times LLM models are not connected to search engines like Google (or DuckDuckGo or Yahoo). By enabling this, you can allow Gemini Model to browse the internet and get the latest information like date, climate and so on.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#043-use-rag","title":"043 Use RAG:","text":"<p>Similar to Grounding with Google Search, you may find your in a case where you want the model to automatically learn information about a certain project or certain dataset that is private to you or your team or organization. In such cases, using RAG has turned out to be an amazing investment.</p> <p>Using Vertex AI\u2019s RAG feature, all the provided data is converted into vectors and stored in a Vector database. When user queries, relevant data is identified from this Vector database and included as part of the overall context for the Model. As you may expect now this allows the model to get relevant answers.</p> <p>Here is a quick recap of takeaway 04.</p> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#takeaway-05-simplicity","title":"Takeaway 05: Simplicity","text":"<p>Simplicity stands as a crucial principle. Throughout my development and refinement of applications, I\u2019ve experimented with various LLM assisting tools. The Gemini CLI consistently proved most effective. Yet, even with this preference, it occasionally exhibited hallucinations, while other selections yielded even less favorable results. One time as an experiment, I allowed these changes to persist, meticulously reviewing each. This process finally led to a visually very appealing application with substantial code. During testing, as errors emerged, the model would rectify them. After several iterations, I realized I no longer genuinely enjoyed the application I had built. While aesthetically pleasing, I lacked the confidence to assume ownership for future feature additions or bug fixes. Gemini handled much of the work, but my engineering accountability kept me on edge.</p> <p></p> <p>Consider this: if a bug exists within 10 lines of code, the probability of quickly identifying it is 10%; however, in 500 lines of code, that probability drops to 0.2% (The layered approach in modules 03 and 04 simplified my code, resulting in faster debugging and error correction.)</p> <p>Simple designs are easier to understand, build, and maintain. As mentioned in Takeaway 01, a layered approach is helpful, but always prioritize simplicity in its execution. Ultimately, ensure your code is readable and simple for humans.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#conclusion","title":"Conclusion:","text":"<p>When building Gemini applications, a clear, layered, and straightforward design leads to robust, debuggable, and maintainable solutions, enabling powerful, efficient, and cost-effective Gemini-powered experiences.</p> <p>As this series concludes, remember that simplicity is key to great Gemini apps. These five takeaways are your secret ingredients for success. The next adventure awaits with the Google Agent Development Kit.</p> <p>Don\u2019t stop here! Our next post will explore the Google Agent Development Kit (ADK), built on the Google Gen AI SDK, to enhance your Gemini development journey. Lets go beyond the basics and build something outstanding in Gen AI.</p> <p>Now something for fun :)</p> <p></p>"},{"location":"services/","title":"Build a World-Class Developer Platform","text":"<p>In today's market, a powerful developer experience (DX) is not a luxury\u2014it's a necessity. Top-tier DX attracts the best developers, accelerates innovation, and creates a loyal community around your products. With over a decade of engineering leadership and 5 years driving developer outreach (GTM) for global platforms, I provide the strategic insights you need to build a developer ecosystem that wins.</p> <p>My services are designed for one purpose: to make your platform the obvious choice for developers.</p>"},{"location":"services/#core-offerings","title":"Core Offerings","text":"<p>1. Developer Experience (DX) &amp; Product Audits</p> <p>Is your onboarding seamless? Is your documentation clear and inspiring? I perform deep-dive audits of your developer-facing products to identify friction points and opportunities for delight.</p> <ul> <li>Onboarding Journey Analysis: From \"Hello World\" to production, I map and refine every step.</li> <li>API Design &amp; Usability Review: Ensuring your APIs are intuitive, consistent, and powerful.</li> <li>Documentation &amp; Content Assessment: Transforming your docs from a simple reference to a strategic asset.</li> </ul> <p>2. AI Integration &amp; Strategy</p> <p>The AI revolution is here. I help you navigate the complexities of integrating cutting-edge AI into your stack, specializing in Google Cloud and Gemini.</p> <ul> <li>Generative AI Implementation: Strategy for using large language models (LLMs) to create next-generation features.</li> <li>AI-Powered Developer Tooling: Designing intelligent tools that make developers more productive.</li> <li>Internal AI Upskilling: Workshops and training to level-up your engineering team.</li> </ul> <p>3. Developer Go-to-Market (GTM) &amp; Content Strategy</p> <p>Great technology is only half the battle. You need a GTM strategy that builds momentum and a community that advocates for you.</p> <ul> <li>Technical Content Creation: Articles, tutorials, and videos that showcase the power of your platform.</li> <li>Community Building Playbook: Strategies for fostering a vibrant, engaged developer community.</li> <li>Keynote &amp; Workshop Delivery: Inspiring and educating developers at conferences and events.</li> </ul> <p>4. Corporate Training &amp; Enablement</p> <p>Empower your engineering teams with the skills to build at the speed of AI. As a Google Cloud Developer Programs Engineer and a lead for major AI training initiatives, I deliver world-class workshops on Google Gemini and the Google Agent Development Kit (ADK).</p> <ul> <li>Customized Workshops: Tailored training programs, from introductory bootcamps to advanced, deep-dive sessions on agentic AI, context engineering, and memory.</li> <li>Hands-On Codelabs: Practical, hands-on labs that take developers from theory to production-ready code.</li> <li>Team Onboarding: Accelerate your team's adoption of Gemini and ADK, ensuring they can leverage these powerful tools to their full potential.</li> </ul>"},{"location":"services/#ready-to-build-your-empire","title":"Ready to Build Your Empire?","text":"<p>Let's connect and build a developer platform that dominates the market.</p> <p>Contact me on LinkedIn</p>"}]}