{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Digital Garden","text":"<p>This is a space where I share my learnings, thoughts, and experiments. You'll find a collection of my personal notes and things I am learning.</p>"},{"location":"#gemini-cookbook","title":"Gemini Cookbook","text":"<p>I'm excited to introduce my \"Cookbook\" series! This is where I'll be sharing step-by-step guides and practical examples for various technologies.</p> <p>The first cookbook in the series is the Google Cloud Gemini Cookbook.</p>"},{"location":"#blog","title":"Blog","text":"<p>I also have a blog where I post short articles and updates. It's a place for more informal thoughts and quick tips.</p> <p>I hope you find these resources helpful!</p>"},{"location":"Gemini/","title":"Getting Started: Gemini Cookbook Setup","text":"<p>This guide provides step-by-step instructions to set up your local environment and Google Cloud project to follow the recipes in the Gemini Cookbook.</p>"},{"location":"Gemini/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>Python: Version 3.8 or higher.</li> <li>Google Cloud Account: An active GCP account with billing enabled. If   you're new, you can sign up for a   free trial.</li> <li>Basic Knowledge: Familiarity with Python, the command line, and Git.</li> </ul>"},{"location":"Gemini/#1-environment-setup","title":"1. Environment Setup","text":"<p>First, let's set up the project on your local machine.</p>"},{"location":"Gemini/#clone-the-repository","title":"Clone the Repository","text":"<p>Open your terminal and clone the cookbook's GitHub repository:</p> <pre><code>git clone https://github.com/msampathkumar/msampathkumar.github.io.git\ncd msampathkumar.github.io\n</code></pre>"},{"location":"Gemini/#set-up-a-virtual-environment","title":"Set Up a Virtual Environment","text":"<p>It's a best practice to use a virtual environment to manage project dependencies.</p> <pre><code># Create a virtual environment\npython3 -m venv .venv\n\n# Activate the virtual environment\n# On macOS and Linux:\nsource .venv/bin/activate\n# On Windows:\n# .\\.venv\\Scripts\\activate\n</code></pre>"},{"location":"Gemini/#install-dependencies","title":"Install Dependencies","text":"<p>Install the common Python packages needed for the cookbook examples:</p> <pre><code>pip install google-generativeai google-cloud-aiplatform streamlit python-dotenv\n</code></pre>"},{"location":"Gemini/#2-google-cloud-configuration-cli","title":"2. Google Cloud Configuration (CLI)","text":"<p>Next, configure the Google Cloud CLI (<code>gcloud</code>) to interact with your GCP project.</p>"},{"location":"Gemini/#install-and-initialize-google-cloud-sdk","title":"Install and Initialize Google Cloud SDK","text":"<p>If you don't have it, install the Google Cloud SDK. After installation, initialize it:</p> <pre><code>gcloud init\n</code></pre> <p>Follow the on-screen prompts to log in, select your GCP project, and set a default region.</p>"},{"location":"Gemini/#authenticate-for-local-development","title":"Authenticate for Local Development","text":"<p>For local development, authenticating with your user credentials is the easiest way to get started.</p> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"Gemini/#enable-required-apis","title":"Enable Required APIs","text":"<p>You need to enable the Vertex AI API to use Gemini models.</p> <pre><code># Set your project ID (if you didn't during gcloud init)\nexport PROJECT_ID=\"your-gcp-project-id\"\ngcloud config set project $PROJECT_ID\n\n# Enable the Vertex AI API\ngcloud services enable aiplatform.googleapis.com --project $PROJECT_ID\n</code></pre> <p>Note: Replace <code>your-gcp-project-id</code> with your actual Google Cloud Project ID.</p>"},{"location":"Gemini/#next-steps","title":"Next Steps","text":"<p>You are all set! Your environment is configured and you're ready to start building with Gemini.</p> <p>Head over to the Full Lesson Plan to see all the available recipes.</p>"},{"location":"cookbook/","title":"Google Cloud Gemini Cookbook: A Practical Guide to Learn Fundamentals and Build Applications","text":""},{"location":"cookbook/#vision-executive-summary","title":"Vision &amp; Executive Summary","text":"<p>This project is a cookbook-style series designed to teach developers and AI enthusiasts how to build practical, real-world applications using Google Cloud's Gemini models. Through a series of hands-on blog posts and a central GitHub repository, this guide will provide clear, step-by-step instructions, making generative AI accessible even to those with limited prior experience. The goal is to empower builders, foster a collaborative community, and showcase the power of Gemini.</p>"},{"location":"cookbook/#guiding-principles","title":"Guiding Principles","text":"<ul> <li>Practical First: Focus on hands-on examples and code snippets that solve   real problems.</li> <li>Clarity and Simplicity: Provide clear, step-by-step instructions that are   easy to follow.</li> <li>Gemini Focused: Deep-dive into Google Cloud Gemini, its specific   features, and its ecosystem.</li> <li>Fundamental Concepts: Cover the necessary foundational knowledge to use   Gemini effectively.</li> </ul>"},{"location":"cookbook/#target-audience","title":"Target Audience","text":"<p>This series is for developers, AI enthusiasts, and anyone interested in learning how to build practical AI applications with Gemini.</p>"},{"location":"cookbook/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic Python programming knowledge.</li> <li>A Google Cloud Platform (GCP) account with billing enabled.</li> <li>Familiarity with the command line and GitHub is helpful.</li> </ul>"},{"location":"cookbook/#content-outline-lesson-plan","title":"Content Outline &amp; Lesson Plan","text":"<p>The series will be released as a sequence of lessons, each building upon the last.</p> <ul> <li> <p>Lesson 1: Building a Basic Chatbot with Gemini and Streamlit</p> </li> <li> <p>Objective: Introduce the fundamentals of the Gemini API and build a     simple, interactive chatbot and deploy to Cloud.</p> </li> <li>Core Concepts: API keys, model initialization, generating text,     streaming responses.</li> <li> <p>Tech Stack: Python, <code>google-genai</code> SDK, Streamlit.</p> </li> <li> <p>Lesson 2: Enhancing the Chatbot with Memory and Gemma</p> </li> <li> <p>Objective: Add conversational memory to the chatbot and explore using     open models like Gemma for specific tasks.</p> </li> <li>Core Concepts: Chat history management, context passing, integrating     local/open-source models.</li> <li>Tech Stack: Vertex AI Memory Bank, Gemma, (Optional) Google ADK.</li> </ul>"},{"location":"cookbook/#future-lessons-proposed-agenda","title":"Future Lessons (Proposed Agenda)","text":"<ul> <li> <p>Lesson 3: Unlocking Multimodality with Gemini Pro Vision</p> </li> <li> <p>Objective: Build an application that can understand and analyze     information from both images and text simultaneously.</p> </li> <li>Use Case Example: An app that takes a picture of a whiteboard diagram     and generates code, or identifies products in an image and searches for     them online.</li> <li> <p>Core Concepts: Multimodal prompts, image data handling, combining     visual and text inputs, prompt engineering for vision models.</p> </li> <li> <p>Lesson 4: Building a Knowledge Base Q&amp;A with RAG</p> </li> <li> <p>Objective: Create a Retrieval-Augmented Generation (RAG) system that     answers questions based on a custom document set (e.g., PDFs, text files).</p> </li> <li>Use Case Example: A chatbot that can answer questions about a company\u2019s     internal policy documents.</li> <li> <p>Core Concepts: Vector embeddings, vector databases (e.g., ChromaDB,     Pinecone), document chunking, semantic search.</p> </li> <li> <p>Lesson 5: Advanced RAG with Knowledge Graphs</p> </li> <li> <p>Objective: Go beyond simple vector search by building a RAG system that     understands the relationships between entities in your data, leading to     more accurate and context-aware answers.</p> </li> <li>Use Case Example: A financial analyst bot that can answer complex     queries like \"Which companies in our portfolio have board members who also     sit on the boards of their competitors?\"</li> <li> <p>Core Concepts: Entity and relationship extraction, building a knowledge     graph (e.g., with Neo4j), translating natural language to graph queries     (e.g., Cypher), combining graph retrieval with LLM generation.</p> </li> <li> <p>Lesson 6: Creating Autonomous Agents with Function Calling</p> </li> <li> <p>Objective: Empower Gemini to interact with external tools and APIs to     perform actions in the real world.</p> </li> <li>Use Case Example: A personal assistant that can check the weather, send     an email, or book a meeting by calling external APIs.</li> <li> <p>Core Concepts: Tool definition, function calling, structured data     extraction, handling API errors and responses.</p> </li> <li> <p>Lesson 7: Building Collaborative AI with Multi-Agent Systems</p> </li> <li> <p>Objective: Design a system where multiple specialized AI agents     collaborate to solve a complex problem that a single agent could not handle     alone.</p> </li> <li>Use Case Example: A research team with a \"Web Search\" agent, a \"Data     Analyst\" agent, and a \"Report Writer\" agent that work together to produce a     market analysis.</li> <li> <p>Core Concepts: Agent roles and specialization, inter-agent     communication, task decomposition, state management, and using a     manager/orchestrator agent.</p> </li> <li> <p>Lesson 8: Practical AI Safety and Model Evaluation</p> </li> <li> <p>Objective: Learn to build responsible, reliable AI applications and     objectively measure their performance before they reach production.</p> </li> <li>Use Case Example: Adding a validation step to a customer service bot to     ensure its answers are factually correct and non-toxic before sending them     to a user.</li> <li> <p>Core Concepts: Implementing guardrails, protecting against prompt     injection, detecting and mitigating bias, using evaluation frameworks     (e.g., RAGAs, TruLens) to measure faithfulness and relevance.</p> </li> <li> <p>Lesson 9: Deploying and Scaling on Google Cloud</p> </li> <li> <p>Objective: Take a prototype application and prepare it for production.</p> </li> <li>Core Concepts: Containerizing with Docker, deploying to Cloud Run,     managing API keys securely with Secret Manager, monitoring and logging.</li> </ul>"},{"location":"cookbook/#distribution-community-strategy","title":"Distribution &amp; Community Strategy","text":"<ul> <li>Source of Truth: A public GitHub repository will host all code,   resources, and drafts.</li> <li>Primary Publications: Blog posts will be published on Medium.com and   Dev.to to reach a broad developer audience.</li> <li>Community Engagement: Announcements, key takeaways, and discussions will   be shared on X (formerly Twitter) and LinkedIn to foster community   interaction and feedback.</li> </ul>"},{"location":"cookbook/#potential-impact-success-metrics","title":"Potential Impact &amp; Success Metrics","text":"<ul> <li>Empower Developers: Lower the barrier to entry for building and deploying   AI-powered applications.</li> <li>Foster Community: Create a hub for Gemini users to share knowledge,   collaborate, and get feedback.</li> <li>Showcase Gemini: Highlight the versatility and power of Gemini for   solving real-world problems.</li> <li>Success Metrics: Track GitHub stars/forks, blog post views/claps, social   media engagement, and community contributions.</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/ProjectMgmt/RACI/","title":"RACI Matrix","text":"<p>A RACI chart can be an extremely effective way to define project roles, give direction to each team member and stakeholder, and ensure work gets done efficiently. Having a RACI chart available throughout the duration of your project as a quick visual can be invaluable. In this reading, we will cover the function of a RACI chart and its components and explore how project managers use RACI charts to define and document project roles and responsibilities.</p> <p>Elements of a RACI chart A RACI chart creates clear roles and gives direction to each team member and stakeholder. Over your career, you may hear a RACI chart referred to as a Responsibility Assignment Matrix (RAM), RACI diagram, or RACI matrix. The ultimate goal of this chart is to clarify each person\u2019s role on your project.</p> <p>First, let\u2019s break down each of the roles people can be assigned:</p> <p>R: Responsible: who gets the work done</p> <p>A: Accountable: who makes sure the work is done</p> <p>C: Consulted: who gives input or feedback on work</p> <p>I: Informed: who needs to know the outcome</p> <p>Note that RACI charts can be organized in different ways, depending on personal preference, number of tasks being assigned, and number of people involved. In the previous video, we showed you one RACI chart format. The template below shows another way a typical RACI chart might be organized:</p> <p></p> <p>Responsible Individuals who are assigned the \u201cresponsible\u201d role for a task are the ones who are actually doing the work to complete the task. Every task needs at least one responsible party. It\u2019s a best practice to try to limit the number of team members assigned to a task\u2019s responsible role, but in some cases, you may have more than one.</p> <p>A couple of questions to ask yourself when determining which person or people should be placed in the responsible role for a given task are:</p> <p>What department does the work fall under?</p> <p>Who will perform the work?</p> <p>It is helpful to evaluate the people on your team to determine the role that suits them. Remember that you may need to list roles rather than names, if some people take on more than one role.</p> <p>Let\u2019s dig deeper into our example with Office Green. Our task is to develop price points for the project, and the Financial Analyst will complete the work for this task. Therefore, we will list \u201cFinancial Analyst\u201d in the responsible role for this task in the RACI chart.</p> <p>A section of a RACI chart, where the Financial Analyst is in the \"responsible\" role Accountable The \u201caccountable\u201d person is responsible for making sure the task gets done. It is important to have only one individual accountable for each task. This helps clarify ownership of the task. The accountable person ultimately has the authority to approve the deliverable of the responsible party.</p> <p>In order to determine who should be tagged as the accountable team member, consider:</p> <p>Who will delegate the task to be completed?</p> <p>Who will review the work to determine if the task is complete?</p> <p>You may encounter a situation where the responsible party is also accountable, but where possible, it is helpful to separate these roles. Ensuring that accountability is not shared ensures that there is no confusion on who the ownership belongs to.</p> <p>Continuing with our Office Green example, you have assigned the \u201caccountable\u201d role to the Head of Finance. The Head of Finance has to make sure the project stays in budget and makes a profit, so they have the ultimate authority over the price points for the product. Therefore, they will need to approve the Financial Analyst\u2019s work on the task.</p> <p>A section of a RACI chart, where the Head of Finance is in the \"accountable\" role Consulted Team members or stakeholders who are placed in the \u201cconsulted\u201d role have useful information to help complete the task. There is no maximum or minimum number of people who can be assigned a \u201cconsulted\u201d role, but it\u2019s important that each person has a reason for being there.</p> <p>Here are a few ways you can help identify who is appropriate for the role:</p> <p>Who will the task impact?</p> <p>Who will have input or feedback for the responsible person to help the work be completed?</p> <p>Who are the subject matter experts (SMEs) for the task?</p> <p>The consulted people will be in frequent, two-way communication with the responsible party, so it is key to make sure that the right people are in this role to help accomplish the task efficiently and correctly.</p> <p>Back to the project at Office Green, we\u2019ve got a \u201cresponsible\u201d Financial Analyst and an \u201caccountable\u201d Head of Finance. Who else would need to provide input on the product\u2019s price points? Whose decisions and feedback will directly affect the task? The Director of Product will need to be consulted on the matter, as they oversee all product offerings. This person will have information about potential changes to the product and how these changes might affect price points.</p> <p>A section of a RACI chart, where the Director of Product is in the \"consulted\" role Informed Individuals who are identified as needing to be \u201cinformed\u201d need to know the final decisions that were made and when a task is completed. It is common to have many people assigned to this category and for some team members to be informed on most tasks. Team members or stakeholders here will not be asked for feedback, so it is key to make sure people who are in this group only require status updates and do not need to provide any direct feedback for the completion of the effort.</p> <p>Key questions to ask yourself in order to ensure that you have appropriately captured individuals in the \u201cinformed\u201d role are:</p> <p>Who cares about this task\u2019s completion?</p> <p>Who will be affected by the outcome?</p> <p>Now that you\u2019ve determined who is responsible, accountable, and consulted on the Office Green project task, it is time to determine who needs to be informed about the task. Your Financial Analyst has set the price points with input from the Director of Product, and the Head of Finance has approved. You will now need to inform the Sales Team about the final price points, as they will need this information to sell the product.</p> <p>A section of a RACI chart, where the Sales Team is in the \"informed\" role Pro tip: You could end up with a large number of team members and stakeholders who are placed in the \u201cinformed\u201d role. If so, make sure that you have a plan to keep them informed that is not labor-intensive. Something as easy as view-only access to your project plan or meeting notes could prevent you from having to create separate communications along the way.</p> <p>Key takeaway The RACI chart is a valuable tool. It can help you define and document project roles and responsibilities, give direction to each team member and stakeholder, and ensure work gets done efficiently. A RACI chart can also help you analyze and balance the workload of your team. While it may take many revisions to make sure that your team members and stakeholders are being placed into the right roles in your RACI chart, doing this work up front helps save time and prevent miscommunications later on.</p>"},{"location":"blog/ProjectMgmt/Tasks-and-Milestones/","title":"Tasks &amp; Milestones","text":"<p>A project task is an activity that needs to be accomplished within a set period of time and is assigned to one or more individuals for completion. The work of a project is broken down into many different project tasks.</p> <p>Tip1: When creating a task, start with a verb. Example: \u201cDevelop a proposal.\u201d, \"Create \", Tip2: Provide as much as possible when creating tasks.</p> <p>A project milestone is an important point within the project schedule that usually signifies the completion of a major deliverable. Milestones are significant checkpoints in your project, and keeping track of them helps ensure that your project is on schedule to meet its goals.</p> <p></p> <p>Setting tasks can help you clearly define milestones. You can do this in two ways:</p> <p>Top-down scheduling: In this approach, the project manager lays out the higher-level milestones, then works to break down the effort into project tasks. The project manager works with their team to ensure that all tasks are captured.</p> <p>Bottom-up scheduling: In this approach, the project manager looks at all of the individual tasks that need to be completed and then rolls those tasks into manageable chunks that lead to a milestone.</p>"},{"location":"blog/ProjectMgmt/Tasks-and-Milestones/#work-breakdown-structure-wbs-tool","title":"Work breakdown Structure (WBS) (Tool)","text":"<p>A work breakdown structure is tool to help us organize our work in a way that makes sense for our team. It\u2019s like a map of the project, showing what we\u2019re going to do and how it will be done.</p> <p>Once way to do this is by build a GRAPH.</p>"},{"location":"blog/ProjectMgmt/Tasks-and-Milestones/#workload-balance-strategy","title":"Workload Balance Strategy","text":"<p>WBS can also become a good workload balance strategy. You assign the tasks to team member, ensuring that the workload is equally managed across all of your team members. (Also, assigning the tasks can make feel people the ower of their work.)</p>"},{"location":"blog/ProjectMgmt/Tasks-and-Milestones/#steps-to-build-a-wbs","title":"Steps to build a WBS","text":"<p>There are three main steps to follow when creating a WBS:</p> <ol> <li> <p>Start with the high-level, overarching project picture. Brainstorm with your    team to list the major deliverables and milestones. Example: Imagine you are    planning a company event. Your major milestones might include categories    like \u201csecure venue,\u201d \u201cfinalize guest logistics,\u201d and \u201cestablish agenda.\u201d</p> </li> <li> <p>Identify the tasks that need to be performed in order to meet those    milestones. Example: You could break a milestone like \u201csecure venue\u201d down    into tasks like \u201cresearch venues,\u201d \u201ctour and decorate space,\u201d \u201cmake down    payment,\u201d and so on.</p> </li> <li> <p>Examine those tasks and break them down further into sub-tasks. Example: You    could break down a task like \u201ctour and decorate space\u201d further into    sub-tasks like \u201corganize decorating committee,\u201d \u201cpurchase decorations,\u201d    \u201cassign decorating responsibilities,\u201d and so on.</p> </li> </ol> <p></p> <p>Learn more at https://www.lucidchart.com/blog/how-to-create-a-work-breakdown-structure-and-why-you-should</p>"},{"location":"blog/ProjectMgmt/top-level-summary/","title":"Phases of Project (Plant pals Example","text":"<ol> <li>Get started on the Plant Pals Operations and Training plan</li> </ol> <p>In the initiation phase, you made a project charter for the Plant Pals Operations and Training plan. Now you\u2019re ready for the planning phase. You\u2019ll use the charter to create sustainable fulfillment and delivery protocols and support your customer base.</p> <ol> <li>Identify major milestones and assign task owners</li> </ol> <p>First, you\u2019ll define the work your team needs to do for the plan\u2014like sourcing materials and training employees. Creating a work breakdown structure (WBS) diagram and spreadsheet will help you identify and organize major milestones and assign task owners.</p> <ol> <li>Map project schedule and tasks using a Gantt chart</li> </ol> <p>Next, you\u2019ll add those tasks and milestones\u2014along with due dates and durations\u2014to a Gantt chart. This chart helps you clarify and map out task timelines and dependencies, so your team knows what to do at each stage.</p> <ol> <li>Estimate costs and create a budget</li> </ol> <p>Once you\u2019ve set your schedule, it\u2019s time to estimate the costs of your milestones and tasks. Your budget should include both one-time and recurring expenses\u2014as well as a buffer to cover any overages.</p> <ol> <li>Create a Statement of Work (SoW)</li> </ol> <p>When your team hires a vendor to complete a project milestone, you\u2019ll create a Statement of Work (SoW). This legal document defines what you need from the vendor (and what they need from you), so everyone knows what\u2019s expected.</p> <ol> <li>Create a risk management plan</li> </ol> <p>Next, you\u2019ll assess potential risks to the budget and schedule, like staffing shortages or shipping delays. Identifying, evaluating, and preparing for specific risks helps you keep the project on track if things go wrong.</p> <ol> <li>Create a communication plan</li> </ol> <p>When it\u2019s time to train employees, you\u2019ll need to manage various communications among stakeholders. Your communication plan will track senders and recipients, communication goals, key dates, and other details. That way, everyone gets the right information at the right time.</p> <ol> <li>Organize the project artifacts</li> </ol> <p>Finally, to keep your project artifacts organized, you\u2019ll create a project plan in a central location. Your team members and stakeholders can use it to find project documents, and you can refer to it when you plan future projects.</p>"},{"location":"blog/posts/blog-template/","title":"H1 Heading","text":"<p>Hello World!</p>"},{"location":"blog/posts/blog-template/#h2-heading","title":"H2 Heading","text":"<p>Hello world!:</p> <pre><code>&gt;&gt;&gt; \"hello_world\"\nhello_world\n</code></pre>"},{"location":"blog/posts/conductor-gemini-cli/","title":"Give Your AI a Brain: Context-Driven Development with Conductor for Gemini CLI","text":"<p>Stop coding in \"YOLO Mode\". Learn how to give Gemini CLI the full context of your project for better, more accurate results</p> <p>[Header Image Idea: A split screen. Left side: A messy desk with loose post-it notes labeled \"YOLO Mode\". Right side: A neat desk with a structured binder labeled \"Conductor - CDD\".]</p> <p>\"Failing to plan, is planning to fail.\"</p> <p>Software development isn't just typing code. It\u2019s planning, tracking, executing, testing, and iterating. For large corporate projects, we have heavyweights like Jira. But what about pet projects, POCs, or solo features? We are often stuck in the middle ground\u2014Jira is overkill, but having zero structure leaves us vulnerable to scope creep and losing direction.</p> <p>Enter the era of AI-assisted coding. If we humans need structure to stay focused, our AI tools need it even more. If you\u2019re just firing random prompts at Gemini CLI, you are coding in \"YOLO Mode.\" The AI has no memory of what you asked five minutes ago and no idea what the end goal is.</p> <p>How do we bring structured planning to the \"AI Way\" of doing things?</p>"},{"location":"blog/posts/conductor-gemini-cli/#introducing-conductor-for-gemini-cli","title":"Introducing Conductor for Gemini CLI","text":"<p>Conductor is an open-source, optional extension for the Gemini CLI. It bridges the gap between unstructured prompting and full-blown project management tools.</p> <p>You can think of Conductor as a lightweight project manager that lives directly inside your terminal and repository. It introduces Context-Driven Development (CDD) to the Gemini CLI experience.</p> <ul> <li>Website: Google Developers Blog Introduction</li> <li>Repo: GitHub (Apache-2.0 License)</li> </ul>"},{"location":"blog/posts/conductor-gemini-cli/#the-why-context-is-king","title":"The \"Why\": Context is King","text":"<p>Why should you care about another tool? Because context is what separates a generic code snippet from a perfectly integrated feature.</p> <p>Conductor adds a strict protocol used to specify, plan, and implement software features. By maintaining files in your repository that define the product guidelines, tech stack, and current feature specifications, Conductor ensures that every prompt you send to Gemini is automatically enriched with the bigger picture.</p> <p>It stops the AI from guessing your tech stack and forces it to adhere to your agreed-upon plan.</p>"},{"location":"blog/posts/conductor-gemini-cli/#the-how-a-practical-walkthrough","title":"The \"How\": A Practical Walkthrough","text":"<p>Let's look at how Conductor actually works on a local machine.</p>"},{"location":"blog/posts/conductor-gemini-cli/#step-1-installation","title":"Step 1: Installation","text":"<p>Installing Conductor is as simple as adding any other Gemini extension.</p> <pre><code>gemini extensions install https://github.com/gemini-cli-extensions/conductor\n\n</code></pre>"},{"location":"blog/posts/conductor-gemini-cli/#step-2-initialization-giving-the-ai-context","title":"Step 2: Initialization (Giving the AI Context)","text":"<p>Just like you <code>git init</code> a repo, you need to initialize Conductor in your project root.</p> <p>(Recommended Screenshot/GIF: Run this command and show the output indicating files were created)</p> <pre><code># Start a Gemini session\ngemini\n\n# Inside the session run:\nconductor init\n</code></pre> <p>As you do this, Gemini CLI works to fulfill the requirements and creates a new folder named <code>conductor/</code> populated with guideline files.</p> <p>(Recommended Screenshot: Show your iTerm file tree showing this structure)</p> <pre><code>conductor/\n\u251c\u2500\u2500 product-guidelines.md  &lt;-- How we build things here\n\u251c\u2500\u2500 product.md             &lt;-- What this product is\n\u251c\u2500\u2500 tech-stack.md          &lt;-- The tools we use (React, Python, etc.)\n\u2514\u2500\u2500 tracks/                &lt;-- Where active work lives\n\n</code></pre> <p>These files are accessible by both you and Gemini. You should fill out <code>tech-stack.md</code> and <code>product.md</code> first. This is the baseline context Gemini will now use for everything in this project.</p>"},{"location":"blog/posts/conductor-gemini-cli/#step-3-the-workflow-tracks-specs-and-plans","title":"Step 3: The Workflow (Tracks, Specs, and Plans)","text":"<p>Conductor organizes work into \"Tracks\" (like a feature branch or a Jira ticket).</p> <p>1. Create a Track Let's say we want to add a chatbot feature.</p> <p>(Recommended Screenshot: The output of creating a track)</p> <pre><code># Setup conductor\n# Note: Conductor will create a folder structure for you.\n/conductor:setup\n\n# Create a new track.\n# Example: To create blog post on Quantum Physics\n# Note: Conductor will ask a few follow-up questions to create a plan and an implementation track\n/conductor:newTrack create blog post on Quantum Physics\n\n# Check all open tracks for this project.\n/conductor:status\n\n# To start work on all open tracks for this project.\n/conductor:implement\n</code></pre> <p>This creates a new folder structure under <code>conductor/tracks/chatbot_feature</code> containing a <code>spec</code> file.</p> <p>2. Define the Spec You (the human) edit the <code>spec</code> file in that folder to define what you want. You don't say how to code it, just define the requirements.</p> <p>3. Generate the Plan This is where the magic happens. You ask Conductor to look at your spec, your tech stack, and your product guidelines, and generate a step-by-step implementation plan.</p> <p>(Recommended Video Clip: Record the terminal as Gemini generates the plan. It's satisfying to watch it list the steps.)</p> <pre><code># Inside gemini session, ensuring you have the track selected\nconductor plans generate\n\n</code></pre> <p>Gemini will generate a detailed <code>plan.md</code> file listing numbered tasks required to build your spec.</p> <p>4. Execute the Tasks Now, instead of randomly prompting, you tell Conductor to execute a specific step from the plan it just created.</p> <pre><code># Inside gemini session\nconductor tasks execute 1\n\n</code></pre> <p>Gemini now has the ultimate context: It knows the overall product goal, the specific feature spec, the exact step in the plan it needs to do, and the tech stack rules. It will write the code, and then update the plan to mark task #1 as complete.</p>"},{"location":"blog/posts/conductor-gemini-cli/#conclusion","title":"Conclusion","text":"<p>If you enjoy working with Gemini CLI, Conductor sharpens its focus significantly. It turns an AI coding assistant into an AI development partner that understands the \"definition of done.\"</p> <p>Stop YOLO prompting. Give your AI some context.</p>"},{"location":"blog/posts/conductor-gemini-cli/#part-2-the-youtube-video-script","title":"Part 2: The YouTube Video Script","text":"<p>Video Concept: A fast-paced, \"over-the-shoulder\" look at using Conductor. The tone should be enthusiastic and practical. Estimated Duration: 3-5 minutes.</p> Time Visual Idea Script (Voiceover/Host) 0:00 Face to camera (or engaging terminal intro screen). Text overlay: \"AI Coding: YOLO Mode vs. God Mode\" Are you just throwing random prompts at your terminal hoping for working code? That\u2019s coding in YOLO mode. Your AI has no memory and no context. Today, we\u2019re fixing that. 0:20 Screen share: Google Blog Post about Conductor. Highlight \"Context-Driven Development\". Google recently introduced \"Conductor\" for the Gemini CLI. It introduces \"Context-Driven Development.\" 0:35 Graphic: Simple diagram showing: YOU -&gt; (Prompts) -&gt; GEMINI -&gt; (Random Code?). Then change it to: YOU + CONDUCTOR FILES -&gt; (Rich Context) -&gt; GEMINI -&gt; (Structured Features). Think of Conductor as a lightweight project manager that lives right inside your repo. It forces you to define what you want before the AI starts guessing how to write it. 0:55 Screen Share: iTerm on Macbook. Show an empty project folder. Run the install command. Let\u2019s look at it in action on my local machine. Installing it is just a standard Gemini extension command. 1:10 Screen Share: Terminal. Run <code>gemini start</code> then <code>conductor init</code>. Crucial: Zoom in on the file tree updating on the left. Once we start Gemini, we run <code>conductor init</code>. Watch the file tree on the left. Boom. 1:25 Screen Share: Briefly open the generated markdown files in VS Code or Vim. Point out <code>tech-stack.md</code>. It created a <code>conductor</code> folder. This is the AI's brain for your project. You define your <code>tech-stack</code> here once, and Gemini never has to guess again. 1:45 Screen Share: Terminal. Run <code>conductor tracks create new-landing-page</code>. Show the new folders created. Work happens in \"Tracks\". Let's create a track for a new landing page. Conductor sets up the structure for us. 2:05 Screen Share: Briefly show the <code>spec</code> file. Then back to terminal. Run <code>conductor plans generate</code>. Let the recording run while Gemini types out the plan. I\u2019ve put my requirements into the spec file. Now, I\u2019ll ask Conductor to generate the plan. Look at that\u2014it\u2019s reading my spec, my tech stack, and creating a step-by-step checklist. 2:35 Screen Share: Show the generated <code>plan.md</code>. Point to task #1. Back to terminal: run <code>conductor tasks execute 1</code>. We have a plan. Now we just execute. \"Conductor, do task one.\" 2:55 Screen Share: Show Gemini generating the actual code files for task 1. Because it has all that context, it knows exactly which files to create and what stack to use, without me having to re-explain it in the prompt. 3:15 Face to camera. Conductor turns Gemini CLI from a smart autocomplete into an actual junior developer that understands your project's plan. The links to install it are in the description below. Stop YOLO coding, and give your AI some brainpower."},{"location":"blog/posts/documentation-driven-development/","title":"Proposal: Documentation-Driven Development (D3) - A New Pattern for the AI Era","text":"<p>If you have worked in IT for any length of time, you know one thing for sure: user documentation is often seen as a undervalued chore. In the standard Software Development Life Cycle (SDLC) process, documentation is often an afterthought, usually started just before a product release. As an expert in software development, I can confirm that creating human-readable documentation has always been a significant challenge.</p> <p>However, over the last 18 months, LLMs and Agents have shifted this perspective. Slowly but surely documentation is now starting to become a forethought. This change builds on earlier ideas about Documentation-Driven Development (DDD). The core principle of DDD suggests that if an idea is thoroughly documented, then it can evolve into a product. Similarly, a feature must be drafted, to be developed, tested and then deployed; otherwise, it is considered a bug.</p> <p>Given the rising prevalence of Vibe coding and AI-assisted development, documentation has arguably become a central concern. Reflecting on my own experience (a self-evaluation), I pose this question to you: Are you, in essence, practicing Documentation-Driven Development (D3) while coding with AI?</p> <p>Let's discuss this in the comments. Is documentation still a 'boring chore' for your team, or has it become your primary development tool? I\u2019m seeing a massive shift toward D3\u2014are you seeing it too?</p>"},{"location":"blog/posts/documentation-driven-development/#ai-softwareengineering-d3-vibecoding-gemini","title":"AI #SoftwareEngineering #D3 #VibeCoding #Gemini","text":""},{"location":"blog/posts/gemini-2.5-flash-image-preview/","title":"Gemini 2.5 - A New Era for AI Image Generation","text":""},{"location":"blog/posts/gemini-2.5-flash-image-preview/#gemini-25-flash-vs-gemini-20-flash-a-new-era-for-ai-image-generation","title":"Gemini 2.5 Flash vs. Gemini 2.0 Flash: A New Era for AI Image Generation","text":"<p>Google has once again pushed the boundaries of generative AI with the release of Gemini 2.5 Flash Image (Preview), a significant upgrade to its predecessor, Gemini 2.0 Flash Image. While both models offer impressive image generation capabilities, Gemini 2.5 Flash introduces a suite of new features and enhancements that provide you with unprecedented creative control and higher-quality outputs. In this blog post, we will delve into the key differences between these two models, highlighting the advancements that make Gemini 2.5 Flash a game-changer for creators and developers.</p>"},{"location":"blog/posts/gemini-2.5-flash-image-preview/#key-advancements-in-gemini-25-flash","title":"Key Advancements in Gemini 2.5 Flash","text":"<p>Conversational Editing and Iterative Refinement: One of the most significant improvements in Gemini 2.5 Flash is its ability to engage in conversational editing. This means you can now refine and edit images through a natural language dialogue. Instead of starting from scratch with a new prompt for every minor adjustment, you can provide iterative feedback. For instance, you can ask the model to \"make the sky a deeper blue\" or \"add a dog in the background,\" and it will make the changes while preserving the rest of the image. This feature was more limited in Gemini 2.0, making the creative process in 2.5 much more fluid and intuitive.</p> <p></p> <p>You can find my final image at the link.</p> <p>Multi-Image Fusion and Composition: Gemini 2.5 Flash introduces multi-image fusion, a powerful feature that allows you to combine up to three images to create a single, seamless visual. This opens up a world of creative possibilities, from placing a product in a new environment to creating surrealist art. While Gemini 2.0 had some capabilities in this area, 2.5 takes it a step further with more sophisticated blending and composition, resulting in more photorealistic and coherent images.</p> <p></p> <p>Character and Style Consistency: Maintaining a consistent character or style across multiple images has always been a challenge for AI image generators. Gemini 2.5 Flash addresses this with enhanced character and style consistency. You can now generate images of the same character in different scenes, outfits, or poses, and the model will maintain their key features and appearance. This is a massive leap forward for storytelling, branding, and any application that requires a consistent visual identity.</p> <p>Improved Visual Quality(Aspect ratio) and Text Rendering: Gemini 2.5 Flash boasts a noticeable improvement in overall visual quality. The images generated are more detailed, vibrant, and photorealistic than those from Gemini 2.0. Furthermore, the model's ability to render text within images has been significantly enhanced. This means you can create posters, invitations, and other designs with clear, legible text, a task that was often a struggle for previous models.</p>"},{"location":"blog/posts/gemini-2.5-flash-image-preview/#_1","title":"Gemini 2.5 - A New Era for AI Image Generation","text":""},{"location":"blog/posts/gemini-2.5-flash-image-preview/#the-verdict-a-leap-forward-in-creative-control","title":"The Verdict: A Leap Forward in Creative Control","text":"<p>While Gemini 2.0 Flash was a powerful tool in its own right, Gemini 2.5 Flash represents a significant evolution in AI image generation. The introduction of conversational editing, multi-image fusion, and enhanced consistency gives creators an unparalleled level of control over the final output. These advancements, combined with the improvements in visual quality and text rendering, make Gemini 2.5 Flash an indispensable tool for anyone looking to push the boundaries of their creativity. As AI-powered image generation continues to evolve, Gemini 2.5 Flash has set a new standard for what's possible.</p>"},{"location":"blog/posts/gemini-2.5-flash-image-preview/#links","title":"Links:","text":"<ul> <li>Gemini 2.5 Flash Image Preview</li> <li>Gemini 2.0 Flash Image</li> </ul>"},{"location":"blog/posts/gemini-as-your-culinary-guide/","title":"Gemini as your Culinary Guide","text":"<p>title: \"Gemini: Your Culinary Guide in Italy \ud83c\uddee\ud83c\uddf9 \ud83c\udf55\" date: 2025-08-17 authors: [sampathm] description: \"Gemini as you wonderful guide to Italian Cuisines. categories: - General</p>"},{"location":"blog/posts/gemini-as-your-culinary-guide/#gemini-your-culinary-guide-in-italy","title":"Gemini: Your Culinary Guide in Italy \ud83c\uddee\ud83c\uddf9 \ud83c\udf55","text":"<p>Savoring Italy, One Personalized Recommendation at a Time</p> <p>Last week I had a wonderful trip to Bologna, Italy. Since I don\u2019t speak italian \ud83c\uddee\ud83c\uddf9my biggest concern was how do I ask for Indian vegetarian food ? Indian Vegetarian also known as lato-vegeterian. In short, they are vegans who also eat any milk products.</p> <p>Usually my trips are planned ahead of time, but this trip was n\u2019t. Luckily the Gemini proved invaluable. Instead of manually searching for answers to various inquiries, I leveraged its capabilities for real-time recommendations.</p> <p>To Gemini, I'd provide restaurant details I am in, and it would swiftly analyze reviews to offer a curated list of top choices based on my criteria. I visited 3-4 restaurants, and by maintaining the same chat thread, Gemini developed a clear understanding of my preferences, ensuring each recommendation was highly tailored.</p> <p>With Gemini assistance, I am looking forward to visiting more Italian restaurants \ud83c\uddee\ud83c\uddf9\ud83c\udf55\ud83d\ude0b #LLM #AI #Gemini #EverydayTech #TravelTips #Innovation</p>"},{"location":"blog/posts/gemini-cli-cheatsheet-printouts/","title":"Gemini CLI Cheatsheet Printout","text":"<p>Google's Gemini models are a powerful family of multimodal AI models. For developers and power users who live in the terminal, <code>gcloud</code> provides a robust command-line interface to interact with Gemini on Vertex AI.</p> <p>To help you keep the most common commands at your fingertips, I've created a handy CLI cheatsheet available in both light and dark themes. Download your preferred version and get started!</p>"},{"location":"blog/posts/gemini-cli-cheatsheet-printouts/#light-theme","title":"Light Theme","text":"<p>Download Links: PNG &amp; PDF.</p>"},{"location":"blog/posts/gemini-cli-cheatsheet-printouts/#dark-theme","title":"Dark Theme","text":"<p>Download Links: PNG &amp; PDF.</p>"},{"location":"blog/posts/gemini-cli-cheatsheet-printouts/#getting-started","title":"Getting Started","text":"<p>Before you can use these commands, you need to set up your local environment and Google Cloud project. Follow the Gemini Cookbook Setup guide to get everything configured correctly.</p> <p>Happy coding!</p>"},{"location":"blog/posts/gemini-cli-top-commands/","title":"Master Your Workflow: Top Gemini CLI Commands You Should Know","text":""},{"location":"blog/posts/gemini-cli-top-commands/#master-your-workflow-top-gemini-cli-commands-you-should-know","title":"Master Your Workflow: Top Gemini CLI Commands You Should Know","text":"<p>Picture this: you're deep in your coding workflow. You need to refactor a complex function, write a clear and concise commit message for your latest changes, and then figure out an obscure shell command to deploy your work. Each task requires switching context\u2014moving from your editor to a browser, then to your Git client, and back again. It's a constant juggling act that breaks your focus.</p> <p>What if you could do all of this from one place, your terminal? The Gemini CLI brings the power of Google's state-of-the-art AI directly to your command line, acting as a seamless, context-aware pair programmer. It's designed to understand your project, streamline your tasks, and keep you in the flow.</p>"},{"location":"blog/posts/gemini-cli-top-commands/#getting-started-your-first-steps","title":"\ud83d\ude80 Getting Started: Your First Steps","text":"<p>Before you can master your workflow, you need to get set up and oriented. These commands are the foundation.</p> <ul> <li> <p>/init \ud83c\udfac: This is where your journey should begin. Running <code>/init</code> in your project directory allows Gemini to analyze your codebase and create a <code>GEMINI.md</code> file.</p> </li> <li> <p>/about \u2139\ufe0f: Curious about your setup? This command quickly displays your Gemini CLI version, the underlying model being used, and your current authentication method.</p> </li> <li> <p>/auth \ud83d\udd11: Security and access are paramount. The <code>/auth</code> command lets you configure how you authenticate with Google's AI services.</p> </li> <li> <p>/help &amp; /docs \ud83d\udcda: The <code>/help</code> command is your go-to for a quick overview. For more details, <code>/docs</code> opens the full documentation in your browser.</p> </li> <li> <p>/quit \ud83d\udc4b: When you're done for the day, simply use <code>/quit</code> to exit the Gemini CLI application.</p> </li> </ul>"},{"location":"blog/posts/gemini-cli-top-commands/#core-workflow-commands","title":"\ud83d\udee0\ufe0f Core Workflow Commands","text":"<p>These are the commands you'll use day-to-day to interact with Gemini and your codebase.</p> <ul> <li> <p><code>!</code> (Shell Commands) \ud83d\udc1a: This is one of the most powerful features. Press <code>SHIFT + 1</code> to enter \"shell mode,\" allowing you to execute shell commands or describe them in natural language.</p> </li> <li> <p>/tools \ud83e\uddf0: Ever wonder what capabilities Gemini has? The <code>/tools</code> command lists all available tools Gemini can use to help you.</p> </li> <li> <p>/editor \u270d\ufe0f: Pro-Tip: For complex, multi-line prompts, use <code>/editor</code> to set your preferred external editor (like Vim or VS Code), then use <code>Ctrl+X</code> to open it.</p> </li> </ul>"},{"location":"blog/posts/gemini-cli-top-commands/#managing-your-conversations","title":"\ud83d\udcac Managing Your Conversations","text":"<p>A conversation with Gemini is a valuable asset. Here's how to manage it effectively.</p> <ul> <li> <p>/chat (save, resume, list, delete, share) \ud83d\udcbe: The <code>/chat</code> command is a powerful tool for managing your session history. <code>save</code> a conversation, <code>resume</code> it later, or <code>share</code> it as a file.</p> </li> <li> <p>/clear \ud83e\uddf9: Need a fresh start? <code>/clear</code> will wipe the screen and your current conversation history.</p> </li> <li> <p>/compress \ud83e\udde0: When a conversation gets long, <code>/compress</code> intelligently summarizes the context to keep things focused.</p> </li> </ul>"},{"location":"blog/posts/gemini-cli-top-commands/#customization-and-stats","title":"\ud83c\udfa8 Customization and Stats","text":"<p>Tailor the CLI to your liking and keep an eye on your usage.</p> <ul> <li> <p>/theme \ud83d\udd8c\ufe0f: Customize your experience. The <code>/theme</code> command allows you to change the look and feel of the Gemini CLI.</p> </li> <li> <p>/stats \ud83d\udcca: Curious about your usage? <code>/stats</code> provides statistics for your current session, including model and tool usage.</p> </li> </ul>"},{"location":"blog/posts/gemini-cli-top-commands/#gemini-cli-keyboard-shortcuts-you-should-know","title":"\u2328\ufe0f Gemini CLI Keyboard Shortcuts You Should Know","text":"<p>To work even faster, it's worth learning a few essential keyboard shortcuts.</p> <ul> <li><code>ESC</code>: Cancel a long-running task or clear your input.</li> <li><code>Ctrl+C</code>: Quit the application (press twice).</li> <li><code>Ctrl+X</code> / <code>Meta+Enter</code>: Open your input in an external editor.</li> <li><code>Enter</code>: Send your query to Gemini.</li> </ul>"},{"location":"blog/posts/gemini-cli-top-commands/#common-use-cases-to-try-today","title":"\u2728 Common Use Cases to Try Today","text":"<p>Here are a few practical ideas to get you started:</p> <ul> <li>Quick Refactoring: Use <code>@</code> to provide a file as context (e.g., <code>@src/utils.py</code>) and ask Gemini: \"Refactor the <code>calculate_total</code> function in this file to be more readable.\"</li> <li>Commit Message Generation: Run <code>!git diff --staged</code> and then ask Gemini: \"Based on the diff, write a conventional commit message.\"</li> <li>Command-Line Coach: Paste a complex shell command and ask: <code>tar -xzvf archive.tar.gz</code> \"What do each of these flags do?\"</li> </ul> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 &gt;   tar -xzvf archive.tar.gz what does each flag do                                                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\u2726 Of course. The command tar -xzvf archive.tar.gz is used to decompress and extract files from a gzipped tar archive.\n\n  Here is a breakdown of what each flag does:\n\n   * -x: eXtract. This tells tar to get files out of the archive.\n   * -z: gZip. This tells tar to decompress the archive using gzip. This is necessary for files ending in .gz.\n   * -v: Verbose. This makes tar list each file as it is being extracted, so you can see the progress.\n   * -f: File. This flag must be followed by the name of the archive file you want to process (in this case, archive.tar.gz).\n\n  You can think of the command as saying: \"eXtract the gZipped contents Verbosely from the File named archive.tar.gz.\"\n</code></pre>"},{"location":"blog/posts/gemini-cli-top-commands/#takeaway-youre-one-step-ahead","title":"\ud83c\udf89 Takeaway: You're One Step Ahead!","text":"<p>Congratulations on making it through this guide! By familiarizing yourself with these commands and workflows, you're already one step ahead of many developers who might not be tapping into the full potential of having an AI assistant directly in their terminal.</p> <p>Thank you for investing the time to level up your skills. We encourage you to start integrating these commands into your daily routine. You'll be surprised at how much time you can save and how much more you can accomplish.</p> <p>Now we'd love to hear from you! What new command did you learn that you're most excited to try? Share your thoughts in the comments below!</p>"},{"location":"blog/posts/miyamoto-musashi-10-principles/","title":"10 principles of Miyamoto Musashi","text":"<p>??? About Miyamoto Musashi</p> <pre><code>Miyamoto Musashi, a legendary figure in the annals of Japanese history, stands as a symbol of unparalleled mastery in the way of the sword. Renowned not only for his exceptional martial skills but also for his profound philosophical insights, Musashi is a name that resonates through the ages. Born in the late 16th century, during a tumultuous period of Japan\u2019s history, Musashi\u2019s life was marked by a relentless pursuit of perfection in the art of swordsmanship. His remarkable journey led him to become the author of the timeless classic, \u201cThe Book of Five Rings,\u201d which continues to inspire warriors and thinkers alike, transcending both time and culture. This introductory glimpse into the life and legacy of Miyamoto Musashi only scratches the surface of his remarkable story, a tale of discipline, relentless ambition, and the quest for martial and spiritual enlightenment.\n</code></pre> <ol> <li>Accept everything just the way it is.</li> <li>Do not seek pleasure for its own sake.</li> <li>Do not under any circumstances, depend on a partial feeling.</li> <li>Think lightly of yourself and deeply of the world.</li> <li>Be detached from desire your whole life long.</li> <li>Do not regret what you have done.</li> <li>Never be jealous.</li> <li>Never let yourself be surrounded by a separation.</li> <li>Resentment and complaint are appropriate neither for oneself nor others.</li> <li>Do not let yourself be guided by the feeling of lust or love.</li> </ol> <p>Source: Link</p>"},{"location":"blog/posts/poem-sky-sea-truth/","title":"Sky, Sea and Truth","text":"<p>A vast blue canvas, Sea and sky, a mirror's gleam, Moments, swiftly pass.</p> <p>Life's a rushing stream, We stand still, in silent thought, Seeds grow, futures bloom.</p> <p>Birth and death entwined, Happiness, a passing shade, Truth, a fleeting breath.</p> <p>Sea and sky reflect, Constant change, yet ever the same, The now, the only truth.</p>"},{"location":"blog/posts/quantum-ai-era/","title":"The Quantum Leap in AI: Faster, Greener, and More Powerful","text":""},{"location":"blog/posts/quantum-ai-era/#the-quantum-leap-in-ai-faster-greener-and-more-powerful","title":"The Quantum Leap in AI: Faster, Greener, and More Powerful","text":"<p>The world is buzzing with the incredible power of Artificial Intelligence. From writing emails to creating stunning images, AI models like Gemini are transforming our digital lives. But this explosion in AI capability comes with a cost \u2013 a voracious appetite for energy and computational power that is pushing the boundaries of our current technology. What if there was a way to not only meet these demands but to unlock a new level of intelligence altogether?</p> <p>Enter the strange and wonderful world of quantum computing. It\u2019s a field that sounds like science fiction, but it holds the potential to revolutionize everything we know about computation. This isn't just about faster computers; it's about a fundamentally new way of processing information. In this post, we'll explore the exciting synergies between quantum computing and AI, and how this powerful partnership could lead to a future with faster, greener, and more powerful artificial intelligence.</p>"},{"location":"blog/posts/quantum-ai-era/#the-ai-race-and-its-hurdles","title":"The AI Race and Its Hurdles","text":"<p>The artificial intelligence landscape is evolving at an unprecedented pace. Large Language Models (LLMs) and generative AI are no longer concepts from sci-fi movies; they are here, assisting us, creating for us, and even conversing with us. This rapid advancement, however, comes with significant challenges. Training these sophisticated AI models requires immense computational power, often running on vast data centers filled with thousands of powerful Graphics Processing Units (GPUs). This process consumes staggering amounts of energy, contributing to growing concerns about environmental impact and sustainability. Beyond energy, there's also the sheer computational hurdle: for certain incredibly complex problems, even the most powerful classical supercomputers struggle to find solutions within practical timeframes. The sheer scale and complexity of the data involved can push classical computing architectures to their limits.</p>"},{"location":"blog/posts/quantum-ai-era/#quantum-computing-to-the-rescue","title":"Quantum Computing to the Rescue","text":"<p>So, what is this \"quantum computing\" that promises to be a game-changer? Forget the bits (0s and 1s) of classical computers. Quantum computers use \"qubits,\" which can represent both 0 and 1 simultaneously, thanks to phenomena like superposition and entanglement. This allows quantum computers to process vast amounts of information in parallel and explore many possibilities at once. While still in its early stages, this technology has the potential to solve certain types of problems that are practically impossible for even the fastest conventional supercomputers.</p> <p>How does this apply to AI? Imagine an AI trying to sift through an astronomical number of possibilities to find the optimal solution \u2013 be it designing a new drug molecule, optimizing a global logistics network, or even recognizing complex patterns in chaotic data. A classical computer approaches this by trying options sequentially or through clever heuristics. A quantum computer, by leveraging its unique properties, could potentially explore these possibilities in a fundamentally different and much faster way.</p>"},{"location":"blog/posts/quantum-ai-era/#the-quantum-ai-synergy","title":"The Quantum-AI Synergy","text":"<p>The marriage of quantum computing and AI isn't just about making existing AI faster; it's about making it smarter and more sustainable.</p> <ul> <li> <p>Faster AI: Quantum machine learning (QML) algorithms can accelerate the training of AI models, particularly deep learning networks. Tasks like complex matrix operations and processing vast datasets, which are bottlenecks for classical AI, could be handled with unprecedented speed on quantum processors. This means AI models could learn more efficiently and deliver insights faster.</p> </li> <li> <p>More Powerful AI: Quantum algorithms are particularly adept at solving complex optimization problems. For AI, this translates into more precise predictions, better decision-making in intricate scenarios, and the ability to tackle problems currently deemed intractable. Fields like drug discovery, financial modeling, and materials science, where AI struggles with immense combinatorial complexities, could see revolutionary breakthroughs.</p> </li> <li> <p>Greener AI: Perhaps one of the most compelling aspects is the potential for significant energy savings. While classical AI models consume megawatts of power, quantum computers generally operate at kilowatt levels for their core computations. Although the cryogenic cooling required for many quantum systems does consume energy, the actual information processing is highly efficient. Furthermore, advancements in room-temperature quantum computing are emerging, which could further reduce the overall energy footprint. By performing complex calculations more efficiently, quantum AI could drastically reduce the energy demands associated with training and running the AI models of the future, helping to mitigate the environmental impact of the burgeoning AI industry.</p> </li> </ul>"},{"location":"blog/posts/quantum-ai-era/#gemini-and-the-future","title":"Gemini and the Future","text":"<p>Consider the capabilities of a cutting-edge AI model like Google's Gemini \u2013 capable of multimodal understanding, complex reasoning, and creative generation. Now, imagine a \"Quantum Gemini.\" If Gemini's underlying architecture could be accelerated and enhanced by quantum processing units, its ability to learn from vast datasets, understand nuanced context, and generate highly sophisticated responses could reach new heights. Such a system could process information with unparalleled efficiency, learn from fewer examples, and even explore solution spaces that are currently beyond our grasp. This is not a certainty, but rather a compelling expectation \u2013 a potential path for explorers in both quantum computing and AI to navigate, leading to truly groundbreaking advancements.</p>"},{"location":"blog/posts/quantum-ai-era/#the-dawn-of-a-new-intelligence","title":"The Dawn of a New Intelligence","text":"<p>The journey into the quantum realm of AI is just beginning. What we've discussed today is not a guaranteed roadmap, but a glimpse into a future brimming with possibilities. The synergy between quantum computing and artificial intelligence has the potential to redefine what's possible, making AI not only more powerful and intelligent but also more sustainable and aligned with our planet's needs. As we stand at the crossroads of these two revolutionary technologies, one thing is certain: the future of intelligence\u2014be it human, artificial, or quantum\u2014is set to be an extraordinary adventure.</p>"},{"location":"blog/posts/spot-fakes-leaders/","title":"How to Spot an Ineffective Manager or Leader","text":"<p>Identifying poor leadership is a critical skill for career growth and mental well-being. In many corporate environments, a person's position or title can mask their true leadership capabilities. Ineffective leaders often exploit communication gaps and their perceived authority, creating a toxic environment for their team.</p> <p>Here are three common warning signs.</p>"},{"location":"blog/posts/spot-fakes-leaders/#1-they-stifle-autonomy-and-micromanage","title":"1. They Stifle Autonomy and Micromanage","text":"<p>A classic sign of a bad leader is the tendency to suffocate their team's freedom and autonomy. They take broad, collaborative goals and twist them into high-pressure, individual tasks.</p> <p>For example, a company-wide directive like \"we need to improve application performance\" becomes a direct, isolating order to a single developer: \"Your application's performance is not satisfactory. Fix it.\" What should be a team effort becomes a solitary burden, setting the individual up for failure and burnout.</p>"},{"location":"blog/posts/spot-fakes-leaders/#2-they-withhold-recognition","title":"2. They Withhold Recognition","text":"<p>Ineffective leaders are often the last to acknowledge your hard work and the first to take credit for it. While your peers might celebrate your achievements, a bad manager will either ignore them or offer empty praise that feels disingenuous, often followed by piling on more responsibilities.</p> <p>These managers are deeply concerned with their own reputation. They enjoy the benefits of your team's successes to bolster their own image, but they don't genuinely care about or understand the work you do. This behavior leads to a feeling of being overworked, underappreciated, and overlooked for growth opportunities.</p>"},{"location":"blog/posts/spot-fakes-leaders/#3-they-lack-accountability-and-offer-fake-support","title":"3. They Lack Accountability and Offer Fake Support","text":"<p>Trust is the foundation of any healthy team. Bad leaders erode this trust with a pattern of dishonesty and avoidance. They are quick to make promises they can't or won't keep.</p> <p>They might demand you work weekends to fix a critical issue but will not invest their own time to understand the technology or the root cause of the problem. They are not problem-solvers; they are masters of evasion. This lack of genuine support breaks your confidence in their leadership and leaves you feeling isolated.</p>"},{"location":"blog/posts/spot-fakes-leaders/#what-should-you-do","title":"What Should You Do?","text":"<p>If these signs resonate with your experience, it's essential to take action to protect your career and well-being.</p>"},{"location":"blog/posts/spot-fakes-leaders/#1-prioritize-your-well-being-consider-your-options","title":"1. Prioritize Your Well-being: Consider Your Options","text":"<p>If you feel consistently undermined and unsupported, the best long-term solution may be to find a new role. Ineffective managers are often skilled at applying pressure and may even resort to bad-mouthing team members when things go wrong.</p> <p>Leaving a toxic environment is not a sign of failure. While the thought of changing jobs can be daunting, your mental health and financial future are better served in a supportive workplace. Many people suffer severe burnout and trauma from a lack of support at work. Not everyone has the luxury to quit immediately, but it's crucial to start thinking about a way out.</p>"},{"location":"blog/posts/spot-fakes-leaders/#2-plan-a-strategic-exit","title":"2. Plan a Strategic Exit","text":"<p>Because these managers often lack deep technical expertise, they tend to focus on project-level timelines and deliverables, expecting you to handle all the details. They often lay out work in two- to three-month cycles. Use this time to your advantage. Plan your exit, update your resume, and start networking.</p>"},{"location":"blog/posts/spot-fakes-leaders/#3-document-everything","title":"3. Document Everything","text":"<p>Keep a clear record of your accomplishments, tasks, and any relevant interactions. Use email to confirm conversations and decisions. This documentation is not just for performance reviews; it's a way to protect yourself and maintain a clear, factual account of your contributions.</p> <p>Lastly, don't forget to remember that you're in control of your career path. Don't let the one bad environment dictate your future.</p>"},{"location":"blog/posts/spot-fakes-leaders/#4-stay-positive","title":"4. Stay Positive","text":"<p>Remember, this is not an end; it's a new beginning. You have the power to shape your own destiny and create a better work environment for yourself and others.</p>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/","title":"Python in Google Cloud: More Than Just Examples for Gemini","text":"<p>Is Python the ultimate language for Google Cloud development? Is it Google's official recommendation, or do alternatives like Java, Go, or Rust hold sway in specific scenarios? These are crucial questions for any developer building on Google Cloud, particularly when diving into Generative AI with Gemini. This article offers an opinionated, yet fact-driven, perspective on Python's prominent role, clarifying why you often encounter Python examples in Google Cloud documentation, especially for Gemini features.</p>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/#the-genesis-of-a-preference-pythons-rise-in-my-early-career","title":"The Genesis of a Preference: Python's Rise in My Early Career","text":"<p>My journey into advanced programming and data analysis began long before \"Data Science\" became a household term. Before an internship at Standard Chartered Bank in 2012, my programming toolkit was dominated by C and Java, with Python being more of a personal project companion.</p> <p>The internship presented a unique challenge: sifting through mountains of Unix system health and performance data to predict maintenance needs and identify system improvements. My initial inclination was to leverage Java, given my familiarity. However, my mentor, Asjorn, encouraged me to explore Python. This pivotal decision unveiled Python's unexpected power and laid the groundwork for its consistent presence in my projects.</p> <p>Over the years, Python became the language of choice for almost all data science initiatives I led, driven by several compelling advantages:</p> <ol> <li>Code Readability and Maintainability: Python's clean, intuitive syntax    drastically lowered the learning curve, making code easier to understand,    write, and debug. This is a benefit Google itself recognizes and    inspired Go language    development in certain ways.</li> <li>Efficiency for Lean Teams: Python's conciseness allowed smaller    development teams to achieve significant outcomes, often comparable to what    larger teams using more complex or \"low-code\" solutions might accomplish.    Its extensive ecosystem contributes to rapid development.</li> <li>Prioritizing Availability Over Raw Performance: For many projects, the    immediate availability of services, scripts, and tools proved more critical    than bleeding-edge performance. With compute power becoming increasingly    affordable, Python enabled rapid deployment and iteration. This is    particularly true for scripting and prototyping.</li> <li>\"Batteries Included\" for Data-Intensive Tasks: Python boasts a rich    standard library and a vast ecosystem of third-party packages, particularly    beneficial for data manipulation and analysis. This extensive library    support is a consistent benefit for cloud development.</li> </ol>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/#pythons-enduring-reign-in-data-science-and-cloud-computing","title":"Python's Enduring Reign in Data Science and Cloud Computing","text":"<p>Python's appeal in data science continued to grow for two additional reasons, deeply intertwined with cloud development:</p> <ol> <li>Rapid Prototyping: The interactive interpreter and tools like    Google Colab make Python invaluable for agile    data analytics workflows. This capability is essential for quickly testing    ideas and developing proofs of concept.</li> <li>Robust Community and Specialized Libraries: Public libraries such as    NumPy, Pandas, and Scikit-learn, despite often having performance-critical    core logic written in lower-level languages like C or Fortran, offer    user-friendly Python interfaces. Google taps into Python's extensive library    ecosystem, with machine learning and AI projects benefiting from libraries    like TensorFlow.</li> </ol>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/#decoding-pythons-presence-in-google-clouds-generative-ai-gemini","title":"Decoding Python's Presence in Google Cloud's Generative AI (Gemini)","text":"<p>This brings us to Python's specific role within Google Cloud's Generative AI, particularly with Gemini. You'll find a significant number of Python examples for Gemini features, and this isn't by accident.</p> <p>The primary reason for the prevalence of Python samples is its readability and speed of prototyping. For quickly demonstrating capabilities, enabling developers to get started swiftly, and iterating on ideas, Python excels. The official Google GenAI SDK is indeed available for Python, alongside JavaScript/TypeScript, Go, and Java. The Python GenAI SDK provides an interface to integrate Google's generative models into Python applications, supporting both the Gemini Developer API and Vertex AI APIs.</p> <p>While Python is a widely-used, high-level programming language known for its simplicity and readability, it's crucial to understand the nuance: these Python examples are provided for ease of use and quick experimentation, not as an exclusive recommendation for all production environments. Google Cloud itself supports a variety of popular languages, each suited for different development needs.</p> <p>Lastly, about the absence of official GenAI SDKs for languages like Rust or C# is often a matter of demand and ecosystem focus, though Google Cloud generally supports a broad range of languages for various services. In fact, Google Cloud Service Extension uses a very interesting mix of GO, Java, Python and Web Assembly.</p>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/#beyond-python-a-polyglot-approach-to-cloud-development","title":"Beyond Python: A Polyglot Approach to Cloud Development","text":"<p>Google Cloud's philosophy embraces a polyglot environment, recognizing that no single language fits every use case. While Python shines in data science, scripting, and rapid development, other languages are vital:</p> <ul> <li>Go: Developed by Google, Go is a natural fit for GCP, known for   performance, simplicity, and efficiency in cloud computing, distributed   systems, and microservices.</li> <li>Java: A robust, object-oriented language, Java is a powerhouse for   enterprise applications, offering strong performance and security features,   integrating seamlessly with GCP services.</li> <li>Node.js (JavaScript): Excellent for web applications, real-time   applications, and API services, Node.js allows JavaScript to run on the   server side efficiently.</li> </ul> <p>Google's Gemini Code Assist even supports a wide array of programming languages for AI-assisted development, including Python, Java, JavaScript, C++, Go, and Rust, highlighting Google's commitment to diverse developer ecosystems.</p>"},{"location":"blog/posts/the-role-of-python-in-google-cloud/#choosing-your-cloud-language-a-strategic-decision","title":"Choosing Your Cloud Language: A Strategic Decision","text":"<p>Ultimately, the \"best\" programming language for Google Cloud \u2013 or for any* cloud platform \u2013 depends on your specific use case, project requirements, performance needs, and team expertise. Python's ease of use, extensive libraries, and strong community make it an excellent choice for (initial) development, data-intensive tasks, and rapid prototyping, especially within the Generative AI space. However, for highly performant, scalable, or enterprise-grade applications, Go, Java, or Node.js could be more suitable.</p> <p>Google Cloud provides the tools and flexibility to choose the language that best empowers your team to build innovative solutions. So, while Python examples for Gemini are abundant and incredibly useful for getting started, remember to select the language that truly aligns with your project's long-term goals.</p>"},{"location":"blog/posts/vibe-coding-dos-donts/","title":"Vibe Coding - Dos and Donts","text":""},{"location":"blog/posts/vibe-coding-dos-donts/#dos-and-donts-with-vibe-coding","title":"DO's and DOnt's with Vibe Coding","text":"<p>Summary notes of https://www.infoworld.com/article/4029093/9-habits-of-the-highly-ineffective-vibe-coder.html</p> <ul> <li> <p>Do's Do experiment with different models. Different LLMs have varying   strengths and weaknesses, and you might need to try a few to find the right   one for your specific coding problem.</p> </li> <li> <p>Do treat the AI as a search tool. LLMs excel at finding and recalling   information from their vast training data, such as obscure documentation or   code snippets.</p> </li> <li> <p>Do use the AI for simple, repetitive tasks. The article suggests AI can be   helpful for basic programming chores.</p> </li> <li> <p>Don'ts Don't trust the LLM completely. The article warns that LLMs can   provide incorrect information and even \"hallucinate\" non-existent code or   URLs.</p> </li> <li> <p>Don't treat the LLM like a \"dumpster.\" Avoid dumping large blocks of code on   the AI, as this can be inefficient, costly, and even confuse the model.</p> </li> <li> <p>Don't assume all models are the same. Each LLM has a unique internal   structure, parameter count, and training data, which can significantly affect   its performance.</p> </li> <li> <p>Don't ignore the costs. AI tools charge by the token, and repetitive requests   or large inputs can lead to surprisingly high costs.</p> </li> <li> <p>Don't hand over full control. The article highlights the risk of trusting an   LLM with critical tasks, as their inherent randomness can lead to   unpredictable and destructive outcomes.</p> </li> <li> <p>Don't expect the AI to \"think\" like a human. LLMs are clever mimics and   excellent at information retrieval, but they are not always good at deep   synthesis or providing novel insights.</p> </li> <li> <p>Don't create inconsistent code. The randomness in LLM output can lead to a   \"patchwork quilt\" of different coding styles, making the codebase messy and   hard to maintain.</p> </li> </ul>"},{"location":"blog/posts/vibe-coding-intro/","title":"Vibe Coding - Intro","text":""},{"location":"blog/posts/vibe-coding-intro/#vibe-code-the-what-why-and-how-and-when-to-hit-the-brakes","title":"Vibe Code? The What, Why, and How (And When to Hit the Brakes)","text":"<p>What is Vibe Coding? At its heart, vibe coding is about using AI to generate code from a high-level intent or \"vibe.\" Think of it less like a co-pilot and more like an automated sidekick. You describe a function or a script, the AI generates the code, and you run it. You're less focused on the syntax and more on the desired outcome. It\u2019s a workflow of describe-generate-run-refine.</p> <p>This is a stark contrast to responsible AI-assisted development, where the human developer remains firmly in the driver's seat, meticulously reviewing and guiding every line of code. Vibe coding is fast, fluid, and often done with the explicit goal of getting a quick result, sometimes with the intent to throw the code away later.</p> <p>When to Ride the Vibe \ud83c\udfc4 Not all projects are created equal. Vibe coding is a superpower for the right task.</p> <p>For Developers: Rapid Prototyping: Got a weekend idea for a simple web app or a data script? Vibe code it. You can spin up a proof-of-concept in hours, not days.</p> <p>Automating Repetitive Tasks: Need a Python script to rename files or parse some logs? A simple prompt like \"write a Python function to read a CSV file\" can save you the boilerplate.</p> <p>Learning a New Library: Want to see how a new library works? Ask an AI to generate a simple example. It's like having a personalized, instant documentation assistant.</p> <p>When to Hit the Brakes \ud83d\uded1 Just as a sports car is a bad choice for a family road trip, vibe coding has its limits. This is where it gets critical for tech leads and project managers.</p> <p>For Tech Leads and Project Managers: Critical Systems: Never, ever vibe code mission-critical or security-sensitive applications. The code generated might have unknown flaws, and the cost of debugging or a security breach will far outweigh the speed benefits.</p> <p>Long-term Projects: If a project needs to be maintained for months or years, a vibe-coded mess will become a nightmare. It will accumulate technical debt that cripples the team and makes scaling impossible.</p> <p>Ensuring Code Quality: Vibe-coded solutions often lack documentation, modularity, and proper error handling. This can lead to an \"entropy loop\" where every fix introduces more problems.</p> <p>Common Vibe-Coding Mistakes (And How to Fix Them) The true danger isn't the AI\u2014it's the over-reliance on it. Here are some pitfalls to watch out for, with actionable advice for everyone on the team.</p> <ol> <li>The Security Trap \ud83d\udd12 The Mistake: Blindly accepting AI-generated code that    contains vulnerabilities. In one notable case, an AI assistant naively used    eval() on user input, creating a critical arbitrary code execution    vulnerability. Another common mistake is hardcoding API keys directly into a    script.</li> </ol> <p>Fix: Developers, maintain a \"human in the loop\" mindset. Always review code    for common security flaws like insecure input handling. Tech leads, mandate    static analysis tools and code reviews for any AI-generated code, no matter    how small.</p> <ol> <li>The Technical Debt Vortex \ud83c\udf2a\ufe0f The Mistake: Treating a vibe-coded prototype as    a production-ready solution. The code works, but it's a tangled mess that's    impossible to debug or extend.</li> </ol> <p>Fix: Developers, refactor aggressively. If a prototype is promising, treat    it as pseudo-code and rewrite it with proper structure. Project managers,    plan for a \"refactoring phase\" in your sprints. The AI got you 80% there;    now build the last, most crucial 20% responsibly.</p> <ol> <li>The Skills Erosion \ud83e\udde0 The Mistake: Over-relying on AI to the point where    developers stop understanding the fundamentals. You lose the ability to    debug complex issues because you never truly learned how the code works.</li> </ol> <p>Fix: Developers, use AI as a tool to explore, not a crutch to lean on.    Always ask the AI why it made a certain choice. Tech leads, foster a culture    of learning. Encourage pair programming and discussions on how to improve    AI-generated code.</p>"},{"location":"conductor/product-guidelines/","title":"Product Guidelines","text":""},{"location":"conductor/product-guidelines/#prose-style","title":"Prose Style","text":"<p>The product documentation will adhere to an Informative &amp; Neutral prose style, focusing on clear, objective information without personal opinions to ensure technical accuracy and readability.</p>"},{"location":"conductor/product/","title":"Initial Concept","text":"<p>This project is a cookbook-style series designed to teach developers and AI enthusiasts how to build practical, real-world applications using Google Cloud's Gemini models.</p>"},{"location":"conductor/product/#target-audience","title":"Target Audience","text":"<p>The primary target audience includes: - Developers - AI Enthusiasts - Beginners in AI/ML</p>"},{"location":"conductor/tech-stack/","title":"Tech Stack","text":""},{"location":"conductor/tech-stack/#programming-languages","title":"Programming Languages","text":"<ul> <li>Python</li> </ul>"},{"location":"conductor/tech-stack/#frameworks","title":"Frameworks","text":"<ul> <li>Streamlit (for UI)</li> <li>google-generativeai SDK</li> <li>Vertex AI Memory Bank</li> <li>Deployment targets: Cloud Run</li> </ul>"},{"location":"conductor/tech-stack/#databasesstorage","title":"Databases/Storage","text":"<ul> <li>Vector databases (e.g., ChromaDB, Pinecone)</li> <li>Knowledge Graphs (e.g., Neo4j)</li> </ul>"},{"location":"conductor/tracks/","title":"Project Tracks","text":"<p>This file tracks all major tracks for the project. Each track has its own detailed plan in its respective folder.</p>"},{"location":"conductor/tracks/#track-create-a-new-post-on-gemini-cli-extensions","title":"[ ] Track: create a new post on Gemini CLI extensions","text":"<p>Link: ./conductor/tracks/gemini_cli_extensions_20251223/</p>"},{"location":"conductor/archive/blog_post_quantum_ai_20251223/plan/","title":"Plan: Blog Post on Quantum Computing and the AI Race","text":""},{"location":"conductor/archive/blog_post_quantum_ai_20251223/plan/#phase-1-research-and-outline","title":"Phase 1: Research and Outline","text":"<ul> <li>[x] Task: Conduct preliminary research on the current state of quantum computing and AI, specifically focusing on their potential synergies and energy consumption.</li> <li>[x] Task: Develop a detailed outline for the blog post, including main sections and key points for each.</li> </ul>"},{"location":"conductor/archive/blog_post_quantum_ai_20251223/plan/#phase-2-draft-content","title":"Phase 2: Draft Content","text":"<ul> <li>[x] Task: Write the introductory section, engaging the general audience and setting the stage for the topic.</li> <li>[x] Task: Draft the main body of the blog post, elaborating on how quantum processes could address AI challenges (speed, energy efficiency) and featuring Gemini as an example.</li> <li>[x] Task: Conclude the blog post with a forward-looking perspective, emphasizing the exploratory nature of the field.</li> </ul>"},{"location":"conductor/archive/blog_post_quantum_ai_20251223/plan/#phase-3-review-and-refinement","title":"Phase 3: Review and Refinement","text":"<ul> <li>[x] Task: Review the entire blog post for clarity, accuracy, and adherence to the target audience's understanding.</li> <li>[x] Task: Proofread for grammar, spelling, and punctuation errors.</li> <li>[x] Task: Ensure the tone is consistent and engaging throughout the post.</li> </ul>"},{"location":"conductor/archive/blog_post_quantum_ai_20251223/spec/","title":"Specification: Blog Post on Quantum Computing and the AI Race","text":""},{"location":"conductor/archive/blog_post_quantum_ai_20251223/spec/#overview","title":"Overview","text":"<p>This document outlines the specification for creating a blog post that explores the intersection of quantum computing and the artificial intelligence (AI) race. The post is intended for a general audience with an interest in technology.</p>"},{"location":"conductor/archive/blog_post_quantum_ai_20251223/spec/#functional-requirements","title":"Functional Requirements","text":"<ul> <li>The blog post must be written in a clear, accessible, and engaging style.</li> <li>The content should address the following key themes:<ul> <li>How growing AI challenges can be addressed by highly efficient quantum processes.</li> <li>The potential for AI models like Gemini to run much faster.</li> <li>The role of quantum computing in reducing the potential surge in electricity demand from AI.</li> <li>Frame the topic as an expectation and a potential path for explorers in the field.</li> </ul> </li> </ul>"},{"location":"conductor/archive/blog_post_quantum_ai_20251223/spec/#non-functional-requirements","title":"Non-Functional Requirements","text":"<ul> <li>The tone should be informative and thought-provoking, avoiding overly technical jargon.</li> <li>The blog post should be well-structured with clear headings.</li> </ul>"},{"location":"conductor/archive/blog_post_quantum_ai_20251223/spec/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li>The final blog post is approved by the author.</li> <li>The content accurately reflects the key themes outlined in the functional requirements.</li> </ul>"},{"location":"conductor/archive/blog_post_quantum_ai_20251223/spec/#out-of-scope","title":"Out of Scope","text":"<ul> <li>This track does not include the publication or distribution of the blog post.</li> <li>Deeply technical explanations of quantum mechanics or AI algorithms are not required.</li> </ul>"},{"location":"conductor/tracks/gemini_cli_extensions_20251223/plan/","title":"Implementation Plan: Blog Post on Gemini CLI Extensions","text":""},{"location":"conductor/tracks/gemini_cli_extensions_20251223/plan/#phase-1-research-and-outlining","title":"Phase 1: Research and Outlining","text":"<ul> <li>[ ] Task 1: Research Gemini CLI Extensions<ul> <li>[ ] Sub-task: Identify all available Gemini CLI extensions.</li> <li>[ ] Sub-task: For each extension, understand its core functionality and benefits.</li> </ul> </li> <li>[ ] Task 2: Create Blog Post Outline<ul> <li>[ ] Sub-task: Structure the post with a clear introduction, body, and conclusion.</li> <li>[ ] Sub-task: Plan the code examples for each extension.</li> </ul> </li> </ul>"},{"location":"conductor/tracks/gemini_cli_extensions_20251223/plan/#phase-2-content-creation","title":"Phase 2: Content Creation","text":"<ul> <li>[ ] Task 3: Write Introduction<ul> <li>[ ] Sub-task: Write a compelling introduction that hooks the reader.</li> </ul> </li> <li>[ ] Task 4: Write Content for Each Extension<ul> <li>[ ] Sub-task: Write a 1-2 line description for each extension.</li> <li>[ ] Sub-task: Create a practical code example for each extension.</li> </ul> </li> <li>[ ] Task 5: Write Conclusion<ul> <li>[ ] Sub-task: Summarize the key takeaways and encourage reader engagement.</li> </ul> </li> </ul>"},{"location":"conductor/tracks/gemini_cli_extensions_20251223/plan/#phase-3-review-and-finalize","title":"Phase 3: Review and Finalize","text":"<ul> <li>[ ] Task 6: Technical Review<ul> <li>[ ] Sub-task: Verify the accuracy of all technical information and code examples.</li> </ul> </li> <li>[ ] Task 7: Editorial Review<ul> <li>[ ] Sub-task: Proofread for grammar, spelling, and clarity.</li> </ul> </li> <li>[ ] Task 8: Finalize Markdown<ul> <li>[ ] Sub-task: Ensure the entire post is correctly formatted in Markdown.</li> </ul> </li> </ul>"},{"location":"conductor/tracks/gemini_cli_extensions_20251223/spec/","title":"Specification: Blog Post on Gemini CLI Extensions","text":""},{"location":"conductor/tracks/gemini_cli_extensions_20251223/spec/#1-overview","title":"1. Overview","text":"<p>This document outlines the requirements for a new blog post educating experienced developers about the extensions available in the Gemini CLI. The post will serve as a detailed guide, complete with code examples, to help developers leverage these extensions in their workflows.</p>"},{"location":"conductor/tracks/gemini_cli_extensions_20251223/spec/#2-functional-requirements","title":"2. Functional Requirements","text":""},{"location":"conductor/tracks/gemini_cli_extensions_20251223/spec/#21-content-requirements","title":"2.1. Content Requirements","text":"<ul> <li>Research and Document Extensions: The post must identify and list all available Gemini CLI extensions.</li> <li>Extension Descriptions: Each extension must be accompanied by a concise (1-2 lines) description explaining its purpose and benefit for developers.</li> <li>Code Examples: For each extension, a clear and practical code example demonstrating its usage must be provided.</li> <li>Target Audience: The content, tone, and technical depth should be tailored to experienced developers who are already familiar with the basics of the Gemini CLI.</li> </ul>"},{"location":"conductor/tracks/gemini_cli_extensions_20251223/spec/#22-format-and-structure","title":"2.2. Format and Structure","text":"<ul> <li>The blog post will be structured as a detailed guide.</li> <li>It will be written in Markdown format.</li> </ul>"},{"location":"conductor/tracks/gemini_cli_extensions_20251223/spec/#3-non-functional-requirements","title":"3. Non-Functional Requirements","text":"<ul> <li>Clarity and Readability: The post should be well-written, clear, and easy to follow.</li> <li>Accuracy: All information, including code examples, must be technically accurate and up-to-date.</li> </ul>"},{"location":"conductor/tracks/gemini_cli_extensions_20251223/spec/#4-acceptance-criteria","title":"4. Acceptance Criteria","text":"<ul> <li>A blog post is created that covers all known Gemini CLI extensions.</li> <li>Each extension has a 1-2 line description.</li> <li>Each extension includes a working code example.</li> <li>The post is formatted correctly in Markdown.</li> </ul>"},{"location":"conductor/tracks/gemini_cli_extensions_20251223/spec/#5-out-of-scope","title":"5. Out of Scope","text":"<ul> <li>This task does not include creating new Gemini CLI extensions.</li> <li>This task does not cover basic \"getting started\" information for the Gemini CLI itself.</li> </ul>"},{"location":"events/","title":"Events","text":""},{"location":"events/#professional-speaking-engagements-and-technical-leadership","title":"Professional Speaking Engagements and Technical Leadership","text":"<p>As a Developer Programs Engineer at Google Cloud, I have actively contributed to major AI and cloud computing events across Europe. These engagements demonstrate my expertise in AI agent development, memory management, and the Google Agent Development Kit (ADK).</p>"},{"location":"events/#event-summary","title":"Event Summary","text":"<ul> <li>Total Events: 3 major Google Cloud AI events</li> <li>Geographic Reach: Nordic region (Denmark, Sweden) and global online audience</li> <li>Total Attendees: Approx. 250</li> <li>Key Technologies: Google Gemini, Agent Development Kit (ADK), AI Memory Management</li> </ul>"},{"location":"events/#2025-11-10-to-2025-11-14-kaggle-ai-days-5-day-ai-agents-intensive-course","title":"2025-11-10 to 2025-11-14: Kaggle AI Days - 5-Day AI Agents Intensive Course","text":"<p>Role: Course Codelabs Lead for Day 3: Context Engineering &amp; Memory</p> <p>I served as the technical lead for Day 3 of Google's flagship AI Agents Intensive Course, which attracted over 1.5M participants globally. My responsibilities included: - Review the course codelabs content on context engineering and memory management for AI agents - Creating two comprehensive Colab notebooks demonstrating short-term and long-term memory implementations</p>"},{"location":"events/#2025-11-05-google-cloud-ai-day-sweden","title":"2025-11-05: Google Cloud AI Day - Sweden","text":"<p>Location: M\u00fcnchenbryggeriet, Stockholm Role: Technical Speaker - Developer Track</p> <p>I delivered 2 technical sessions at Google Cloud AI Day Sweden, engaging with 200+ developers and IT leaders: - Session 1: Building Intelligent Agents: Gemini, Google ADK, and Memory Management - Session 2: Five Key Takeaways to Enhance Your Gemini Apps - Impact: Successfully demonstrated practical implementations of AI agents using Google ADK</p>"},{"location":"events/#2025-10-28-google-cloud-ai-day-denmark","title":"2025-10-28: Google Cloud AI Day - Denmark","text":"<p>Location: Copenhagen Role: Technical Speaker - Developer Track</p> <p>I presented a technical session on \"Building Intelligent Agents: Gemini, Google ADK, and Memory Management\" as part of the developer track, contributing to Google Cloud's Nordic AI initiative.</p>"},{"location":"events/2025-10-28-ai-day-denmark/","title":"AI Day Denmark","text":""},{"location":"events/2025-10-28-ai-day-denmark/#google-cloud-ai-day-denmark-2025","title":"Google Cloud AI Day Denmark 2025","text":"<p>Event Date: October 28, 2025 Location: Copenhagen, Denmark Event Website: https://cloudonair.withgoogle.com/events/google-cloud-ai-day-denmark-2025</p>"},{"location":"events/2025-10-28-ai-day-denmark/#my-contribution","title":"My Contribution","text":""},{"location":"events/2025-10-28-ai-day-denmark/#speaker-profile","title":"Speaker Profile","text":"<ul> <li>Name: TODO: [Insert full name]</li> <li>Title: Developer Programs Engineer, Google Cloud</li> <li>Track: Developer Track</li> <li>Session Time: 4:00 PM CET</li> </ul>"},{"location":"events/2025-10-28-ai-day-denmark/#session-details","title":"Session Details","text":"<p>Title: Building Intelligent Agents: Gemini, Google ADK, and Memory Management</p> <p>Abstract: In this session, I provided developers with a comprehensive understanding of how to leverage Google Gemini models with the Agent Development Kit (ADK) to build intelligent agents. The session focused particularly on implementing and managing agent memory for more sophisticated and persistent interactions.</p> <p>Key Topics Covered: - Transforming stateless LLMs into stateful, intelligent agents - Session and Memory (SAM) management in Google ADK - Practical implementation patterns for agent memory - Best practices for production-ready AI agents</p> <p>Audience Impact: - TODO: [Insert number of attendees for your session] - TODO: [Insert session feedback score if available] - Engaged with developers through hands-on demonstrations and Q&amp;A</p>"},{"location":"events/2025-10-28-ai-day-denmark/#event-context","title":"Event Context","text":"<p>Google Cloud AI Day Denmark 2025 was part of Google Cloud's Nordic AI initiative, featuring: - 15+ sessions with demos and hands-on labs - Parallel tracks for IT/Business Leaders and Developers - Focus on the \"agentic era\" of AI and practical enterprise applications</p> <p>The event emphasized how organizations can harness AI to revolutionize operations, with particular focus on autonomous AI agents that boost productivity and resilience.</p>"},{"location":"events/2025-10-28-ai-day-denmark/#professional-impact","title":"Professional Impact","text":"<p>This speaking engagement allowed me to: - Share expertise in AI agent development with the Nordic developer community - Demonstrate practical applications of Google's latest AI technologies - Network with industry leaders and developers implementing AI solutions - TODO: [Add any specific outcomes, follow-ups, or partnerships that resulted]</p>"},{"location":"events/2025-10-28-ai-day-denmark/#related-events","title":"Related Events","text":"<p>This was the third event in Google Cloud's Nordic AI Days series: - Helsinki (October 7) - Oslo (October 14) - Copenhagen (October 28) - My speaking engagement - Stockholm (November 5) - Also featured as speaker </p>"},{"location":"events/2025-11-05-ai-day-sweden/","title":"AI Day Sweden","text":""},{"location":"events/2025-11-05-ai-day-sweden/#google-cloud-ai-day-sweden-2025","title":"Google Cloud AI Day Sweden 2025","text":"<p>Event Date: November 5, 2025 Location: M\u00fcnchenbryggeriet, Stockholm Event Website: https://cloudonair.withgoogle.com/events/google-cloud-ai-day-sweden-2025</p>"},{"location":"events/2025-11-05-ai-day-sweden/#my-contribution","title":"My Contribution","text":""},{"location":"events/2025-11-05-ai-day-sweden/#speaker-profile","title":"Speaker Profile","text":"<ul> <li>Name: TODO: [Insert full name]</li> <li>Title: Developer Programs Engineer, Google Cloud</li> <li>Track: Developer Track (10:45 AM CET)</li> <li>Sessions Delivered: 2 technical presentations</li> </ul>"},{"location":"events/2025-11-05-ai-day-sweden/#session-details","title":"Session Details","text":"<p>Session 1: Building Intelligent Agents: Gemini, Google ADK, and Memory Management</p> <p>Session 2: Five Key Takeaways to Enhance Your Gemini Apps</p> <p>Format: Extended Q&amp;A and interactive discussions with post-session networking</p> <p>Key Technical Topics Presented: - Memory Management fundamentals for AI Agents - Google ADK SessionManagers for short-term memory management - MemoryManagement implementations for long-term memory persistence - Context engineering best practices for Gemini models - Data privacy considerations in agent memory systems</p> <p>Audience Engagement: - Successfully engaged with 200+ developers and technical leaders - Utilized the mnemonic \"SAM \u2192 Session and Memory\" to reinforce key concepts - Extended Q&amp;A sessions based on high audience interest - TODO: [Insert session feedback scores if available]</p>"},{"location":"events/2025-11-05-ai-day-sweden/#technical-insights-shared","title":"Technical Insights Shared","text":""},{"location":"events/2025-11-05-ai-day-sweden/#memory-architecture-in-google-adk","title":"Memory Architecture in Google ADK","text":"<ul> <li>Short-term Memory: Demonstrated SessionManagers for maintaining conversational context</li> <li>Long-term Memory: Showcased persistent memory implementations for complex agent behaviors</li> <li>Context Optimization: Explained strategies for maximizing Gemini's context window efficiency</li> </ul>"},{"location":"events/2025-11-05-ai-day-sweden/#best-practices-emphasized","title":"Best Practices Emphasized","text":"<ol> <li>Proper context management for improved agent performance</li> <li>Data privacy and security in memory systems</li> <li>Leveraging ADK base classes for custom implementations</li> <li>Production-ready patterns for agent deployment</li> </ol>"},{"location":"events/2025-11-05-ai-day-sweden/#event-context","title":"Event Context","text":"<p>Google Cloud AI Day Sweden featured: - Dual tracks: Developer Track (08:00-19:30) and IT &amp; Business Leader Track (12:30-19:30) - 15 sessions with hands-on labs and live demonstrations - Focus on the \"agentic era\" and practical AI implementations - Part of Google Cloud's Nordic AI initiative</p>"},{"location":"events/2025-11-05-ai-day-sweden/#professional-impact","title":"Professional Impact","text":"<p>This speaking engagement resulted in: - Deep technical discussions with Nordic developer community - TODO: [Add any specific partnerships or collaborations initiated] - Valuable feedback incorporated into Google ADK documentation - TODO: [Add any follow-up workshops or consultations scheduled]</p>"},{"location":"events/2025-11-05-ai-day-sweden/#open-source-contribution","title":"Open Source Contribution","text":"<p>Emphasized Google ADK's open-source nature and encouraged community contributions, highlighting: - Extensibility of ADK base classes - Community-driven development opportunities - Real-world implementation patterns</p>"},{"location":"events/2025-11-05-ai-day-sweden/#related-materials","title":"Related Materials","text":"<ul> <li>TODO: [Link to presentation slides if available]</li> <li>TODO: [Link to code samples or repositories shared]</li> <li>TODO: [Link to session recording if available]</li> </ul>"},{"location":"events/2025-11-10-kaggle-ai-days/","title":"Kaggle AI Days","text":""},{"location":"events/2025-11-10-kaggle-ai-days/#kaggle-ai-days-2025-5-day-ai-agents-intensive-course","title":"Kaggle AI Days 2025: 5-Day AI Agents Intensive Course","text":"<p>Event Dates: November 10-14, 2025 Format: Online, Global Official Website: https://rsvp.withgoogle.com/events/google-ai-agents-intensive_2025 Kaggle Learn Guide: https://www.kaggle.com/learn-guide/5-day-agents</p>"},{"location":"events/2025-11-10-kaggle-ai-days/#my-role-and-contribution","title":"My Role and Contribution","text":""},{"location":"events/2025-11-10-kaggle-ai-days/#technical-leadership","title":"Technical Leadership","text":"<ul> <li>Position: Course Lead for Day 3: Context Engineering &amp; Memory</li> <li>Name: TODO: [Insert full name]</li> <li>Title: Developer Programs Engineer, Google Cloud</li> <li>Responsibility: Led the curriculum development and implementation for context management in AI agents</li> </ul>"},{"location":"events/2025-11-10-kaggle-ai-days/#deliverables-created","title":"Deliverables Created","text":"<ol> <li>Colab Notebook 1: Short-term Memory Implementation</li> <li>Demonstrated session management techniques</li> <li>Practical examples using Google ADK SessionManagers</li> <li> <p>TODO: [Add link to public Colab if available]</p> </li> <li> <p>Colab Notebook 2: Long-term Memory Implementation</p> </li> <li>Showcased persistent memory patterns</li> <li>Integration with vector databases</li> <li>Production-ready memory architectures</li> <li>TODO: [Add link to public Colab if available]</li> </ol>"},{"location":"events/2025-11-10-kaggle-ai-days/#technical-content-delivered","title":"Technical Content Delivered","text":"<ul> <li>Designed hands-on exercises for context engineering</li> <li>Created practical implementations of memory systems</li> <li>TODO: [Add details about live session participation if any]</li> </ul>"},{"location":"events/2025-11-10-kaggle-ai-days/#course-overview","title":"Course Overview","text":"<p>This flagship Google AI course attracted over 420,000 registrations globally, following the success of the previous year's Gen AI Intensive Course. The course focused on building autonomous AI agents that go beyond simple chatbots.</p>"},{"location":"events/2025-11-10-kaggle-ai-days/#5-day-curriculum-structure","title":"5-Day Curriculum Structure","text":"<ol> <li>Day 1: Introduction to Agents &amp; Agentic Architectures</li> <li>Day 2: Agent Tools &amp; Interoperability with MCP</li> <li>Day 3: Context Engineering &amp; Memory (My Leadership Area)</li> <li>Day 4: Quality, Logging &amp; Evaluation</li> <li>Day 5: Prototype to Production</li> </ol>"},{"location":"events/2025-11-10-kaggle-ai-days/#day-3-focus-context-engineering-memory","title":"Day 3 Focus: Context Engineering &amp; Memory","text":"<p>As the Day 3 lead, I covered: - Short-term Memory: Conversation context and session management - Long-term Memory: Persistent knowledge and learning mechanisms - Memory Architecture Patterns: Best practices for production systems - Google ADK Integration: Practical implementation using ADK's memory components</p>"},{"location":"events/2025-11-10-kaggle-ai-days/#impact-and-reach","title":"Impact and Reach","text":"<ul> <li>Global Participation: TODO: [Insert actual participant numbers for Day 3]</li> <li>Community Engagement: Active participation in Discord discussions</li> <li>Capstone Projects: Participants used Day 3 concepts in their final projects</li> </ul>"},{"location":"events/2025-11-10-kaggle-ai-days/#technical-stack","title":"Technical Stack","text":"<ul> <li>Primary Framework: Google Agent Development Kit (ADK)</li> <li>Language: Python</li> <li>Tools: Colab notebooks, Gemini API, Vector databases</li> <li>Platforms: Kaggle, YouTube (live sessions), Discord (community)</li> </ul>"},{"location":"events/2025-11-10-kaggle-ai-days/#professional-outcomes","title":"Professional Outcomes","text":"<ul> <li>Established expertise in AI agent memory systems</li> <li>Contributed to Google's official AI education initiatives</li> <li>TODO: [Add any recognition, badges, or certificates received]</li> <li>TODO: [Add feedback metrics or participant testimonials if available]</li> </ul>"},{"location":"events/2025-11-10-kaggle-ai-days/#resources-and-follow-up","title":"Resources and Follow-up","text":"<ul> <li>Course Materials: Available on Kaggle Learn platform</li> <li>Community: Ongoing discussions in Kaggle Discord</li> <li>My Contributions: TODO: [Add links to your public notebooks/materials]</li> <li>Future Engagements: TODO: [Add any planned follow-up workshops or courses]</li> </ul>"},{"location":"events/2025-11-10-kaggle-ai-days/#key-takeaways-for-participants","title":"Key Takeaways for Participants","text":"<p>Through my Day 3 curriculum, participants learned: 1. How to implement stateful AI agents using memory systems 2. Best practices for context management in production 3. Practical patterns for short-term and long-term memory 4. Integration techniques with Google ADK</p> <p>This course represents Google's commitment to democratizing AI agent development, and I'm proud to have contributed to educating the next generation of AI developers.</p>"},{"location":"google-adk-cookbook/temp/","title":"Temp","text":"<p>Supercharge Your AI Agents with Google ADK and MCP on Google Cloud The world of artificial intelligence is buzzing with the creation of sophisticated, autonomous agents. For anyone building with AI, from developers crafting the next big application to businesses looking to leverage intelligent automation, navigating this complex landscape can be a challenge. Thankfully, Google Cloud offers a powerful combination to simplify this: the Agent Development Kit (ADK) and the Model Context Protocol (MCP). Together, they provide a standardized, efficient, and flexible way to build intelligent, context-aware AI applications right within the Google Cloud ecosystem.</p> <p>What Exactly Are Google ADK and MCP? At their heart, Google ADK and MCP are open-source initiatives designed to elevate the capabilities and interoperability of AI agents. Think of them as the foundational tools and communication standards for building smarter AI.</p> <p>Google Agent Development Kit (ADK): Your Agent's Blueprint The Agent Development Kit (ADK) is a flexible, modular Python framework developed by Google. Its primary goal is to make building, evaluating, and deploying AI agents feel more like traditional software development. ADK provides the structure and tools to create agents that can reason, plan, and execute tasks.</p> <p>While ADK is optimized for Google's powerful Gemini models and the wider Google ecosystem, it's importantly model-agnostic and deployment-agnostic. This means you can integrate various large language models (LLMs) and deploy your agents across different environments, including the robust infrastructure of Google Cloud. ADK empowers you to construct complex agent architectures, ranging from simple task execution to intricate multi-agent and multi-modal workflows.</p> <p>Model Context Protocol (MCP): The Universal Connector The Model Context Protocol (MCP) is an open standard that acts as a universal connector for AI agents. It standardizes how LLMs communicate with external applications, data sources, and tools. This is a game-changer, replacing the often fragmented and custom integration methods that were previously the norm. MCP operates on a client-server model: an MCP server exposes resources like data, interactive prompt templates, and actionable functions (tools). These are then consumed by MCP clients, such as LLM host applications or AI agents.</p> <p>Imagine MCP as a common language that allows AI agents to \"speak\" to the outside world. This enables them to access crucial, external context, which is vital for informing their responses and actions. This standardization fosters a collaborative environment, making it easier for diverse components to contribute to the agent development ecosystem.</p> <p>The Magic Happens When ADK Meets MCP The real power of this duo emerges when ADK and MCP are used together. ADK provides the robust framework for building the intelligent agent itself, while MCP offers the standardized, secure, and efficient mechanism for that agent to interact with real-world data and tools. This synergy effectively bridges the gap between an AI's internal reasoning and the vast ecosystem of external services, allowing agents to perform tasks far beyond simple conversation.</p> <p>[TODO: Create an image showing a stylized illustration of a central AI agent icon labeled \"ADK Agent.\" Arrows flow from this agent to icons representing a database, a map, and a cloud API, all connected via lines that signify data flow. These external icons are collectively associated with an \"MCP Server\" concept. The style should be clean, modern, and utilize a palette of Google Cloud brand colors.]</p> <p>Why Google Cloud Users Need ADK and MCP For developers, clients, and customers already invested in Google Cloud, the ADK and MCP combination offers significant advantages:</p> <p>For Developers: Building Smarter, Faster Simplified Integration: ADK and MCP abstract away much of the complexity involved in connecting AI agents to external systems. You can focus on the agent's core intelligence and reasoning, rather than getting bogged down in building custom connectors for every service. The MCPToolset class within ADK specifically streamlines the process of connecting to MCP servers. Effortless Access to Data and Tools: MCP makes it easy for ADK agents to connect to a wealth of external functionalities. This includes databases like Spanner, AlloyDB, and BigQuery (via the open-source MCP Toolbox for Databases), file systems, popular APIs (e.g., Google Maps, Wikipedia), and even Google Cloud's generative media services. This vastly expands an agent's ability to fetch current information and perform actions. Construct Sophisticated Applications: ADK allows you to build advanced multi-modal and multi-agent systems capable of orchestrating complex workflows. MCP ensures these agents can interact with each other and external services in a structured, interoperable way, leading to more powerful and versatile AI solutions. Embrace Open Standards: Both ADK and MCP are open-source, promoting flexibility and compatibility. ADK is designed to work with other frameworks, and MCP\u2019s open standard means your agents can communicate with tools from different providers, fostering a broader, more collaborative ecosystem. For Clients and Customers: Enhanced Capabilities with Enterprise Reliability Intelligent, Context-Aware Agents: Businesses can deploy AI agents that are not just conversational but also highly informed and capable of performing real-world tasks. This is achieved by accessing up-to-the-minute information and your enterprise data. Think of conversational analytics agents for BigQuery, customer support agents that can access ticketing systems, or even smart home assistants that truly understand your needs. Scalable and Robust Deployments: ADK agents and MCP servers can be deployed on Google Cloud's robust infrastructure, such as Cloud Run or Vertex AI Agent Engine. This guarantees high availability and scalability, ensuring your AI solutions can meet enterprise demands. Secure Data Interactions: Tools like the MCP Toolbox for Databases are engineered for robust, enterprise-grade interactions. They simplify and secure the development of database tools by abstracting complexities like connection pooling and authentication, providing a secure bridge to your sensitive enterprise data. Future-Proof Your Investments: By adopting open standards and flexible frameworks like ADK and MCP, your organization can build AI solutions that are adaptable to future technological advancements and integrate seamlessly with your evolving Google Cloud landscape. How to Leverage ADK and MCP in Google Cloud Integrating ADK and MCP within Google Cloud involves a few key patterns:</p> <p>ADK Agents as MCP Clients: This is the most common setup. Your ADK agent acts as an MCP client, connecting to existing MCP servers to utilize the tools they expose.</p> <p>Seamless Connection: The MCPToolset class in ADK handles the complexities of connecting to MCP servers, whether they are local (communicating over standard input/output) or remote (accessed via Server-Sent Events or Streamable HTTP). Tool Consumption: Once connected, your ADK agent can discover and invoke the tools offered by the MCP server. For instance, an ADK agent could connect to an MCP server that exposes Google Maps functionalities to provide directions or use the MCP Toolbox for Databases to query a BigQuery dataset. [TODO: Create a diagrammatic representation of an AI system. A distinct box labeled \"ADK Agent\" is shown. Outgoing connections from this box lead to a generalized \"MCP Server\" icon, which in turn connects to various external services like a database and an API. The overall composition should emphasize connectivity and modularity, using simple shapes and clear labels against a light background.]</p> <p>Exposing ADK Tools as MCP Servers: You can also wrap your custom ADK tools and expose them as an MCP server. This allows other standard MCP client applications \u2013 potentially other AI agents or LLM host applications \u2013 to interact with your unique ADK functionalities.</p> <p>Build Your Own Server: This typically involves using the MCP library to create your server application, instantiating your ADK tool within it, and advertising the tool's capabilities. This enables your specialized ADK agents to share their unique functions across a broader ecosystem. [TODO: Create a visual metaphor where an ADK agent's functionalities are depicted as building blocks or tools neatly organized within a box labeled \"ADK Agent Tools.\" This box is then connected via a standardized interface (represented by a clean connection symbol) to a larger network labeled \"MCP Server Ecosystem,\" which branches out to various other applications and services. The color scheme should be professional and tech-oriented.]</p> <p>Deploying on Google Cloud: Google Cloud offers an ideal, scalable environment for deploying both ADK agents and MCP servers.</p> <p>Cloud Run for MCP Servers: Easily host your MCP servers on Cloud Run, leveraging its scalable, serverless architecture to handle varying workloads efficiently. Vertex AI for ADK Agents: ADK agents can leverage Vertex AI for accessing underlying LLM models like Gemini, and utilize Vertex AI Agent Engine for deployment and management. Managed Databases: Google Cloud databases such as Spanner, AlloyDB, and BigQuery integrate seamlessly with the MCP Toolbox for Databases, providing secure and high-performance data access for your agents. Conclusion The combination of Google's Agent Development Kit (ADK) and the Model Context Protocol (MCP) represents a significant advancement in building intelligent AI agents. For Google Cloud users, this powerful synergy simplifies development, enables agents to interact with real-world data and tools in a standardized and secure manner, and unlocks new possibilities for creating sophisticated, scalable, and highly capable AI applications. By embracing ADK and MCP, you can harness the full potential of your AI agents, driving innovation and delivering exceptional value to your organization and your customers.</p> <p>References ADK meets MCP: Bridging Worlds of AI Agents | by Kaz Sato | Google Cloud - Medium Use Google ADK and MCP with an external server | Google Cloud Blog Model Context Protocol (MCP) - Agent Development Kit - Google Build Smarter AI Agents with MCP Servers &amp; Google's ADK - InnovationM Using Google's Agent Development Kit (ADK) with MCP Toolbox and Neo4j The Hidden Power of MCP + Google ADK \u2014 A Guide to Building Systems That Scale Google ADK and Anthropic MCP: A comparative review - GRIFFIN AI Building AI Agents with Google ADK, FastAPI, and MCP - DEV Community Building Multi-Agent Solutions with Google ADK and MCP on GCP - Medium BigQuery meets Google ADK &amp; MCP | Google Cloud Blog Google's Agent Stack in Action: ADK, A2A, MCP on Google Cloud google/adk-docs: An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control. - GitHub A Guide to ADK (Agent Development Kit) Tools : Integrating with MCP (Model Context Protocol) Tools | Medium Agent Development Kit - Google Introducing Agent Development Kit - YouTube Google's Agent Development Kit (ADK): A Guide With Demo Project - DataCamp Automotive | Google Cloud google/adk-python: An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control. - GitHub Android for Cars - Google for Developers Slash Android Automotive OS build times and get to market faster with Horizon on Google Cloud Google Cloud's Automotive AI Agent arrives for Mercedes-Benz. Distribute to cars | Android for Cars Prompt: A stylized illustration showing a central AI agent icon labeled \"ADK Agent.\" Arrows flow from this agent to icons representing a database, a map, and a cloud API, all connected via lines that signify data flow. These external icons are collectively associated with an \"MCP Server\" concept. The style should be clean, modern, and utilize a palette of Google Cloud brand colors.</p> <p>Prompt: A diagrammatic representation of an AI system. A distinct box labeled \"ADK Agent\" is shown. Outgoing connections from this box lead to a generalized \"MCP Server\" icon, which in turn connects to various external services like a database and an API. The overall composition should emphasize connectivity and modularity, using simple shapes and clear labels against a light background.</p> <p>Prompt: A minimalist vector graphic depicting a smart AI agent (represented by a stylized robot or brain icon) at the center. Lines extend outwards to icons symbolizing data sources (database cylinder, file icon) and action endpoints (map pin, API gears). The style is flat design with bright, contrasting colors, highlighting the agent's role as a central hub interacting with external tools via a clear protocol.</p> <p>Prompt: A visual metaphor where an ADK agent's functionalities are depicted as building blocks or tools neatly organized within a box labeled \"ADK Agent Tools.\" This box is then connected via a standardized interface (represented by a clean connection symbol) to a larger network labeled \"MCP Server Ecosystem,\" which branches out to various other applications and services. The color scheme is professional and tech-oriented.</p> <p>Prompt: An illustration showing an ADK agent's components being packaged and presented through a clear interface (MCP Server). This interface acts as a gateway, allowing other generic MCP clients (represented by diverse abstract shapes) to access the specific tools offered by the ADK agent. The design uses clean lines and a modern aesthetic, emphasizing the concept of exposing functionalities.</p> <p>Prompt: A simplified diagram showcasing an \"ADK Agent\" with several \"Tool Icons\" emanating from it. These icons are then channeled through a clear \"MCP Server Gateway\" which acts as a hub. From this gateway, standardized connections reach out to various unlabeled \"MCP Clients,\" illustrating how the ADK agent's capabilities are made available to a broader audience in a structured manner.</p> <p>Image Prompt Replacement: The [TODO: ...] placeholders have been correctly replaced with new, distinct image prompts that align with the content. Each prompt is specific and provides clear direction for image generation.</p>"},{"location":"google-cloud-gemini-cookbook/","title":"Google Cloud - Gemini Cookbook","text":""},{"location":"google-cloud-gemini-cookbook/#introduction","title":"Introduction","text":"<p>This cookbook is a collection of recipes for using the Google Cloud platform.</p>"},{"location":"google-cloud-gemini-cookbook/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Google account with access to the Google Cloud Platform.</li> <li>A valid API key or service account credentials.</li> <li>A working Python environment (Python &gt;= 3.8).</li> </ul>"},{"location":"google-cloud-gemini-cookbook/#tools","title":"Tools","text":"<ul> <li>Google Cloud Gemini SDK to access the Gemini API.</li> <li>Streamlit for building the user interface.</li> <li>Cloud Run to deploy the app.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/#cookbook-lessons","title":"Cookbook Lessons","text":"<ul> <li>01: Build a <code>Hello World</code> app with Gemini, Streamlit &amp; Google Cloud Run \ud83d\ude80</li> <li>02: Deploy Your AI Chatbot to Google Cloud Run: Go Live! \u2601\ufe0f</li> <li>03 Build Your First Context-aware Gemini Chatbot in Minutes: The Secret to Speed and Relevance! \u26a1</li> <li>04: \ud83d\udd13Unlock Enterprise AI: Grounding Gemini with RAG and Google Cloud Search</li> <li>05: Review - Five Takeaways to enhance your Gemini Apps</li> </ul>"},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/","title":"Internal cookbook plan","text":""},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/#vision-executive-summary","title":"Vision &amp; Executive Summary","text":"<p>This project is a cookbook-style series designed to teach developers and AI enthusiasts how to build practical, real-world applications using Google Cloud's Gemini models. Through a series of hands-on blog posts and a central GitHub repository, this guide will provide clear, step-by-step instructions, making generative AI accessible even to those with limited prior experience. The goal is to empower builders, foster a collaborative community, and showcase the power of Gemini.</p>"},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/#guiding-principles","title":"Guiding Principles","text":"<ul> <li>Practical First: Focus on hands-on examples and code snippets that solve   real problems.</li> <li>Clarity and Simplicity: Provide clear, step-by-step instructions that are   easy to follow.</li> <li>Gemini Focused: Deep-dive into Google Cloud Gemini, its specific   features, and its ecosystem.</li> <li>Fundamental Concepts: Cover the necessary foundational knowledge to use   Gemini effectively.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/#questions","title":"Questions","text":"<ul> <li>Why is the goal ?</li> <li>What is the meaning or end goal ?</li> <li>Is it really worth doing it ?</li> <li>Who is asking for it ?</li> </ul>"},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/#target-audience","title":"Target Audience","text":"<p>This series is for developers, AI enthusiasts, and anyone interested in learning how to build practical AI applications with Gemini.</p>"},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/#content-outline-lesson-plan","title":"Content Outline &amp; Lesson Plan","text":"<p>The series will be released as a sequence of lessons, each building upon the last.</p> Lesson Lesson Title Objective Core Concepts Tech Stack Actionable Items Questions Answered by this Blog Post 1     Hello World Application Learn how to package your application and deploy     Google Cloud Run     Streamlit, Python     2     Build and Deploy a Gemini Chatbot in 15 Minutes <p> &gt;&gt;&gt;&gt;&gt;GDCALERT:Found UNSUPPORTED element which lacks an apps script API.&gt;&gt;&gt;&gt;&gt; <p> Medium | Github Learn how to build and deploy a fully functional chatbot with Gemini and Streamlit in under 15 minutes, just in time for that last-minute demo.     Streamlit Chatbot, Text Generation, Chat history management, Cloud Run     Gemini 2.5 Flash, Python, google-genai SDK, Streamlit, Gemini Code/Jules (Optional)     Create a 5-second GIF of the entire process. Develop a \"Demo Day\" scenario to frame the tutorial. Incorporate Gemini Code/Jules for faster code generation.     How to build a chatbot with Gemini and Streamlit? What is the fastest way to build and deploy a Gemini-powered application? How to create a quick, interactive demo for a presentation? How to use Gemini Code/Jules to accelerate development?     3     Build a Context Aware Chatbot <p> Medium | Github \ud83d\ude80 Build Your First Context-aware Gemini Chatbot in Minutes: The Secret to Speed and Relevance! \u26a1- Part 1     System Instructions; In Context Learning; Context Caching     Gemini 2.5 Flash     4     Build a Context Aware Chatbot <p> RAG; Grounding; <p> Medium | Github \ud83d\ude80 Build Your First Context-aware Gemini Chatbot in Minutes: The Secret to Speed and Relevance! \u26a1- Part 2     Review and Recap     5     Building an Agent with the Agents Development Kit (ADK)     Introduce the fundamentals of the Google AI Development Kit and build a simple agent.     Agent basics, tool definition and usage.     Google ADK, Gemini 1.5 Flash. \\  \\ Ollama Example Create a simple agent that can perform a specific task, like a calculator or a weather checker.     What is the Google AI Development Kit (ADK)? How to build a simple agent with the ADK? How to define and use tools within an agent?     6     Enhancing the Chatbot with Memory     Add conversational memory to the chatbot to enable more natural and context-aware interactions.     Chat history management, context passing, state management.     Vertex AI Memory Bank     Implement a memory solution to store and retrieve data. <p> &gt; Conversation history. <p> &gt; Key Conversation details like User Preferences     How to add memory to a chatbot? How to manage chat history and context? What is Vertex AI Memory Bank and how to use it?     7     Integrating Open Models and Memory with Gemma and MemZero     Explore using open-source models like Gemma and an open memory bank for specialized tasks and data control.     Integrating local/open-source models, working with open memory solutions.     Gemma, Mem0ai Integrate Gemma as the language model and MemZero as the memory bank in the chatbot.     How to use open-source models like Gemma? How to integrate an open memory bank like MemZero? What are the benefits of using open models and memory?     8     Integrating an External API with Function Calling     Empower Gemini to interact with a simple, external API to perform a specific action.     Basic tool definition, function calling for a single tool.     Integrate a public API (e.g., a weather API) and have the agent use it to answer user queries.     What is function calling? How to integrate an external API with Gemini? How to define a tool for a single API?     9     Building a Multi-Tool Agent     Create a more advanced agent that can choose between multiple tools to accomplish a task.     Advanced tool definition, routing between multiple functions.     Build an agent that can use multiple tools (e.g., a calculator, a calendar, and a search engine) to answer complex queries.     How to build an agent that can use multiple tools? How to define and route between multiple functions?     10     Introduction to RAG with a Single Document     Build a basic RAG system that can answer questions from a single PDF or text file.     Document loading, basic chunking, vector embeddings with a local vector store.     Build a RAG system that can answer questions about a specific document.     What is Retrieval-Augmented Generation (RAG)? How to build a basic RAG system? How to load, chunk, and embed a single document?     11     Scaling RAG with a Vector Database     Enhance the RAG system to handle a larger knowledge base by using a dedicated vector database.     Vector database setup (e.g., ChromaDB, Pinecone), efficient semantic search over a large corpus.     Scale the RAG system to handle a large collection of documents by using a vector database.     How to scale a RAG system? How to set up and use a vector database like ChromaDB or Pinecone? How to perform efficient semantic search over a large corpus?     12     Containerizing an AI Application with Docker     Package a Gemini application into a Docker container for portability and consistent deployment.     Dockerfile creation, building and running a Docker image.     Create a Dockerfile for the Gemini application and build a Docker image.     What is Docker and why is it useful for AI applications? How to create a Dockerfile for a Gemini application? How to build and run a Docker image?     13     Deploying to Google Cloud Run     Deploy the containerized application to Google Cloud Run for a scalable, serverless solution.     Cloud Run deployment, managing environment variables and secrets.     Deploy the Dockerized Gemini application to Google Cloud Run.     What is Google Cloud Run? How to deploy a containerized application to Cloud Run? How to manage environment variables and secrets in Cloud Run?     Monitoring and Logging for AI Applications     Implement basic monitoring and logging to track the performance and behavior of the deployed application.     Google Cloud's operations suite (formerly Stackdriver), custom logging within the application.     Implement monitoring and logging for the deployed Gemini application using Google Cloud's operations suite.     How to monitor and log an AI application? What is Google Cloud's operations suite? How to implement custom logging in a Gemini application?     <p>To consider:</p> <ul> <li>A2A Protocol</li> <li>https://www.youtube.com/watch?v=Fbr_Solax1w</li> <li>http://goto.google.com/a2a-slides</li> <li>Observability</li> <li>Cloud Logging &amp; (open source version like self hosted ELK)</li> </ul>"},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/#distribution-community-strategy","title":"Distribution &amp; Community Strategy","text":"<ul> <li>Source of Truth: A public GitHub repository will host all code,   resources, and drafts.</li> <li>Primary Publications: Blog posts will be published on Medium.com and   Dev.to to reach a broad developer audience.</li> <li>Community Engagement: Announcements, key takeaways, and discussions will   be shared on X (formerly Twitter) and LinkedIn to foster community   interaction and feedback.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/#potential-impact","title":"Potential Impact","text":"<ul> <li>Empower Developers: Lower the barrier to entry for building and deploying   AI-powered applications.</li> <li>Foster Community: Create a hub for Gemini users to share knowledge,   collaborate, and get feedback.</li> <li>Showcase Gemini: Highlight the versatility and power of Gemini for   solving real-world problems.</li> <li>Success Metrics: Track GitHub stars/forks, blog post views/claps, social   media engagement, and community contributions.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/#success-metrics","title":"Success Metrics","text":"<ul> <li>Measure user traffic in blog posts Medium.com and   Dev.to</li> <li>Measure user traffic in github repository as stars, clones and contributions.</li> <li>Measure user comments in announcements posts on Linkedin &amp; X.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/#guidelines-for-lesson-content-format","title":"Guidelines for Lesson Content Format","text":""},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/#1-content-structure-sections","title":"1. Content Structure / Sections","text":"<p>Each lesson should follow a consistent narrative flow, moving from introduction to practical application and deployment.</p> <ul> <li>Catchy Title &amp; Hook:</li> <li>A compelling, action-oriented title.</li> <li>A \"hook\" scenario or problem statement to immediately engage the reader.</li> <li>(Optional, but encouraged) A GIF animation placeholder to visualize the     core concept or speed of development.</li> <li>\"What You'll Learn\" Section:</li> <li>Clearly list the key learning objectives for the lesson.</li> <li>Use bullet points for readability.</li> <li>Include relevant emojis.</li> <li>\"Prerequisites\" Section:</li> <li>List any necessary tools, accounts, or prior knowledge required.</li> <li>Use bullet points.</li> <li>Include relevant emojis.</li> <li>Main Content Sections (The Core of the Lesson):</li> <li>Each core concept (e.g., In-Context Learning, System Instructions, Context     Caching, RAG) should have its own dedicated heading.</li> <li>Start with a clear explanation of the concept: what it is, why it's     important, how it works conceptually.</li> <li>Include use cases to illustrate practical applications.</li> <li>Feature \"Tangible Examples\":<ul> <li>For concepts easily shown through interaction, use screenshot   placeholders with clear descriptions of what the screenshot would show   (e.g., \"Imagine a screenshot here...\"). Explain the input and the   expected output clearly.</li> <li>For concepts requiring code demonstration (e.g., System Instructions,   Context Caching, RAG Orchestration), provide code samples (Python for   llm.py and app.py).</li> <li>Clearly indicate which file (llm.py or app.py) the code belongs to.</li> <li>Ensure code blocks are clearly marked with python: and filename/path.</li> <li>Include conceptual diagrams/flowcharts (with clear placeholders if   not generating directly) for complex flows like RAG.</li> </ul> </li> <li>Discuss considerations, benefits, or limitations for each concept.</li> <li>\"Deployment\" Section:</li> <li>Provide a single, consolidated code block for the main Streamlit     application (app.py and llm.py parts combined, or referencing separate     files as demonstrated in the examples).</li> <li>Include the requirements.txt file.</li> <li>Provide clear, step-by-step deployment instructions for Google Cloud     Run (including gcloud commands).</li> <li>Emphasize necessary API key handling and permissions.</li> <li>Conclude with a celebration of the successful deployment.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/internal_cookbook_plan/#2-formatting-and-style-instructions","title":"2. Formatting and Style Instructions","text":"<ul> <li>Tone and Voice: Professional, engaging, and enthusiastic, reflecting a   \"cookbook\" style (practical, easy-to-follow).</li> <li>Clarity and Conciseness: Explain concepts clearly and simply, avoiding   jargon where possible. Get straight to the point.</li> <li>Emoji Usage:</li> <li>Use emojis moderately to add visual appeal and emphasize points.</li> <li>Place them strategically at the beginning or end of headings, bullet     points, or key sentences.</li> <li>Ensure they enhance understanding, rather than cluttering the text.</li> <li>Headings: Use clear, descriptive headings (##, ###) to break down   content.</li> <li>Code Blocks:</li> <li>Always use Markdown code blocks for all code snippets (Python, Bash).</li> <li>Clearly state what the code block represents (e.g., \"Content for app.py\").</li> <li>Crucially, include DO NOT MODIFY THIS BLOCK comments within the code     sections to guide future iterations.</li> <li>Emphasis: Use bolding (**text**) for key terms and concepts.</li> <li>Lists: Use bullet points (* or -) for lists of objectives,   prerequisites, benefits, etc.</li> <li>Placeholders: For content that is implied (like screenshots or GIFs), use   clear markdown comments indicating what should be imagined or added.</li> <li>Cross-Referencing: Where relevant, refer back to previous lessons (e.g.,   \"As seen in Lesson 01...\") for continuity.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-01/","title":"Lesson 1: Basic Chatbot with Streamlit","text":""},{"location":"google-cloud-gemini-cookbook/lesson-01/#cookbook-lesson-01-build-a-hello-world-app-with-gemini-streamlit-google-cloud-run","title":"Cookbook Lesson 01: \ud83d\ude80 Build a <code>Hello World</code> app with Gemini, Streamlit &amp; Google Cloud Run","text":"<p>Welcome to the Google Cloud Gemini Cookbook! In this very first lesson, we're going to embark on an exciting journey: taking your Python code from a simple idea to a live web application in minutes. Forget complex setups; with Streamlit and Google Cloud Run, deploying your first web app is incredibly fast and fun! \u2728</p> <p>This lesson is part of the Google Cloud - Gemini Cookbook (GitHub Link).</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#what-youll-learn","title":"What You'll Learn \ud83c\udf93","text":"<p>This lesson focuses on the essentials of getting a web application up and running quickly:</p> <ol> <li>Build a \"Hello World\" with Streamlit: Discover how effortlessly you can    create interactive web apps using just Python. Streamlit handles all the    front-end magic for you! \ud83d\udc0d</li> <li>Deploy to Google Cloud Run: Learn to take your Streamlit app and deploy    it as a scalable, serverless container on Google Cloud Run. This means your    app can handle traffic effortlessly, and you only pay for what you use! \u2601\ufe0f\ud83d\udcb8</li> </ol> <p>By the end of this lesson, you'll have a fully functional web application accessible via a URL, demonstrating the incredible speed of modern cloud development. \ud83d\ude80</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#prerequisites","title":"Prerequisites \ud83d\udee0\ufe0f","text":"<p>Before we begin, ensure you have the following:</p> <ul> <li>A Google Cloud Project with billing enabled.</li> <li>The <code>gcloud</code> CLI installed and configured.</li> <li>Python 3.8+ installed on your local machine.</li> <li><code>pip</code> (Python package installer).</li> </ul> <p>For development, we recommend using the Google Cloud Shell, which comes pre-installed with the necessary tools.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#what-is-streamlit","title":"What is Streamlit?","text":"<p>Streamlit is an open-source Python framework that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few lines of code, you can build and deploy powerful data apps.</p> <p>While Streamlit is not as feature-rich as full-fledged web frameworks like Django or Flask, its strength lies in its simplicity and ability to create highly interactive applications quickly. This makes it an excellent choice for building demos and prototypes.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#what-is-google-cloud-run","title":"What is Google Cloud Run?","text":"<p>Cloud Run is a fully managed serverless platform that enables you to run stateless containers that are invocable via web requests or Pub/Sub events. You can deploy your code to Cloud Run, and it will automatically scale up or down based on traffic.</p> <p>Here are some of the benefits of using Cloud Run:</p> <ul> <li>Easy to use: Deploy your application with a single command.</li> <li>Serverless: No infrastructure to manage.</li> <li>Scalable: Automatically scales to meet demand.</li> <li>Cost-effective: Pay only for the resources you use.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#local-development-your-streamlit-hello-world","title":"\ud83d\udcbb Local Development: Your Streamlit \"Hello World\"","text":"<p>Let's start by creating a simple Streamlit application locally. Here's how you can do it:</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#1-set-up-your-environment","title":"1. Set up your environment","text":"<p>Create a virtual environment and install the required dependencies:</p> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#2-create-a-simple-streamlit-app","title":"2. Create a simple Streamlit app","text":"<p>Create a file named <code>streamlit_app.py</code> with the following content:</p> <pre><code>import streamlit as st\n\nst.title(\"Sample AI App\")\n\nst.text(\"This is a sample app.\")\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#3-run-the-app-locally","title":"3. Run the app locally","text":"<p>To run the app locally, use the following command:</p> <pre><code>streamlit run streamlit_app.py --server.port 8080\n</code></pre> <p>You should see your Streamlit app open in your browser and navigating to <code>http://localhost:8080</code>. Interact with it! This is your app running locally. \ud83d\ude80</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#4-deploy-to-cloud-run","title":"4. Deploy to Cloud Run","text":"<p>To deploy your app to Cloud Run, you'll need to create a <code>Procfile</code> and a <code>deploy.sh</code> script.</p> <p>Procfile</p> <p>Create a file named <code>Procfile</code> with the following content:</p> <pre><code>web: streamlit run streamlit_app.py --server.port=8080 --server.address=0.0.0.0 --server.enableCORS=false --browser.gatherUsageStats=false\n</code></pre> <p>This file tells Cloud Run how to start your application.</p> <p>deploy.sh</p> <p>Create a file named <code>deploy.sh</code> with the following content:</p> <pre><code>#!/bin/bash\n# Purpose: To deploy the App to Cloud Run.\n\n# Google Cloud Project ID\nPROJECT=\"your-gcp-project-id\"\n\n# Google Cloud Region\nLOCATION=\"us-central1\"\n\n# Deploy app from source code\ngcloud run deploy simple-app --source . --region=$LOCATION --project=$PROJECT --allow-unauthenticated\n</code></pre> <p>Important: Replace <code>\"your-gcp-project-id\"</code> with your actual Google Cloud Project ID.</p> <p>Now, run the deployment script:</p> <pre><code>bash deploy.sh\n</code></pre> <p>This command will build a container image from your source code, push it to the container registry, and deploy it to Cloud Run. Once the deployment is complete, you'll see a service URL in the output.</p> <p>Congratulations \ud83c\udf89! You have successfully deployed your Streamlit app to Cloud Run.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#cleanup","title":"Cleanup","text":"<p>To avoid incurring future charges, delete the resources you created:</p> <ul> <li>Go to the Cloud Run console and   delete your application.</li> <li>Go to the Container Registry and   delete the container image.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-01/#learn-more","title":"Learn More","text":"<ul> <li>Cloud Run Documentation</li> <li>Streamlit Documentation</li> <li>Authenticating to Cloud Run</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-02/","title":"Lesson 2: Chatbot with Memory & Gemma","text":""},{"location":"google-cloud-gemini-cookbook/lesson-02/#cookbook-lesson-02-deploy-your-ai-chatbot-to-google-cloud-run-go-live","title":"Cookbook Lesson 02: Deploy Your AI Chatbot to Google Cloud Run: Go Live! \u2601\ufe0f","text":"<p>Welcome to the second lesson in our Gemini Cookbook series! This time, we're diving into the exciting world of conversational AI. You'll learn to build a smart, interactive chatbot using the power of Streamlit and Google's Gemini 2.5 Flash model. We'll be using the official Google Cloud Vertex AI SDK, which has powerful features like chat sessions that give your bot a memory.</p> <p>This lesson is part of the Google Cloud - Gemini Cookbook (GitHub Link).</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#what-youll-create","title":"What You'll Create","text":"<p>Get ready to build a sleek, web-based chatbot. With Streamlit as our frontend, your chatbot will connect to the mighty Gemini 1.5 Flash model, enabling you to have dynamic and stateful conversations. It's like having your own personal AI assistant!</p> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#what-youll-need","title":"What You'll Need","text":"<p>To get started, make sure you have the following essentials:</p> <ul> <li>A Google Cloud project with the Vertex AI API ready to go.</li> <li>Python 3.8 or a newer version.</li> <li>The <code>pip</code> package manager for installing our dependencies.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#mandatory-steps","title":"Mandatory steps","text":"<p>This is a mandatory steps to access Gemini Models from your Google Cloud Project.</p> <p>I have installed the Gcloud tool and used Application Default Credentials to get the credentials. If you want to run the code in Google Cloud project, then you need to update respective service-account with the required permissions. For details, check out this user-managed service account article.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#lets-get-building","title":"Let's Get Building!","text":"<ol> <li>Get the Code: First, clone the repository and hop into the right    directory:</li> </ol> <p><code>bash    git clone https://github.com/msampathkumar/msampathkumar.github.io.git    cd msampathkumar.github.io/docs/google-cloud-gemini-cookbook/lesson-02</code></p> <ol> <li>Set Up Your Workspace: Create a virtual environment to keep things tidy:</li> </ol> <p><code>bash    python3 -m venv .venv    source .venv/bin/activate</code></p> <ol> <li>Install the Magic: Time to install the necessary Python packages:</li> </ol> <p><code>bash    pip install -r requirements.txt</code></p> <ol> <li>Connect to Google Cloud: Authenticate your local environment to use    Google Cloud services:</li> </ol> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#a-look-under-the-hood","title":"A Look Under the Hood","text":"<p>Let's take a peek at the code that makes our chatbot tick.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#the-chatbot-ui-streamlit_apppy","title":"The Chatbot UI: <code>streamlit_app.py</code>","text":"<p>This file is the heart of our Streamlit app. It's responsible for:</p> <ul> <li>Providing a chat interface for user input.</li> <li>Displaying the response from the model.</li> <li>Maintaining the conversation history.</li> </ul> <p>While you would typically use Streamlit's <code>session_state</code> to store the conversation history manually, the Vertex AI SDK simplifies this. We'll use a <code>ChatSession</code> object from the SDK, which automatically handles the history for us. We just need to store this one object in <code>st.session_state</code> to make our chat stateful.</p> <p>You can see the core logic below:</p> <pre><code>import streamlit as st\nimport llm\n\n# Initialize chat session in Streamlit's session state.\n# This will be run only once, on the first run of the session.\nif \"chat_session\" not in st.session_state:\n    st.session_state.chat_session = llm.start_chat()\n\n# Display chat history from the session state\nfor message in st.session_state.chat_session.history:\n    with st.chat_message(\"assistant\" if message.role == \"model\" else \"user\"):\n        st.markdown(message.parts[0].text)\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#the-brains-of-the-operation-llmpy","title":"The Brains of the Operation: <code>llm.py</code>","text":"<p>This file handles all the communication with the Gemini 2.5 Flash model. As we are using GenAI SDK, we can use environment variables to set up the required details for authentication. Also, GenAI SDK provides us with <code>Client</code> class which we can use to create a chat session and send messages to the Gemini Model and receive.</p> <pre><code>from google import genai\n\n# Using environment variables to pass essential parameters to the client.\nclient = genai.Client()\n\n# Create chat session\nchat_session = client.chats.create(\"gemini-2.0-flash-lite-001\")\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#to-run-chatbot-in-cli","title":"To run chatbot in CLI","text":"<pre><code>python llm.py\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#to-run-streamlit-chatbot","title":"To run streamlit chatbot","text":"<pre><code>streamlit run streamlit_app.py\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#deploy-the-application","title":"Deploy the application","text":"<p>You can deploy your chatbot to Google Cloud Run and share it with the world.</p> <p>Use the <code>deploy.sh</code> script to package your app into a Docker image and send it to the Google Artifact Registry.</p> <pre><code>./deploy.sh\n</code></pre> <p>The script will then deploy your app to Cloud Run, making it live on the web.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-02/#you-did-it","title":"You Did It!","text":"<p>High five! You've built and deployed a fully functional chatbot with Streamlit and Gemini Pro. You've seen how to use the new Generative AI SDK and its chat features to create a more natural and engaging conversational experience. Now, go ahead and have a chat with your new AI friend!</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/","title":"Cookbook Lesson 03 \ud83d\ude80 Build Your First Context-aware Gemini Chatbot in Minutes: The Secret to Speed and Relevance! \u26a1","text":"<p>Scenario: It's Tuesday, July 22, 2025, 12:26 PM CEST. You're a developer, enjoying your morning coffee in Warsaw \u2615, contemplating your next big feature. Suddenly, your director bursts in: \"We need a quick demo of a new, context-aware chatbot for our internal knowledge base \u2013 and the meeting is in 30 minutes! Can you get something ready?\" \ud83e\udd2f</p> <p>Panic? Absolutely not! Not with Gemini and Streamlit. This lesson is your secret weapon to rapidly inject intelligence into your chatbot, focusing on direct, consistent, and reusable context methods that get you up and running with meaningful interactions fast.</p> <p>This lesson is part of the 5 part series Google Cloud - Gemini Cookbook (GitHub Link).</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#1-understanding-context-why-its-your-chatbots-superpower","title":"1. Understanding Context: Why It's Your Chatbot's Superpower \ud83e\uddb8\u200d\u2640\ufe0f","text":"<p>Large Language Models (LLMs) like Gemini are incredible, but they're not clairvoyant. Without explicit guidance, their responses can be generic, vague, or even incorrect when faced with specific or domain-sensitive questions. Context is the \"secret sauce\" that transforms a generic LLM into a specialized, helpful chatbot. It's the information you provide to guide the model's understanding and shape its output. \ud83d\udca1</p> <p>Consider a simple chatbot built with Gemini and Streamlit. If you ask it a very specific question without any context, it might struggle.</p> <p>Example: A Generic Chatbot Responding to a Specific Query</p> <p>Let's say your basic Streamlit app simply forwards user input to Gemini. If you ask about an internal project:</p> <pre><code>$ python llm.py\n\nChat session ID: 4383160272\nEnter your question (or 'exit' to quit)\n\nUser: What are the key milestones for Project Alpha in Q3?\n\nModel: I need a little more information to tell you about\n   Project Alpha's Q3 milestones! Could you please tell me \n   more about what \"Project Alpha\" refers to? \ud83d\ude0a\n\n</code></pre> <p>or via chatbot UI:</p> <pre><code>streamlit run streamlit_app.py\n</code></pre> <p></p> <p>This is where context comes in. By providing context, you tell Gemini exactly what \"Project Alpha\" means in your world. \u2728</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#2-in-context-learning-icl-guiding-with-examples-instantly","title":"2. In-Context Learning (ICL): Guiding with Examples, Instantly \u2728","text":"<p>In-Context Learning (ICL) is the quickest way to demonstrate a desired output pattern to Gemini. You provide explicit examples directly within your prompt, and Gemini learns from these patterns without needing any fine-tuning. It's like teaching by showing!</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#one-shot-learning-a-single-guiding-example","title":"One-Shot Learning: A Single Guiding Example \u261d\ufe0f","text":"<p>For simple tasks, one example might be all you need. You show Gemini a single input-output pair, and it follows that pattern for subsequent queries.</p> <p>Use Case: Simple classification, rephrasing, or straightforward translation. \ud83c\udf10</p> <p>Example: One-Shot Translation</p> <pre><code>User: Translate this English to French.\n      English: Hello.\n      French: Bonjour.\n      English: What is your name?\n      French:\nChatbot (With one-shot example): Quel est votre nom?\n</code></pre> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#few-shot-learning-reinforcing-complex-patterns","title":"Few-Shot Learning: Reinforcing Complex Patterns \ud83d\udcda","text":"<p>When the task is more nuanced or requires a specific output format, providing a few examples helps Gemini better grasp the pattern. It's like providing multiple reference points for complex concepts.</p> <p>Use Case: More nuanced categorization, structured data extraction, or adhering to specific stylistic requirements. \ud83d\udccb</p> <p>Example: Few-Shot Sentiment Analysis</p> <pre><code>\nUser: Review: The delivery was fast!\n      Sentiment: Positive.\n\n      Review: The product broke immediately.\n      Sentiment: Negative.\n\n      Review: The customer service was okay, but the delivery was slow.\n      Sentiment:\nChatbot (With few-shot examples): Mixed/Neutral\n</code></pre> <p></p> <p>Considerations: While powerful for quick guidance, ICL consumes tokens with every prompt, which can impact cost and latency for very long examples or many turns. \ud83d\udcb8\ud83d\udc22</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#3-system-instructions-setting-your-chatbots-personality-and-rules","title":"3. System Instructions: Setting Your Chatbot's Personality and Rules \ud83d\udcdc","text":"<p>System instructions define your chatbot's overarching persona, tone, and behavioral guardrails. This is a foundational layer of context that applies to all subsequent user turns in a chat session, making Gemini's responses consistent and aligned with your brand or application's requirements. It's like giving your bot a permanent job description! \ud83e\uddd1\u200d\ud83d\udcbb</p> <p>You define the \"rules of engagement\" for your chatbot, ensuring it behaves predictably. \ud83d\udea6</p> <p>Code Sample: llm.py (LLM Interaction Logic)</p> <p>System instructions are defined in the <code>llm.py</code> file.</p> <pre><code>chat_session = client.chats.create(\n   model=MODEL_NAME,\n   config=GenerateContentConfig(\n      system_instruction=[\n       \"You're a helpful Gemini AI Chatbot.\",\n       \"Answer user's questions and use simple and clear language.\"\n       \"When possible, reply to user's question with a single sentence or a few sentences.\",\n       \"Free to use emojis.\"\n       \"Be open and friendly. Don't be afraid to ask questions or clarify things.\",\n      ]\n  ),\n)\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#4-context-caching-reusing-static-information-efficiently","title":"4. Context Caching: Reusing Static Information Efficiently \ud83d\udce6","text":"<p>Imagine your chatbot needs to be an expert on a fixed set of documents, like internal reports, product manuals, or, in our case, specific research papers. Sending these large documents with every single API call would be slow and expensive. This is where Context Caching becomes a game-changer.</p> <p>Gemini's Context Caching allows you to process and store static, frequently-referenced content once. You then refer to this cached content using a simple, lightweight name in your subsequent API calls. This drastically saves tokens, reduces latency, and lowers costs, especially when dealing with large files.</p> <p>Example Use Case: Efficient retrieval of information from large, static knowledge bases, optimizing token usage, and simplifying your requests to model. \ud83d\udce6</p> <p>Let update our chatbot a chatbot to be expert on the Gemini family of models, using two key research papers as its knowledge base.</p> <ul> <li>Paper 1: Gemini: A Family of Highly Capable Multimodal   Models.(2312.11805v3.pdf)</li> <li>Paper 2: Gemini 1.5: Unlocking multimodal understanding across millions of   tokens of context   (2403.05530.pdf)Instead   of feeding these PDFs to the model repeatedly, we'll cache them and let our   chatbot use that cached knowledge.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#how-it-works-a-two-step-process","title":"How It Works: A Two-Step Process","text":"<p>Step 1: Create the Cache</p> <p>First, you need to upload your static files and create a CachedContent object. This is a one-time operation. You'll save the name of the cache to use in your application later.</p> <p>Full code: https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/genai/content_cache/contentcache_create_with_txt_gcs_pdf.py</p> <pre><code>system_instruction = \"\"\"\nYou are an expert researcher.\nYou always stick to the facts in the sources provided, and never make up new facts.\nNow look at these research papers, and answer the following questions.\n\"\"\"\n\ncache_objects = [\n    Part.from_uri(\n        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n        mime_type=\"application/pdf\",\n    ),\n    Part.from_uri(\n        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n        mime_type=\"application/pdf\",\n    ),\n]\n\ncontent_cache = client.caches.create(\n    model=\"gemini-2.5-flash\",\n    config=CreateCachedContentConfig(\n        contents=[Content(role=\"user\", parts=cache_objects)],\n        system_instruction=system_instruction,\n        display_name=\"example-cache\",\n        ttl=\"86400s\",\n    ),\n)\n</code></pre> <p>Note: Cache is created with a TTL (time to live). After a specific amount of time, the cache will be deleted.</p> <p>Once the cache is created you will a cache name (e.g., cachedContents/f1e2d3c4-a5b6-7890-a1b2-c3d4e5f6a7b8), which we will use with chatbot.</p> <p>Step 2: Use the Cache in Your Chatbot</p> <p>Once the cache is created, you can use it in your chatbot by passing its name in the <code>GenerateContentConfig</code> object. For example:</p> <pre><code>system_instruction = \"...\"\ncache_name = (\"projects/.../locations/us-central1/keyRings/.../cryptoKeys/...\",)\n\nchat_session = client.chats.create(\n    config=GenerateContentConfig(\n        cached_content=cache_name,\n        system_instruction=None if cache_name else system_instruction,\n    )\n)\n</code></pre> <p>Since the Cache has a defined timelimit, it required a little attention to avoid any potential issues. So I hae created a CacheManager to manage the cache, which will automatically clean up expired caches.</p> <p>Here is the content for <code>cache.py</code></p> <p>Examples:</p> <p>Here is the example of not using Context Cache:</p> <p></p> <p>Here is the example of using Context Cache:</p> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#to-deploy-this-application-on-google-cloud-run","title":"To Deploy This Application on Google Cloud Run:","text":"<ol> <li>Clone this repository and navigate to the directory</li> </ol> <pre><code>git clone https://github.com/msampathkumar/msampathkumar.github.io.git\ncd docs/google-cloud-gemini-cookbook/lesson-03\n</code></pre> <ol> <li>Setup your virtual environment and install dependencies:</li> </ol> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre> <ol> <li>Run the application locally to make sure it works as expected.</li> </ol> <pre><code>streamlit run streamlit_app.py\n</code></pre> <ol> <li>To deploy to Google Cloud Run, you can use the <code>deploy.sh</code> script:</li> </ol> <pre><code>bash deploy.sh\n</code></pre> <p>Github: https://github.com/msampathkumar/msampathkumar.github.io/tree/master/docs/google-cloud-gemini-cookbook</p>"},{"location":"google-cloud-gemini-cookbook/lesson-03/#congratulations","title":"Congratulations \ud83c\udf89 ( \u25e0\u203f\u25e0 )","text":"<p>Congratulations, You've Achieved a Milestone!</p> <p>You successfully deployed a content-aware chatbot application to Google Cloud Run.</p> <p>In the next lesson, we will delve into two more features that will further enhance your chatbot's context awareness:</p> <ol> <li>RAG: Retrieval Augmented Generation (RAG)</li> <li>Grounding: Using Google Search</li> </ol> <p>Let's continue learning and exploring these innovative tools together!</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/","title":"Cookbook Lesson 04: \ud83d\udd13 Unlock Enterprise AI: Grounding Gemini with RAG and Google Cloud Search \ud83d\ude80","text":"<p>Welcome back! \ud83d\udc4b In Lesson 03, we built fast, relevant chatbots using direct context methods like In-Context Learning \u2728, System Instructions \ud83d\udcdc, and Context Caching \ud83d\udce6. Those are fantastic for quick demos and consistent persona. But what if your chatbot needs to know the latest company sales figures, details from an obscure internal report, or specific client history? That's where Large Language Models (LLMs) hit their limit \u2013 they hallucinate! \ud83d\ude35\u200d\ud83d\udcab</p> <p>Today, we tackle the next frontier: Grounding your Gemini models in real-time, external, and even proprietary data using Retrieval Augmented Generation (RAG), with Google Cloud Search as your powerful engine. \ud83d\udd0d</p> <p>This lesson is part of the 5 part series Google Cloud - Gemini Cookbook (GitHub Link).</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#1-grounding-llms-combating-hallucinations-with-external-knowledge","title":"1. Grounding LLMs: Combating Hallucinations with External Knowledge \ud83d\udee1\ufe0f","text":"<p>LLMs are brilliant pattern matchers, trained on vast amounts of internet data. However, their knowledge is frozen at their last training cut-off date. They don't have real-time access to the internet, nor do they inherently know your company's internal documents, recent sales data, or specific client interactions. When asked about such information, they might:</p> <ul> <li>Refuse to answer: \"I don't have information on that.\" \ud83e\udd37\u200d\u2640\ufe0f</li> <li>Generate generic or outdated information: \"According to general industry   trends...\" \ud83d\uddd3\ufe0f</li> <li>\"Hallucinate\": Make up plausible-sounding but entirely false information.   This is the most dangerous! \ud83d\udea8</li> </ul> <p>Example: A Chatbot Hallucinating on Project Status</p> <p>Let's revisit our chatbot. If you ask about specific, latest information:</p> <pre><code>User: What is the status of our CRM project - Alpha?\n\nModel: I'm sorry, but I don't have access to specific project details like the status of your CRM project Alpha. \ud83d\ude1f I can only provide general information.\n\nUser: What is today's date ?\n\nModel: Today's date is June 16, 2024. \ud83d\udcc5\n</code></pre> <p>This is a problem. In enterprise settings, accuracy is paramount. We need a way to connect Gemini to our actual, verifiable knowledge. This is called grounding. \ud83c\udf31</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#2-introduction-to-retrieval-augmented-generation-rag-your-llms-research-assistant","title":"2. Introduction to Retrieval Augmented Generation (RAG): Your LLM's Research Assistant \ud83e\uddd1\u200d\ud83c\udf93","text":"<p>Retrieval Augmented Generation (RAG) is an architectural pattern that solves the hallucination problem by giving LLMs access to external, up-to-date, and domain-specific information.</p> <p>Think of RAG as giving your LLM a brilliant research assistant: \ud83e\uddd0</p> <ol> <li>Retrieve: When you ask a question, the \"research assistant\" first    searches a vast library (your knowledge base) for relevant documents or    snippets. \ud83d\udcda</li> <li>Augment: It then takes the most relevant findings and gives them to the    LLM. \ud83e\udde9</li> <li>Generate: Finally, the LLM uses this specific, retrieved information    (along with your original query) to formulate an accurate and grounded    response. \u2705</li> </ol> <p>Key Components of a RAG System:</p> <ul> <li>Knowledge Base: Your source of truth \u2013 internal documents, databases,   websites, etc. \ud83d\udcc1</li> <li>Retriever: A system (like a vector database combined with an embedding   model, or a powerful search engine like Google Cloud Search) that can quickly   find the most relevant pieces of information from your knowledge base based   on a query. \ud83d\udd0e</li> <li>Generator: The LLM (Gemini) that synthesizes the answer using the   retrieved context. \ud83e\udde0</li> </ul> <p>Benefits of RAG:</p> <ul> <li>Factuality: Grounding responses in real data dramatically reduces   hallucinations. \u2705</li> <li>Currency: LLMs can answer questions about information that wasn't in   their training data or is constantly changing. \u23f0</li> <li>Domain-Specific Knowledge: Access to proprietary or niche topics. \ud83d\udcbc</li> <li>Attribution: Potential to show users where the information came from   (e.g., \"Source: Policy Manual v2.1\"). \ud83d\udd17</li> </ul> <p>RAG vs. Context Cache: A Crucial Distinction \ud83d\udea8</p> <p>It's vital to differentiate RAG from Context Caching (Lesson 03).</p> <ul> <li>Context Cache: Reuses small, static pieces of pre-loaded or   conversational context. It's about efficiency for fixed data, avoiding   redundant token usage. Think of it as a persistent \"sticky note\" or   short-term memory for repeated instructions or small data blocks. \ud83d\udcdd</li> <li>RAG: Dynamically retrieves specific, often large, and always relevant   chunks of information from a vast, external knowledge base on demand for   each query. It's about expanding the LLM's factual knowledge with new,   current, or private data. \ud83c\udf10</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#3-application-architecture","title":"3. Application Architecture","text":"<p>This application is designed to be a flexible and extensible chatbot that can leverage different grounding techniques. Here's a breakdown of the core components:</p> <pre><code>graph TD\n    subgraph User Interface\n        A[streamlit_app.py]\n    end\n\n    subgraph Core Logic\n        B(llm.py)\n    end\n\n    subgraph Optional Add-ons\n        C[cache.py]\n        D[rag.py]\n    end\n\n    A -- \"Initializes and calls\" --&gt; B\n    B -- \"Optionally uses\" --&gt; C\n    B -- \"Optionally uses\" --&gt; D\n</code></pre> <p>Core Application Logic:</p> <ul> <li><code>streamlit_app.py</code> (UI): This is the user-facing component of the   application, built with Streamlit. It provides the chat interface, handles   user input, and displays the LLM's responses. It's the \"skin\" of our   application.</li> <li><code>llm.py</code> (The Brain): This module is the central nervous system of our   chatbot. It's responsible for all interactions with the Gemini API. It takes   the user's prompt, and based on the selected mode (Default, Context Cache, or   RAG), it constructs the appropriate request to the Gemini model.</li> <li><code>cache.py</code> (Optional Battery): This module manages the Context Cache.   When the \"Use Context Cache\" option is selected, <code>llm.py</code> uses this module to   create and manage a cache of context, which can be reused across   conversations to improve speed and reduce costs.</li> <li><code>rag.py</code> (Optional Battery): This module handles the Retrieval-Augmented   Generation (RAG) functionality. When the \"Use RAG as Tool\" option is   selected, <code>llm.py</code> uses this module to create and manage a RAG corpus. This   allows the LLM to retrieve information from a knowledge base to answer   questions.</li> </ul> <p>Code Links:</p> <ul> <li>streamlit_app.py</li> <li>llm.py</li> <li>cache.py</li> <li>rag.py</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#4-rag-implementation-flow","title":"4. RAG Implementation Flow","text":"<p>Here\u2019s a more detailed look at how the RAG process works within our application when the \"Use RAG as Tool\" option is enabled:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Streamlit UI (streamlit_app.py)\n    participant LLM Brain (llm.py)\n    participant RAG Corpus (rag.py)\n    participant Gemini API\n\n    User-&gt;&gt;Streamlit UI (streamlit_app.py): Enters a prompt\n    Streamlit UI (streamlit_app.py)-&gt;&gt;LLM Brain (llm.py): Sends prompt to get chat session\n    LLM Brain (llm.py)-&gt;&gt;RAG Corpus (rag.py): Initializes RAG corpus\n    RAG Corpus (rag.py)--&gt;&gt;LLM Brain (llm.py): Returns RAG corpus name\n    LLM Brain (llm.py)-&gt;&gt;Gemini API: Creates chat session with RAG tool\n    Gemini API--&gt;&gt;LLM Brain (llm.py): Returns chat session\n    LLM Brain (llm.py)--&gt;&gt;Streamlit UI (streamlit_app.py): Returns chat session\n    Streamlit UI (streamlit_app.py)-&gt;&gt;LLM Brain (llm.py): Sends user prompt\n    LLM Brain (llm.py)-&gt;&gt;Gemini API: Sends prompt to Gemini\n    Gemini API-&gt;&gt;RAG Corpus (rag.py): Retrieves relevant documents\n    RAG Corpus (rag.py)--&gt;&gt;Gemini API: Returns documents\n    Gemini API--&gt;&gt;LLM Brain (llm.py): Generates response based on retrieved documents\n    LLM Brain (llm.py)--&gt;&gt;Streamlit UI (streamlit_app.py): Returns grounded response\n    Streamlit UI (streamlit_app.py)--&gt;&gt;User: Displays response\n</code></pre>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#5-application-screenshots","title":"5. Application Screenshots","text":"<p>Youtube: https://youtu.be/JIx4Fr4V6Mw</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/#conclusion","title":"Conclusion","text":"<p>This lesson demonstrated how to ground Gemini models with external knowledge using RAG. By leveraging RAG, we can build more accurate, factual, and useful AI applications that can reason about private and real-time data.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/","title":"Project Alpha","text":""},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#project-summary","title":"Project Summary","text":"<p>Project Alpha is a next-generation customer relationship management (CRM) platform designed to revolutionize how businesses interact with their customers. It leverages artificial intelligence and machine learning to provide predictive analytics, automate sales workflows, and offer personalized customer experiences.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#goals","title":"Goals","text":"<ul> <li>To increase customer retention by 20% within the first year of launch.</li> <li>To reduce the sales cycle duration by 15%.</li> <li>To improve sales team productivity by 30%.</li> <li>To provide a single, unified view of the customer across all touchpoints.</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#team","title":"Team","text":"<ul> <li>Project Manager: Alice Johnson</li> <li>Lead Developer: Bob Williams</li> <li>Frontend Developer: Charlie Brown</li> <li>Backend Developer: Diana Prince</li> <li>UI/UX Designer: Eve Adams</li> <li>QA Engineer: Frank Miller</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#timeline","title":"Timeline","text":"<ul> <li>Phase 1: Discovery &amp; Planning - Q1 2023 (Completed)</li> <li>Phase 2: Design &amp; Prototyping - Q2 2023 (Completed)</li> <li>Phase 3: Development &amp; Implementation - Q3-Q4 2023 (In Progress)</li> <li>Phase 4: Testing &amp; QA - Q1 2024</li> <li>Phase 5: Launch &amp; Deployment - Q2 2024</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#status","title":"Status","text":"<p>Current Status: In Progress</p> <p>We are currently in the middle of Phase 3. The backend team is focused on building out the core APIs for contact management and analytics. The frontend team is developing the main dashboard and reporting components.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/project_alpha/#technologies","title":"Technologies","text":"<ul> <li>Frontend: React, Redux, TypeScript</li> <li>Backend: Python, Django, PostgreSQL</li> <li>AI/ML: TensorFlow, scikit-learn</li> <li>Infrastructure: Google Cloud Platform (GCP), Docker, Kubernetes</li> </ul>"},{"location":"google-cloud-gemini-cookbook/lesson-04/rag_dataset/rag_intro/","title":"Rag intro","text":"<p>Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external data sources when generating responses. Here's a breakdown:</p> <p>What it is:</p> <ul> <li>Combining Retrieval and Generation:</li> <li>RAG combines the strengths of information retrieval systems (like search     engines) with the generative power of LLMs.</li> <li>It enables LLMs to go beyond their pre-trained data and access up-to-date     and specific information.</li> <li>How it works:</li> <li>When a user asks a question, the RAG system first retrieves relevant     information from external data sources (e.g., databases, documents, web     pages).</li> <li>This retrieved information is then provided to the LLM as additional     context.</li> <li>The LLM uses this augmented context to generate a more accurate and     informative response.</li> </ul> <p>Why it's helpful:</p> <ul> <li>Access to Up-to-Date Information:</li> <li>LLMs are trained on static datasets, so their knowledge can become     outdated. RAG allows them to access real-time or frequently updated     information.</li> <li>Improved Accuracy and Factual Grounding:</li> <li>RAG reduces the risk of LLM \"hallucinations\" (generating false or     misleading information) by grounding responses in verified external data.</li> <li>Enhanced Contextual Relevance:</li> <li>By providing relevant context, RAG enables LLMs to generate more precise     and tailored responses to specific queries.</li> <li>Increased Trust and Transparency:</li> <li>RAG can provide source citations, allowing users to verify the information     and increasing trust in the LLM's responses.</li> <li>Cost Efficiency:</li> <li>Rather than constantly retraining large language models, RAG allows for the     introduction of new data in a more cost effective way.</li> </ul> <p>In essence, RAG bridges the gap between the vast knowledge of LLMs and the need for accurate, current, and contextually relevant information.</p> <p>Source: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/","title":"Lesson 5: Advanced RAG with Knowledge Graphs","text":""},{"location":"google-cloud-gemini-cookbook/lesson-05/#cookbook-lesson-05-review-five-takeaways-to-enhance-your-gemini-apps","title":"Cookbook Lesson 05: Review - Five Takeaways to enhance your Gemini Apps","text":"<p>Welcome to Lesson 05 of my Gemini Cookbook series :)</p> <p>The potential of Gemini is limitless, but unlocking its true power requires more than just a \u2018hello world\u2019 app. It demands a strategic approach to performance, cost, and design. In this lesson 05, we\u2019ll transform that potential into reality by exploring 5\ufe0f\u20e3 key takeaways that will elevate your Gemini applications to the next level.</p> <p>In Lessons 01 through 04, we covered essential topics, from building a \u201chello world\u201d app to creating powerful, context-aware chatbots. In this Lesson 05, we\u2019ll do a quick recap of those earlier lessons and then dive into some important tips and takeaways for building great Gemini features applicable to most users.</p> <p>This lesson is the last part of the 5 part series Google Cloud - Gemini Cookbook (GitHub Link).</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#takeaway-01-multi-layered-application","title":"Takeaway 01: Multi-Layered Application","text":"<p>Let me quote from a Google Developer Advocate.</p> <p>\u201cTo effectively monitor your agent, it\u2019s best to adopt a practical, layered approach. Here\u2019s a guide to setting up observability at every stage of the development lifecycle.\u201d - Alvin Prayuda Juniarta Dwiyantoro</p> <p>Building an application and its features can become a cascading challenge, especially when you aim for an app that can be used, maintained, updated, and deployed to production environments. As mentioned in the quote, building applications with a layered approach can help streamline the overall development flow.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#flow-chart","title":"Flow chart","text":"<p>Lesson 04 Github-Link</p> <p></p> <p>This tutorial series built a sophisticated chatbot over four lessons.</p> <ul> <li>Lesson 01: We started with streamlit_app.py to create a basic \u201chello   world\u201d page and learned deployment fundamentals.</li> <li>Lesson 02: We developed llm.py, the chatbot\u2019s \u201cbrain,\u201d using Google\u2019s   GenAI SDK, and designed the user interface for interactive chat sessions.</li> <li>Lesson 03: We added contextual awareness using System Instructions and a   Context Cache to maintain coherent conversations.</li> <li>Lesson 04: We expanded the chatbot\u2019s context to external resources like   Google Search and external data stores via Retrieval Augmented Generation   (RAG).</li> </ul> <p>To maintain simplicity, Lessons 03 and 04 utilize a layered architecture (for code), adding new features as distinct modules. This actually differs with the initial approach, where a single file facilitated early development. As features expanded, I realized the benefits of dedicated modules like RAG.py and Cache.py over modifying existing files, which significantly streamlined development, testing, and debugging.</p> <p>This layered approach proved effective in our application. The following sections detail further takeaways from my experiments and explorations.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#takeaway-02-improve-your-speed","title":"Takeaway 02: Improve your speed","text":"<p>Large Language Models (LLMs) are extensive software applications that utilize significant CPU, memory, and network resources. Much like services such as BigQuery or Cloud SQL, these models are globally hosted, serving users worldwide. In this section, we will observe how you can improve model responses speed.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#021-global-endpoints-ge","title":"021 Global Endpoints (GE):","text":"<p>This section presents my personal explorations and hypotheses.</p> <p>It\u2019s common to assume that proximity to the LLM\u2019s physical location ensures optimal performance. While reduced network latency is a factor, it doesn\u2019t account for the entire picture. LLMs operate as shared services, inherently utilizing a first-in, first-out queuing system. Therefore, the ideal location selection hinges on the equation: min_response(Nearby locations + shorter queue). When incorporating additional variables like time and potential datacenter disruptions (e.g., maintenance, power failures, or even submarine cable issues), the optimal solution evolves to min_response(Nearby *available* locations + shorter queue).</p> <p>However, forecasting all these conditions is challenging. Continuously querying every Gemini model globally to assess response times could paradoxically impede overall performance. This is precisely where Global Endpoints demonstrate their value! They intelligently determine which LLM model will deliver the quickest response, thereby accelerating your interactions.</p> <p>Back in March 2024, I whipped up a demo app that hit a global endpoint instead of a specific regional one, like us-central1 or europe-central2. Since I was working from my place in Warsaw, Poland, with an old router, I figured there might be some lag. But honestly, the huge performance difference between the global endpoint and my go-to europe-west1 was pretty wild-in a good way!</p> <p></p> <p>Based on my personal explorations and practical experience, employing Global Endpoints(GE) can significantly enhance the speed of your Gemini application. While GE are an excellent choice, they do have some limitations.</p> <ol> <li>Data Localization: If your app needs to keep data in a specific place (like    for GDPR), GE might not be the right choice.</li> <li>LLM\u2019s Features Limitation: Certain features, such as RAG (Vector Search)    built in specific locations (e.g., Europe or US-Central), may not be    available for use.</li> </ol>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#022-using-gemini-lite-models","title":"022 Using Gemini-Lite Models","text":"<p>Upon the initial announcement of the Gemini Live API, I was quite enthusiastic, particularly regarding the integration of WebSockets, which I find to be remarkably efficient compared to standard HTTPS requests. Following exploration, I determined that a text-based chat proved to be the optimal choice for my needs, rather than audio or video conferencing.</p> <p>Although the Live API offers robust transcription options for both input and output (https://cloud.google.com/vertex-ai/generative-ai/docs/live-api#transcribe_audio), I found text-based interaction more effective(simple and fast enough) as it allowed me to review my queries before submission.</p> <p>These two considerations led me to consistently utilize the gemini-flash-lite models for straightforward chatbot applications.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#takeaway-03-cost-saving","title":"Takeaway 03: Cost Saving","text":""},{"location":"google-cloud-gemini-cookbook/lesson-05/#031-use-context-caching","title":"031 Use Context Caching","text":"<p>When leveraging Gemini for token usage tracking through observability, you might encounter a pleasant surprise: Gemini\u2019s Implicit Context Caching (ICC) and Explicit Context Caching (ECC) features. By default, ICC is active but can be disabled if necessary.</p> <p>As with any caching mechanism, the effectiveness is determined by the cache-hit to cache-miss ratio. With ICC, cost savings are not guaranteed. For instance, if you process 100\u2013200 distinct queries across various languages, a common token cache is not feasible, and Gemini cannot provide cost savings in such scenarios.</p> <p>In contrast to ICC, ECC offers a guaranteed cost reduction. While ECC is a powerful feature, its implementation is a design choice. Achieving cost-effectiveness with ECC, akin to reaching escape velocity, requires hitting a specific mathematical threshold. It\u2019s not magic, just pure mathematics!</p> <p>My recommendation is to consider using ECC when you have about 200+ queries and need a large context (dataset) in a short duration. For example, (1) you are building an Astro-Science Chatbot knowledgeable in a dozen physics, chemistry, and astrophysics books (2) You have to deal with really smart kids (I hope AI could handle those queries).</p> <p>Here is a quick recap of takeaway 031.</p> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#032-provision-throughput-pt","title":"032 Provision Throughput (PT)","text":"<p>This is Vertex AI only feature.</p> <p>Provisioned Throughput is a fixed-cost, fixed-term subscription available in several term-lengths that reserves throughput for supported generative AI models on Vertex AI. To reserve your throughput, you must specify the model and available locations in which the model runs.</p> <p>It is important to note that exceeding your Provisioned Throughput limit will not halt your requests. While this ensures uninterrupted service by preventing errors, it also means you cannot cap your Gemini costs if budget constraints are a concern.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#takeaway-04-context-awareness","title":"Takeaway 04: Context Awareness","text":"<p>Whether it\u2019s Gemini or some other LLMs, it\u2019s always the same rule. People are calling context the king but when I was learning data science, I heard a similar quote about Data / Information.</p> <p>Let me take a personal example here. Back in 2015, my colleagues had a simple classification challenge but the difficult part was the data distribution. Client has provided 1 TB of data positive data and 10 MB of negative data. (1,000,000 MB of positive data set and 10 MB is -tive dataset). The substantial disparity in data distribution made all our initial models predominantly classified queries as positive, achieving approximately 99.999% accuracy. However it was not the solution we needed.</p> <p>When I say context, it is the data relevancy. If your queries are not relevant to the context or theme you have set for the model, then you may be doing something fundamentally wrong. Imagine studying poetry to improve painting.</p> <p>So to keep your model responses relative to what users need, with gemini I have tested the following approaches</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#041-use-system-instructions","title":"041 Use System Instructions:","text":"<p>Define all the critical details that your model needs to remember. For example, \u201cYou are Bill Nye, the science guy. You are an American science communicator, television presenter, and former mechanical engineer. You are best known as the host of the science education television show Bill Nye the Science Guy and as a science educator in pop culture.\u201d (Copied from wikipedia)</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#042-use-grounding-with-google-search","title":"042 Use Grounding with Google Search:","text":"<p>To the scope limited, many times LLM models are not connected to search engines like Google (or DuckDuckGo or Yahoo). By enabling this, you can allow Gemini Model to browse the internet and get the latest information like date, climate and so on.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#043-use-rag","title":"043 Use RAG:","text":"<p>Similar to Grounding with Google Search, you may find your in a case where you want the model to automatically learn information about a certain project or certain dataset that is private to you or your team or organization. In such cases, using RAG has turned out to be an amazing investment.</p> <p>Using Vertex AI\u2019s RAG feature, all the provided data is converted into vectors and stored in a Vector database. When user queries, relevant data is identified from this Vector database and included as part of the overall context for the Model. As you may expect now this allows the model to get relevant answers.</p> <p>Here is a quick recap of takeaway 04.</p> <p></p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#takeaway-05-simplicity","title":"Takeaway 05: Simplicity","text":"<p>Simplicity stands as a crucial principle. Throughout my development and refinement of applications, I\u2019ve experimented with various LLM assisting tools. The Gemini CLI consistently proved most effective. Yet, even with this preference, it occasionally exhibited hallucinations, while other selections yielded even less favorable results. One time as an experiment, I allowed these changes to persist, meticulously reviewing each. This process finally led to a visually very appealing application with substantial code. During testing, as errors emerged, the model would rectify them. After several iterations, I realized I no longer genuinely enjoyed the application I had built. While aesthetically pleasing, I lacked the confidence to assume ownership for future feature additions or bug fixes. Gemini handled much of the work, but my engineering accountability kept me on edge.</p> <p></p> <p>Consider this: if a bug exists within 10 lines of code, the probability of quickly identifying it is 10%; however, in 500 lines of code, that probability drops to 0.2% (The layered approach in modules 03 and 04 simplified my code, resulting in faster debugging and error correction.)</p> <p>Simple designs are easier to understand, build, and maintain. As mentioned in Takeaway 01, a layered approach is helpful, but always prioritize simplicity in its execution. Ultimately, ensure your code is readable and simple for humans.</p>"},{"location":"google-cloud-gemini-cookbook/lesson-05/#conclusion","title":"Conclusion:","text":"<p>When building Gemini applications, a clear, layered, and straightforward design leads to robust, debuggable, and maintainable solutions, enabling powerful, efficient, and cost-effective Gemini-powered experiences.</p> <p>As this series concludes, remember that simplicity is key to great Gemini apps. These five takeaways are your secret ingredients for success. The next adventure awaits with the Google Agent Development Kit.</p> <p>Don\u2019t stop here! Our next post will explore the Google Agent Development Kit (ADK), built on the Google Gen AI SDK, to enhance your Gemini development journey. Lets go beyond the basics and build something outstanding in Gen AI.</p> <p>Now something for fun :)</p> <p></p>"},{"location":"models/","title":"Gemini Model Cards","text":"<p>A simple and quick overview of Gemini Models and their capabilities.</p> <p>For offline reading, you can download a snapshot of the Wikipedia article:</p> <ul> <li>\u27a1\ufe0f Download as PDF</li> <li>\ud83d\uddbc\ufe0f Download as PNG Image</li> </ul>"},{"location":"models/PROPOSAL/","title":"Project Proposal: Gemini Model Cards","text":""},{"location":"models/PROPOSAL/#1-problem-statement","title":"1. Problem Statement","text":"<p>Google Cloud's official documentation for Gemini models is comprehensive but can be dense and time-consuming for developers and tech enthusiasts who need a quick, high-level overview. There is a need for a more concise, visually appealing, and easily shareable format to understand the key features and capabilities of different Gemini models.</p>"},{"location":"models/PROPOSAL/#2-proposed-solution","title":"2. Proposed Solution","text":"<p>This project aims to create \"Gemini Model Cards\" that provide a snapshot of each Gemini model in a clear and consistent format. These cards will be generated as images or PDFs, making them easy to share on social media, in presentations, or within blog posts.</p> <p>The visual design will be based on the provided mock-up, ensuring a professional and aesthetically pleasing look.</p>"},{"location":"models/PROPOSAL/#3-target-audience","title":"3. Target Audience","text":"<p>The primary audience for these model cards are:</p> <ul> <li>Developers: Who need to quickly compare models and decide which one to   use for their application.</li> <li>Tech Enthusiasts &amp; Students: Who want to learn about the latest   advancements in Gemini models.</li> <li>Content Creators: Who need a quick and easy way to share information   about Gemini models with their audience.</li> </ul>"},{"location":"models/PROPOSAL/#4-key-information-to-include","title":"4. Key Information to Include","text":"<p>Each model card will include the following information, based on the provided mock-up and industry best practices:</p> <ul> <li>Model Details:</li> <li>Model Name (e.g., Gemini 2.5)</li> <li>Model Identifier (e.g., gemini-2.5-flash-lite)</li> <li>Release Date</li> <li>Knowledge Cut-off Date</li> <li>Capabilities:</li> <li>Input/Output Modalities (e.g., Audio, Video, Text)</li> <li>Token Limits</li> <li>Availability:</li> <li>General Availability</li> <li>Preview Availability</li> <li>Private Availability</li> <li>Features:</li> <li>Live API</li> <li>Code Execution</li> <li>System Instructions</li> <li>Function Calling</li> <li>Grounding with Google Search</li> <li>Pricing:</li> <li>Input and Output pricing per million tokens/media type.</li> <li>SDKs &amp; ADKs:</li> <li>Latest versions of relevant Software and Application Development Kits.</li> </ul>"},{"location":"models/PROPOSAL/#5-format","title":"5. Format","text":"<p>The model cards will be generated in the following formats:</p> <ul> <li>Image (PNG): For easy embedding in web pages and social media.</li> <li>PDF: For high-quality printing and sharing.</li> </ul>"},{"location":"models/PROPOSAL/#6-project-structure","title":"6. Project Structure","text":"<ul> <li><code>docs/models/config</code>: Stores configuration files, including the main <code>models.csv</code> data source.</li> <li><code>docs/models/reports</code>: The output directory for the generated HTML reports.</li> <li><code>docs/models/downloads</code>: The output directory for generated PDF and PNG files.</li> <li><code>docs/models/automation</code>: Contains all the scripts and templates for the automation process.</li> </ul>"},{"location":"models/PROPOSAL/#7-next-steps","title":"7. Next Steps","text":"<ol> <li>Finalize a template: Refine the design of the model card based on the    mock-up.</li> <li>Automate card generation: Develop a Python script that takes model data    from a structured source (like a CSV or JSON file) and generates the model    cards.</li> <li>Gather model data: Collect the necessary data for the different Gemini    models.</li> <li>Generate and share: Generate the cards and make them available for the    community.</li> </ol>"},{"location":"models/progress/","title":"Project Progress: Gemini Model Cards","text":"<p>This document tracks the progress of the Gemini Model Cards project.</p>"},{"location":"models/progress/#completed","title":"Completed","text":"<ul> <li>\u2705 Project Proposal: The project proposal has been created and approved.</li> <li>\u2705 Initial Assets: Initial assets, including the mock-up design and Python   scripts, have been created.</li> </ul>"},{"location":"models/progress/#to-do","title":"To Do","text":"<ul> <li>\ud83c\udfa8 Finalize a template: Refine the design of the model card based on the   mock-up.</li> <li>\ud83e\udd16 Automate card generation: Develop a Python script that takes model data   from a structured source (like a CSV or JSON file) and generates the model   cards.</li> <li>\ud83d\udcca Gather model data: Collect the necessary data for the different Gemini   models.</li> <li>\ud83d\ude80 Generate and share: Generate the cards and make them available for the   community.</li> </ul>"},{"location":"models/template/","title":"[Model Name] [Version]","text":"<p><code>[model-identifier-string]</code></p>"},{"location":"models/template/#overview","title":"Overview","text":"<ul> <li>Release Date: <code>[Release Date, e.g., April 9, 2025]</code></li> <li>Knowledge Cut-off: <code>[Knowledge Cut-off, e.g., June 2024]</code></li> <li>Availability: <code>[Availability Status, e.g., General Availability]</code></li> </ul>"},{"location":"models/template/#capabilities","title":"Capabilities","text":"<p>| Input Modalities | Max Tokens | Output Modalities | Max Tokens | | :--------------- | :----------------- | :---------------- | :----------------- | | \ud83d\udcdd Text | <code>[Input Text Tokens]</code> | \ud83d\udcdd Text | <code>[Output Text Tokens]</code> | | \ud83d\uddbc\ufe0f Image | <code>[Input Image Tokens]</code> | \ud83d\uddbc\ufe0f Image | <code>[Output Image Tokens]</code>| | \ud83c\udfb5 Audio | <code>[Input Audio Tokens]</code> | \ud83c\udfb5 Audio | <code>[Output Audio Tokens]</code>| | \ud83c\udfa5 Video | <code>[Input Video Tokens]</code> | | |</p>"},{"location":"models/template/#key-features","title":"Key Features","text":"<ul> <li>\u2705 Live API Access</li> <li>\u2705 Code Execution</li> <li>\u2705 System Instructions</li> <li>\u2705 Function Calling</li> <li>\u2705 Grounding with Google Search</li> <li>\u2705 <code>[Other Feature]</code></li> </ul>"},{"location":"models/template/#pricing","title":"Pricing","text":"<p>| Type | Direction | Price / 1M Tokens | | :---------- | :-------- | :---------------- | | Text | Input | <code>$ [Price]</code> | | Text | Output | <code>$ [Price]</code> | | Image/Video | Input | <code>$ [Price]</code> | | Audio | Input | <code>$ [Price]</code> | | Audio | Output | <code>$ [Price]</code> |</p>"},{"location":"models/template/#developer-resources","title":"Developer Resources","text":"<p>Latest SDKs:</p> <ul> <li>Python: <code>google-genai [version]</code></li> <li>Go: <code>go-genai [version]</code></li> <li>Node.js: <code>@google/genai [version]</code></li> <li>Java: <code>google-genai [version]</code></li> </ul> <p>Other Libraries:</p> <ul> <li>Python: <code>google-adk [version]</code></li> <li>Java: <code>google-adk [version]</code></li> </ul>"}]}